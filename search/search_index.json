{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Discovery","text":"<p>Secure AI Data Analytics for Proprietary Data</p> <ul> <li>Visit : https://app.dwani.ai</li> </ul> <p> </p> <pre><code>sudo apt-get update\nsudo apt-get install poppler-utils -y\n</code></pre> <ul> <li>Client</li> </ul> <pre><code>python ux/ux.py\n</code></pre> <ul> <li>Server</li> </ul> <pre><code>export VLLM_IP=\"your_vllm_ip\"\nuvicorn server.main:app --host 0.0.0.0 --port 18889\n</code></pre> <ul> <li>VLLM Server setup - server/vlm/README.md</li> </ul> <ul> <li>Step to Run Locally docs/local_run.md</li> </ul> <ul> <li>Docker build steps - docs/build_docker.md</li> </ul>"},{"location":"build_docker/","title":"Build docker","text":"<p>Client </p> <p>docker build -t dwani/discovery_ux:latest -f client.Dockerfile . docker push dwani/discovery_ux:latest</p> <p>docker run -p 80:8000 --env DWANI_API_BASE_URL=$DWANI_API_BASE_URL dwani/discovery_ux:latest</p> <p>Server</p> <p>docker build -t dwani/discovery_server:latest -f server.Dockerfile . docker push dwani/discovery_server:latest</p> <p>docker run -p 18888:18888 --env DWANI_API_BASE_URL=$DWANI_API_BASE_URL dwani/discovery_server:latest</p> <p>--</p> <ul> <li>Client Docker Run</li> </ul> <pre><code>docker run -p 80:8000 --env DWANI_API_BASE_URL=&lt;server_ip&gt; dwani/discovery_ux:latest\n</code></pre> <ul> <li>Server  Docker Run </li> </ul> <p>Cuda -d Container Toolkit</p> <p>-- </p> <p>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html</p> <p>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\   &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\     sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\     sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list</p> <p>sudo apt-get update</p> <p>export NVIDIA_CONTAINER_TOOLKIT_VERSION=1.17.8-1   sudo apt-get install -y \\       nvidia-container-toolkit=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\       nvidia-container-toolkit-base=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\       libnvidia-container-tools=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\       libnvidia-container1=${NVIDIA_CONTAINER_TOOLKIT_VERSION}</p> <p>update - daemon.json : /etc/docker/daemon.json</p> <p>sudo systemctl restart docker</p> <p>sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi</p>"},{"location":"cuda_setup/","title":"Cuda setup","text":"<p>Cuda Setup</p> <ul> <li>cuda 12.8 - https://developer.nvidia.com/cuda-12-8-0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=22.04&amp;target_type=deb_local</li> </ul> <p>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</p> <p>sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600</p> <p>wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-ubuntu2204-12-8-local_12.8.0-570.86.10-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu2204-12-8-local_12.8.0-570.86.10-1_amd64.deb</p> <p>sudo cp /var/cuda-repo-ubuntu2204-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/</p> <p>sudo apt-get update</p> <p>sudo apt-get -y install cuda-toolkit-12-8</p> <p>export PATH=/usr/local/cuda/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH</p> <ul> <li>cuda 13 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</li> </ul> <p>sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600</p> <p>wget https://developer.download.nvidia.com/compute/cuda/13.0.0/local_installers/cuda-repo-ubuntu2204-13-0-local_13.0.0-580.65.06-1_amd64.deb</p> <p>sudo dpkg -i cuda-repo-ubuntu2204-13-0-local_13.0.0-580.65.06-1_amd64.deb</p> <p>sudo cp /var/cuda-repo-ubuntu2204-13-0-local/cuda-*-keyring.gpg /usr/share/keyrings/</p> <p>sudo apt-get update</p> <p>sudo apt-get -y install cuda-toolkit-13-0</p> <p>sudo apt-get install -y cuda-drivers</p> <p>--</p> <ul> <li>To Reset / remove CUDA</li> </ul> <p>sudo apt remove --purge nvidia-* sudo apt update sudo apt install linux-headers-$(uname -r) dkms sudo ubuntu-drivers autoinstall</p>"},{"location":"go_steps/","title":"Go steps","text":"<p>docker build -t my-go-app .</p> <p>docker run -p 8080:8080 --env VLLM_IP= my-go-app"},{"location":"local_run/","title":"Local run","text":""},{"location":"local_run/#to-run-locally-english-only","title":"To Run locally  - English only","text":"<pre><code>sudo apt-get update\nsudo apt-get install tesseract-ocr\nsudo apt-get install poppler-utils -y\n\n- server/vlm/llama.md\n    - Till line - 22\n\n\npython3.10 -m venv venv\nsource venv/bin/activate\n\npip install -r server-requirements.txt\n\npip install -r client-requirements.txt\n\nFor server - \n- uvicorn server.local_main:app --host 0.0.0.0 --port 18889\n\n\nFor Client\n- python ux/ux.py\n\n</code></pre>"},{"location":"server_image/","title":"Server image","text":"<p>Server </p> <p>https://docs.lambda.ai/public-cloud/on-demand/#base-images</p>"},{"location":"windows/","title":"Windows","text":"<p>To - run discovery server on windows</p> <ul> <li>run Powershell as Administrator and run below command</li> </ul> <p>Set-ExecutionPolicy RemoteSigned</p> <p>python.exe -m venv venv</p> <p>.\\venv\\Scripts\\Activate.ps1</p> <p>pip.exe install -r server-requirements.txt</p> <p>As - powershell admin choco install poppler</p> <p>Add to PATH C:\\ProgramData\\chocolatey\\lib\\poppler\\tools</p> <p>Install Docker desktop</p> <p>as powershell admin wsl --update</p> <p>docker build -t dwani/discovery_server:latest -f server.Dockerfile . //docker run -p 18889:18889 --env DWANI_API_BASE_URL=0.0.0.0 dwani/discovery_server:latest</p> <p>docker-compose.exe up -d</p> <p>Install cuda toolkit for windows</p> <p>llama-cpp In powershell</p> <p>winget install llama.cpp</p> <p>llama-server -hf ggml-org/gemma-3-4b-it-GGUF --host 0.0.0.0 --port 9000 --n-gpu-layers 99 --ctx-size 8192 --alias gemma3 </p> <p>llama-server -hf ggml-org/gemma-3-4b-it-GGUF --host 0.0.0.0 --port 9000 --n-gpu-layers 99 --ctx-size 1008 --alias gemma3 </p> <p>Q4 : 2.37 ./build/bin/llama-server -hf google/gemma-3-4b-it-qat-q4_0-gguf --host 0.0.0.0 --port 9000 --n-gpu-layers 99 --ctx-size 1008 --alias gemma3</p> <p>Q4 : 2.37 ./build/bin/llama-server -hf ggml-org/gemma-3-4b-it-qat-GGUF --host 0.0.0.0 --port 9000 --n-gpu-layers 99 --ctx-size 1008 --alias gemma3</p> <p>Q2 :   1.73 GB</p> <p>./build/bin/llama-server -hf unsloth/gemma-3-4b-it-GGUF:Q2_k --host 0.0.0.0 --port 8000 --n-gpu-layers 99 --ctx-size 1008 --alias gemma3</p>"},{"location":"code/","title":"Index","text":""},{"location":"code/#discovery-code-gen","title":"Discovery - Code Gen","text":"<ul> <li> <p>sudo apt install libcurl4-openssl-dev</p> </li> <li> <p>git clone https://github.com/ggml-org/llama.cpp.git</p> </li> <li>cd llama.cpp</li> <li> <p>git checkout 0d8831543cdc368fb248bae6f1b4aa5516684edc</p> </li> <li> <p>With NVIDIA GPU and CUDA Toolkit</p> <ul> <li>cmake -B build -DGGML_CUDA=ON</li> </ul> </li> <li> <p>for CPU </p> <ul> <li>cmake -B build</li> </ul> </li> <li> <p>cmake --build build --config Release -j4</p> </li> <li> <p>For Laptop / PC -  gpt-oss-20b</p> <ul> <li>./build/bin/llama-server -hf ggml-org/gpt-oss-20b-GGUF -c 0 -fa --jinja --reasoning-format none --port 9500 -ngl 99</li> </ul> </li> </ul> <p>./build/bin/llama-server -hf ggml-org/Qwen2.5-VL-3B-Instruct-GGUF --host 0.0.0.0 --port 9000 --n-gpu-layers 99 --ctx-size 8192 --alias gemma3</p> <p>ggml-org/Qwen2.5-VL-3B-Instruct-GGUF</p>"},{"location":"rag/chunking/","title":"Chunking","text":"<p>https://github.com/weaviate/recipes/blob/main/weaviate-features/services-research/late_chunking_berlin.ipynb</p>"},{"location":"vlm/","title":"Index","text":""},{"location":"vlm/#discovery-vlm","title":"Discovery - VLM","text":"<p>sudo apt update sudo apt upgrade -y</p> <p>sudo apt-get install libssl-dev libcurl4-openssl-dev python3.12 python3.12-venv python3.12-dev -y</p> <p>wget https://developer.download.nvidia.com/compute/cudnn/9.12.0/local_installers/cudnn-local-repo-ubuntu2204-9.12.0_1.0-1_arm64.deb</p> <p>sudo dpkg -i cudnn-local-repo-ubuntu2204-9.12.0_1.0-1_arm64.deb</p> <p>sudo cp /var/cudnn-local-repo-ubuntu2204-9.12.0/cudnn-*-keyring.gpg /usr/share/keyrings/</p> <p>sudo apt-get update</p> <p>sudo apt-get -y install cudnn</p> <p>sudo apt-get -y install cudnn9-cuda-12</p> <p>python3.12 -m venv venv</p> <p>source venv/bin/activate</p> <p>pip install torch==2.7.1 torchaudio==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128</p> <p>pip install https://github.com/dwani-ai/xformers-arm64/releases/download/v0.0.01/xformers-0.0.33+5d4b92a5.d20250815-cp39-abi3-linux_aarch64.whl</p> <p>pip install https://github.com/dwani-ai/vllm-arm64/releases/download/v.0.0.4/vllm-0.10.1.dev0+g6d8d0a24c.d20250726-cp312-cp312-linux_aarch64.whl</p> <p>vllm serve RedHatAI/gemma-3-27b-it-FP8-dynamic --served-model-name gemma3 --host 0.0.0.0 --port 9000 --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --max-model-len 98304 --disable-log-requests --dtype bfloat16 --enable-chunked-prefill --enable-prefix-caching --max-num-batched-tokens 8192 --chat-template-content-format openai</p> <p>for PC</p> <ul> <li>vllm serve HuggingFaceTB/SmolVLM-256M-Instruct --gpu-memory-utilization 0.4 --served-model-name gemma3 --host 0.0.0.0 --port 9000 --disable-log-requests</li> </ul>"},{"location":"vlm/b200/","title":"B200","text":"<p>curl -LsSf https://astral.sh/uv/install.sh | INSTALLER_DOWNLOAD_URL=https://wheelnext.astral.sh sh uv venv venv --python 3.12.0 source venv/bin/activate</p> <p>uv pip install -U vllm \\     --torch-backend=auto \\     --extra-index-url https://wheels.vllm.ai/nightly</p> <p>vllm serve RedHatAI/gemma-3-27b-it-FP8-dynamic --served-model-name gemma3 --host 0.0.0.0 --port 9000 --gpu-memory-utilization 0.9 --tensor-parallel-size 8 --max-model-len 98304 --disable-log-requests --dtype bfloat16 --enable-chunked-prefill --enable-prefix-caching --max-num-batched-tokens 8192 --chat-template-content-format openai</p> <p>uvicorn main:app --host 0.0.0.0 --port 18888 --workers 32 --loop uvloop --http httptools --backlog 2048 --timeout-keep-alive 30 --limit-concurrency 1000 --limit-max-requests 10000</p> <p>uvicorn main:app --host 0.0.0.0 --port 18888 --workers 32 --loop uvloop --backlog 2048 --limit-concurrency 1000 --limit-max-requests 10000</p>"},{"location":"vlm/build/","title":"Build","text":"<p>sudo apt update sudo apt upgrade -y</p> <p>sudo apt-get install libssl-dev libcurl4-openssl-dev python3.12 python3.12-venv python3.12-dev -y</p> <p>wget https://github.com/Kitware/CMake/releases/download/v4.1.0/cmake-4.1.0.tar.gz tar -zxvf cmake-4.1.0.tar.gz cd cmake-4.1.0 ./bootstrap make -j4 sudo make install</p> <p>python3.12 -m venv venv</p> <p>source venv/bin/activate</p> <p>sudo apt-get install build-essential libnuma-dev -y</p> <p>pip install torch==2.7.1 torchaudio==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128</p> <p>git clone https://github.com/vllm-project/vllm.git</p> <p>cd vllm</p> <p>python use_existing_torch.py </p> <p>pip install --upgrade setuptools twine setuptools-scm</p> <p>pip install -r requirements/cuda.txt</p> <p>export MAX_JOBS=4 export NVCC_THREADS=2 export TORCH_CUDA_ARCH_LIST=\"\" export VLLM_TARGET_DEVICE=cuda</p> <p>python setup.py bdist_wheel pip install dist/*.whl</p> <p>To Run</p> <p>pip install xformers</p> <p>vllm serve</p> <p>vllm serve RedHatAI/gemma-3-27b-it-FP8-dynamic --served-model-name gemma3 --host 0.0.0.0 --port 9000 --gpu-memory-utilization 0.8 --tensor-parallel-size 1 --max-model-len 65536 --disable-log-requests</p> <p>vllm serve google/gemma-3-4b-it --served-model-name gemma3 --host 0.0.0.0 --port 9000 --gpu-memory-utilization 0.8 --tensor-parallel-size 1 --max-model-len 65536     --dtype bfloat16 --disable-log-requests</p>"},{"location":"vlm/fast-vlm/","title":"Fast vlm","text":"<p>Fast VLm</p> <p>https://huggingface.co/apple/FastVLM-0.5B</p> <p>pip install timm</p> <p>For CPU</p> <p>uvicorn fast-vlm-api:app --reload --host 0.0.0.0 --port 8000 -- --device cpu</p> <p>for GPU uvicorn fast-vlm-api:app --reload --host 0.0.0.0 --port 8000 -- --device cuda</p> <p>default with cuda </p> <p>uvicorn fast-vlm-api:app --reload --host 0.0.0.0 --port 8000</p> <p>python fast-vlm-api.py --device cpu</p> <p>python fast-vlm-api.py --device cuda</p>"},{"location":"vlm/llama/","title":"Llama","text":"<ul> <li>To Run - gpt-oss - on PC<ul> <li>llama.cpp for gpt-oss models from OpenAI</li> </ul> </li> </ul> <pre><code>sudo apt install libcurl4-openssl-dev\n\ngit clone --depth 1 https://github.com/ggml-org/llama.cpp.git\ncd llama.cpp\n\n\n</code></pre> <ul> <li>With NVIDIA GPU and CUDA Toolkit</li> </ul> <pre><code>    cmake -B build -DGGML_CUDA=ON\n</code></pre> <ul> <li>for CPU </li> </ul> <pre><code>cmake -B build\n</code></pre> <ul> <li>build llama.cpp executable</li> </ul> <pre><code>cmake --build build --config Release -j4\n</code></pre> <ul> <li>For Vision + Text : Use Gemma</li> </ul> <p>./build/bin/llama-server -hf ggml-org/gemma-3-4b-it-GGUF --host 0.0.0.0 --port 9000 --n-gpu-layers 99 --ctx-size 8192 --alias gemma3 </p> <ul> <li>For Laptop / PC -  gpt-oss-20b</li> </ul> <pre><code>./build/bin/llama-server -hf ggml-org/gpt-oss-20b-GGUF -c 0 -fa --jinja --reasoning-format none --port 9500\n</code></pre> <ul> <li>For H100/H200 - gpt-oss-120b</li> </ul> <pre><code>    ./build/bin/llama-server -hf ggml-org/gpt-oss-120b-GGUF -c 0 -fa --jinja --reasoning-format none --port 9500\n</code></pre> <ul> <li>Then, access http://localhost:9000</li> </ul>"},{"location":"vlm/torhc_2_8/","title":"Torhc 2 8","text":"<p>torch 2.8</p> <p>curl -LsSf https://astral.sh/uv/install.sh | INSTALLER_DOWNLOAD_URL=https://wheelnext.astral.sh sh uv venv venv source venv/bin/activate</p> <p>uv pip install torch torchvision</p> <p>pip install https://github.com/dwani-ai/vllm-arm64/releases/download/v0.0.7/vllm-0.1.dev1+ge5d3d63c4.d20250812.cpu-cp312-cp312-linux_aarch64.whl</p>"},{"location":"vlm/x86/","title":"X86","text":"<p>sudo apt update sudo apt upgrade -y</p> <p>sudo apt-get install libssl-dev libcurl4-openssl-dev python3.12 python3.12-venv python3.12-dev -y</p> <p>python3.12 -m venv venv</p> <p>source venv/bin/activate</p> <p>pip install torch==2.7.1 torchaudio==2.7.1 torchvision </p> <p>pip install xformers flashinfer-python vllm==0.10.0</p> <p>vllm serve RedHatAI/gemma-3-27b-it-FP8-dynamic --served-model-name gemma3 --host 0.0.0.0 --port 9000 --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --max-model-len 98304 --disable-log-requests --dtype bfloat16 --enable-chunked-prefill --enable-prefix-caching --max-num-batched-tokens 8192 --chat-template-content-format openai</p>"},{"location":"vlm/misc/flash-infer/","title":"Flash infer","text":"<p>python3.12 -m venv venv source venv/bin/activate git clone https://github.com/flashinfer-ai/flashinfer.git --recursive cd flashinfer python -m pip install -v .</p> <p>pip install --upgrade setuptools twine setuptools-scm wheel setuptools build</p> <p>python -m build --no-isolation --wheel python -m pip install dist/flashinfer_*.whl</p>"},{"location":"vlm/misc/xfomermer/","title":"Xfomermer","text":"<p>sudo apt-get update sudo apt upgrade -y sudo apt-get install libssl-dev libcurl4-openssl-dev python3.12 python3.12-venv python3.12-dev -y sudo apt-get install build-essential  ninja-build python3-dev</p> <p>wget https://developer.download.nvidia.com/compute/cudnn/9.12.0/local_installers/cudnn-local-repo-ubuntu2204-9.12.0_1.0-1_arm64.deb</p> <p>sudo dpkg -i cudnn-local-repo-ubuntu2204-9.12.0_1.0-1_arm64.deb  sudo cp /var/cudnn-local-repo-ubuntu2204-9.12.0/cudnn-*-keyring.gpg /usr/share/keyrings/</p> <p>sudo apt-get update sudo apt-get -y install cudnn sudo apt-get -y install cudnn9-cuda-12</p> <p>python3.12 -m venv venv source venv/bin/activate</p> <p>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</p> <p>source $HOME/.cargo/env</p> <p>git clone https://github.com/facebookresearch/xformers.git cd xformers</p> <ul> <li> <p>for pytorch 2.8 </p> <ul> <li>git checkout 5d4b92a5e5a9c6c6d4878283f47d82e17995b468</li> </ul> </li> <li> <p>fpr pytorch 2.7</p> <ul> <li>git checkout eb0946a363464da96ea40afd1a7f72a907c25497</li> </ul> </li> </ul> <p>git submodule update --init --recursive</p> <p>pip install --upgrade setuptools twine setuptools-scm wheel setuptools ninja</p> <p>export TORCH_CUDA_ARCH_LIST=\"8.0;8.6;9.0\" pip install torch==2.7.1 torchaudio==2.7.1 torchvision --index-url https://download.pytorch.org/whl/cu128</p> <p>export CUDA_HOME=/usr/local/cuda/</p> <p>export MAX_JOBS=3 python setup.py bdist_wheel</p> <p>pip install xformer*.whl</p>"}]}