{
  "response": "## Key Points from the Text:\n\nHere's a breakdown of the key points from the provided text, organized for clarity:\n\n**1. DINOv3: A New State-of-the-Art Self-Supervised Model:**\n\n*   The paper introduces DINOv3, a new self-supervised learning (SSL) model that builds upon DINO and DINOv2.\n*   DINOv3 achieves state-of-the-art results on various computer vision tasks *without* task-specific fine-tuning, demonstrating its versatility.\n*   It excels in both dense (e.g., segmentation, depth estimation) and global (e.g., classification, retrieval) tasks.\n\n**2. Key Innovations & Techniques:**\n\n*   **Scaling:**  The model is scaled up to 6.7 billion parameters and trained on a larger, curated dataset.\n*   **Data Curation:** A novel data curation technique combining clustering and retrieval methods is used to improve data quality.\n*   **Gram Anchoring:** A new training objective (\"Gram anchoring\") is introduced to improve the quality of dense feature maps, addressing a degradation issue that occurs during extended training. This focuses on consistency of patch-level representations.\n*   **RoPE Enhancement:** A custom variant of Rotary Position Embeddings (RoPE) is used.\n*   **Distillation:** A distillation process is used to create smaller, more efficient models (ConvNeXts) from the larger DINOv3 model.\n\n**3. Performance & Results:**\n\n*   **Superior Dense Features:** DINOv3 significantly outperforms previous methods in generating high-quality dense feature maps.\n*   **Strong Global Representations:**  It also achieves competitive results on tasks requiring global image understanding.\n*   **Outperforms Existing Methods:** DINOv3 surpasses state-of-the-art models (including CLIP and others) on various benchmarks, including object detection, semantic segmentation, and 3D scene understanding.\n*   **Generalization to New Domains:**  The model demonstrates strong performance even when applied to satellite imagery.\n\n**4.  Related Work & Context:**\n\n*   The paper discusses the evolution of self-supervised learning, contrasting it with supervised and weakly-supervised approaches.\n*   It highlights the importance of data quality and scaling in SSL.\n*   It positions DINOv3 within the broader landscape of vision-language pre-training and foundation models.\n\n**5.  Implementation Details:**\n\n*   The model is trained using a distributed data parallel (DDP) implementation.\n*   Specific training parameters (learning rate, weight decay, etc.) are provided.\n*   The paper details the evaluation protocols and datasets used for benchmarking.\n\n\n\nIn essence, the paper presents DINOv3 as a significant step forward in self-supervised learning, offering a powerful and versatile foundation model for a wide range of computer vision applications.  The key to its success lies in its scaling, data curation, novel training objective (Gram anchoring), and efficient distillation process.",
  "extracted_text": {
    "2": "55\n\n45\n\n40\n\nDINoV3\nDINoV2\nDINoV1\n\nSIAE\n\n40\n\n(a) Semantic segmentation (ADE20k)\n\n60\n\n50\n\nRecall\n\nDINoV3\n\n50\n\n60\n\nAccuracy\n\nlog-FLOPs\n\n80\n\n(b) 3D point model matching (NAVI)\n\nlog-FLOPs\n\nFigure 2: Performance of the DINoV3 family of models (compared to other families of self- or weakly-\nsupervised models on different benchmarks. DINoV3 significantly surpasses others on dense benchmarks,\nincluding models that leverage mask annotation priors such as AM-RADIO (Heinrich et al., 2025).\n\nStrong & Versatile Foundational Models DINoV3 aims to offer a high level of versatility along two\naxes, which is enabled by the scaling of the model size and training data. First, a key desirable property\nfor SSL models is to achieve excellent performance while being kept frozen, ideally reaching similar state-\nof-the-art results as specialized models. In that case, a single forward pass can deliver cutting-edge results\nacross multiple tasks, leading to substantial computational savings—an essential advantage for practical\napplications, particularly on edge devices. We show the wide breadth of tasks that DINoV3 can successfully\nbe applied to in Sec. 6. Second, a scalable SSL training pipeline that does not depend on metadata unlocks\nnumerous scientific applications. By pre-training on a diverse set of images, whether raw images or obser-\n vational data, SSL models generalize across a large set of domains and tasks. As illustrated in Fig. 1(d),\nthe PCA of DINoV3 features extracted from a high-resolution aerial image clearly allows to separates roads,\nhouses, and greenery, highlighting the model’s feature quality.\n\nSuperior Feature Maps Through Gram Anchoring Another key feature of DINoV3 is a significant\nimprovement of its dense feature maps. The DINoV3 SSL training strategy aims at producing models\nexcelling at high-level semantic tasks while producing excellent feature maps amenable to solving geometric\ntasks such as depth estimation, or 3D matching. In particular, the models should produce dense feature\nthat can be used off-the-shelf or with little post-processing. The compromise between dense and global\nrepresentation is especially difficult to optimize when training with vast amounts of images. Since the objective\nof high-level understanding conflict with the quality of the dense feature maps. Our contradictory\nobjectives lead to a collapse of dense features with large training data, causing training issues. We overcome\nbetter dense feature maps with Gram Anchoring (Sec. 4.1). As a result, DINoV3 dense features surpass\n the state of the art by 8% on ScanNet (Fig. 2(b)) and by 12% on ADE20k (Fig. 2(a)).\n\nDINoV3 vs. Existing Methods In Fig. 2, we compare DINoV3 to state-of-the-art self-supervised and\nweakly-supervised methods on semantic segmentation (ADE20k) and 3D point cloud matching (NAVI).\nDINoV3 significantly outperforms all other methods on both tasks, achieving a superior trade-off between\naccuracy and computational cost (measured in FLOPs). Notably, DINoV3 surpasses methods that leverage\nmask annotations (e.g., AM-RADIO) even without any mask supervision. This demonstrates the effectiveness\nof our SSL training strategy and the quality of the learned representations. The results highlight that DINoV3\nnot only achieves high accuracy but also does so efficiently, making it a practical choice for real-world\napplications.",
    "3": "Figure 3: High-resolution dense features. We visualize the cosine similarity maps obtained with DINoV2 output features between the patches marked with a red cross and all other patches. Input image at 4096x4096. Please zoom in, do you agree with DINoV2?\n\nOverview of Contributions. In this work, we introduce multiple contributions to address the challenge of scaling SSL towards a large frontier model. We build upon recent advances in automatic data curation (Vo et al., 2024) to obtain a large “background” training dataset that we carefully mix with a bit of specialized data (ImageNet-1k). This allows leveraging large amounts of unconstrained data to improve the model performance. This contribution (i) around data scaling will be described in Sec. 3.1.\nWe increase our main model size to 7B parameters by defining a custom variant of the ViT architecture. We include modern position embeddings (axial RoPE) and develop a regularization technique to avoid positional artifacts. Departing from the multiple cosine schedules in DINoV2, we train with constant hyperparameter schedules for 1M iterations. This allows producing models with stronger performance. This contribution (ii) on model architecture and training will be described in Sec. 3.2.\nWith the above techniques, we are able to train a model following the DINoV2 algorithm at scale. However, as mentioned previously, scale leads to a degradation of dense features. To address this, we propose a core improvement of the pipeline with a Gram anchoring training phase. This cleans the noise in the feature maps, leading to impressive similarity maps, and drastically improving the performance on both parametric and non-parametric dense tasks. This contribution (iii) on Gram training will be described in Sec. 4.\nFollowing previous practice, the last steps of our pipeline consist of a high-resolution post-training phase and distillation into a series of high-performance models of various sizes. For the latter, we develop a novel and\n4",
    "4": "efficient single-teacher multiple-students distillation procedure. This contribution (iv) transfers the power of our 7B frontier model to a family of smaller practical models for common usage, that we describe in Sec. 5.2. As measured in our thorough benchmarking, results in Sec. 6 show that our approach defines a new standard in dense tasks and performs comparably to CLIP derivatives on global tasks. In particular, with a frozen vision backbone, we achieve state-of-the-art performance on longstanding computer vision problems such as object detection (COCO detection, mAP 66.1) and image segmentation (ADE20k, mIoU 63.0), outperforming specialized fine-tuned pipelines. Moreover, we provide evidence of the generality of our approach across domains by applying the DINOv3 algorithm to satellite imagery, in Sec. 8, surpassing all prior approaches.\n\n2 Related Work\nSelf-Supervised Learning Learning without annotations requires an artificial vision task that provides supervision in lieu for training. The art and challenge of SSL lies in carefully designing these so-called pre-text tasks in order to learn powerful representations for downstream tasks. The language domain, by discrete nature, offers straightforward ways to set up such tasks, which led to many successful unsupervised pre-training approaches for text data. Examples include word embeddings (Mikolov et al., 2013; Bojanowski et al., 2017), sentence representations (Devlin et al., 2018; Liu et al., 2019), and plain language models (Zaremba et al., 2014). In contrast, computer vision presents greater challenges due to the continuous nature of the signal. Early attempts mimicking language approaches extracted supervisory signals from parts of an image to predict other parts, e.g. by predicting relative patch position (Doersch et al., 2015), patch re-ordering (Noroozi and Favaro, 2016; Misra and Maaten, 2020), or inpainting (Pathak et al., 2016). Other tasks involve re-colorizing images (Zhang et al., 2016) or predicting image transformations (Gidaris et al., 2018).\nAmong these tasks, inpainting-based approaches have gathered significant interest thanks to the flexibility of the patch-based VQ representation (He et al., 2022; Hao et al., 2021). Recent advances in contrastive learning have shown that an image can be represented as a set of discrete visual tokens, and is conceptually rooted in the work of Oord et al. (2017). Vector Quantization (VQ) (Gray, 1984; Jegou et al., 2011) is used to map continuous image features into a finite set of learned codebook vectors. The resulting discrete representation allows applying transformers (Vaswani et al., 2017) to images, leading to powerful self-supervised models like dVAE (van den Oord et al., 2017), VQ-VAE (Oord et al., 2018) and VQ-GAN (Esser et al., 2021). More recently, DINO (Caron et al., 2021) introduced a self-distillation procedure to learn powerful visual representations without requiring negative samples. DINOv2 (Liao et al., 2022) further improved upon this approach by scaling up the model size and training dataset. Our work builds upon these advances, but differs in several key aspects. First, we leverage a large language model to provide supervisory signals for visual representation learning. Second, we introduce a novel distillation procedure that allows transferring the knowledge from the large language model to smaller vision models. Third, we demonstrate the effectiveness of our approach on a wide range of downstream tasks, including both dense and global tasks.\n\nOur approach is also related to the field of vision-language pre-training (VLP). Models like CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021) and FLAVA (Singh et al., 2022) learn joint embeddings of images and text by contrasting image-text pairs. These models have achieved impressive results on a variety of vision-language tasks, such as image classification, image retrieval and visual question answering. However, these models typically require large amounts of labeled data for training. In contrast, our approach learns visual representations without relying on explicit image-text labels. We instead leverage the knowledge encoded in a pre-trained language model to guide the learning process.",
    "5": "labeled images, showing impressive results. JFT also enabled significant performance gains for Kolesnikov\net al. (2020). In parallel, scaling was explored using a combination of supervised and unsupervised data. \nFor instance, an ImageNet-supervised model can be used to produce pseudo-labels for unsupervised data,\nwhich then serve to train larger networks (Yang et al., 2019). Subsequently, the availability of large\nsupervised datasets such as JFT also facilitated the adaptation of the transformer architecture to computer\nvision (Dosovitskiy et al., 2020). In particular, achieving performance comparable to that of the original\nvision transformer (ViT) without access to JFT requires substantial effort (Touvron et al., 2020; 2022). Due\nto the learning capacity of ViTs, scaling efforts were further extended by Zhai et al. (2022a), culminating in\nDarret et al. (2023) and Slotton (Wen et al., 2023).\n\nGiven the complexity of manually labeling large datasets, weakly-supervised training—where annotations are\nderived from metadata associated with images—provides an effective alternative to supervised training. Early\non, Joulin et al. (2016) demonstrated that a network can be pre-trained by simply predicting all words in the\nimage caption as targets. This initial approach was further refined by leveraging sentence structures (Li et al.,\n2017), incorporating other types of metadata and involve curation (Mahajan et al., 2018), and scaling (Singh\net al., 2022). However, weakly-supervised algorithms only reached their full potential with the introduction\nof contrastive losses and the joint-training of caption representations, as exemplified by Align (Jia et al.,\n2021) and CLIP (Radford et al., 2021).\n\nThis highly successful approach inspired numerous open-source reproductions and scaling efforts. Open-\nCLIP (Cherti et al., 2023) was the first open-source effort to replicate CLIP by training on the LAION\ndataset (Schuhmann et al., 2021), following works leverage pre-trained backbones by fine-tuning them\nin a CLIP-style manner (Sun et al., 2023; 2024). Recognizing that data collection is a critical factor in\nthe success of CLIP training, MetaCLIP (Xu et al., 2024) precisely follows the original CLIP procedure\nto reproduce its results, whereas Fang et al. (2023a) use supervised datasets to curate pretraining data.\nOther works focus on improving the training process, e.g. using sparse loss in SigLIP (Zeng et al., 2023),\nor leveraging a curriculum learning strategy (Yang et al., 2023b). Additionally, recent works explore\ncomponents that are essential for the success of CLIP, such as the importance of the text encoder (Kim et al.,\n2023c) or the learning of effective prompt engineering (Zhao et al., 2023a). Despite these advancements,\nreplicating the performance of CLIP remains challenging, and scaling to larger models often requires\nsubstantial computational resources (Radford et al., 2021).\n\nRecently, a new line of work has emerged that leverages self-supervised learning (SSL) to learn visual\nrepresentations without relying on manual annotations. Specifically, DINO (Carion et al., 2021) has shown\nremarkable results in various downstream tasks, demonstrating the effectiveness of SSL for visual representation\nlearning. DINOv2 (Carion et al., 2023) further improves upon DINO by incorporating a self-distillation\nprocedure and training on a larger and more diverse dataset. In this work, we build upon DINOv2 and explore\nthe impact of scaling the model size and training data on the performance of visual representation learning.\n\nOur main contributions are as follows:\n\n• We scale DINOv2 to a larger model size (6.7B parameters) and train it on a larger dataset (4096x4096\nresolution) with self-supervision.\n• We introduce a novel data curation technique that combines clustering and retrieval-based methods to\nselect high-quality training data.\n• We perform an extensive ablation study to analyze the impact of different training parameters and data\ncuration techniques on the performance of the model.\n• We demonstrate that our approach achieves state-of-the-art results on a variety of downstream tasks,\nincluding image classification, object detection, and semantic segmentation.",
    "6": "Figure 4: DINOv3 at very high resolution. We visualize dense features of DINOv3 by mapping the first\nthree components of a PCA computed over the feature space to RGB. To focus the PCA on the subject,\nwe mask the feature maps via background subtraction. With increasing resolution, DINOv3 produces crisp\nfeatures that stay semantically meaningful. We visualize more PCAs in Sec. 6.1.1.\n\nregion proposals but assume that such proposals exist a priori; this assumption is relaxed by approaches\nsuch as ODIN (Hénaff et al., 2022) and SlotCon (Wen et al., 2022). Without changing the training objective,\nDarret et al. (2023) show that adding register tokens to the input sequence greatly improves dense feature\nmaps, and recent works find this can be done without model training (Jiang et al., 2023; Chen et al., 2023).\nA recent trend are distillation-based, “agglomerative” methods that combine information from multiple\nimage encoders with varying in global and local feature quality, trained using different levels of supervi-\nsion (Ranzinger et al., 2023; Bolya et al., 2023). AM-RADIO (Ranzinger et al., 2024) combines the strengths\nof the fully-supervised SAM (Kirillov et al., 2023), the weakly-supervised CLIP, and the self-supervised Di-\nNOv2 into a unified backbone. The Perception Encoder (Bolya et al., 2023) similarly distills SAM(v2) into\na specialized dense variant called PEspatial. They use an objective enforcing cosine similarity between stu-\ndent and teacher patches to be high, where their teacher is trained with mask annotations. Similar losses\nwere shown to be effective in the context of style transfer, by reducing the inconsistency between the Gram\nmatrices of feature dimensions (Gatys et al., 2016; Johnson et al., 2016; Tao et al., 2024). In this work,\nwe adopt a Gram objective to regularize cosine similarity between student and teacher patches, favoring them\nbeing close. In our case, we use earlier iterations of the SSL model itself as the teacher, demonstrating that\nearly-stage SSL models effectively guide SSL training for both global and dense tasks.\n\nEarly-stage SSL models effectively guide SSL training for both global and dense tasks. Our architecture\nis based on the DINOv2 (Carion et al., 2023) framework, which in turn builds upon the DINO (Carion et al.,\n2021) architecture. DINOv2 introduces a self-distillation procedure, where a student network learns to predict\nthe output of a teacher network. The teacher network is an exponential moving average (EMA) of the student\nnetwork (He et al., 2020). This self-distillation procedure has been shown to improve the performance of SSL\nmodels (Hinton et al., 2015; Zhang et al., 2019; Tian et al., 2020; Carion et al., 2021). The DINOv2 architecture\nconsists of a ViT-Base backbone, a transformer encoder, and a prediction head. The transformer encoder\nprocesses the features extracted by the ViT-Base backbone, and the prediction head predicts the output of the\nteacher network. In this work, we extend the DINOv2 architecture to larger model sizes and higher resolutions.\n\nWe use a ViT-Giant backbone with 40 blocks and increase the embedding dimension to 1536. We also\nincrease the number of attention heads to 32. The prediction head consists of a two-layer MLP with a hidden\ndimension of 6144. We train the model using a self-distillation loss, which consists of a distillation loss and\na cross-entropy loss. The distillation loss measures the similarity between the output of the student network\nand the output of the teacher network. The cross-entropy loss measures the similarity between the predicted\nlabels and the ground truth labels. We train the model on a large dataset of unlabeled images, and we evaluate\nthe performance of the model on a variety of downstream tasks.",
    "7": "Table 1: Influence of training with clustering on features quality shown via performance on downstream tasks.\nWe compare datasets curated with clustering (Vo et al., 2023) and via retrieval (Oquab et al., 2023) to raw data\nfor a data mixture. This ablation study is run for a shorter schedule of 200k iterations.\n\n| Dataset | IN1k | IN1k Linear | ObjectNet | Naturalist 2021 | Paris Retrieval |\n|---|---|---|---|---|---|\n| Raw | 80.1 | 84.8 | 70.3 | 70.1 | 63.3 |\n| Clustering | 79.4 | 85.4 | 72.3 | 81.3 | 85.2 |\n| Retrieval | 84.0 | 86.7 | 70.7 | 86.0 | 82.7 |\n\nLVD-1689M (ours) | 84.6 | 87.2 | 72.8 | 87.0 | 85.9 |\n\ninherent to traditional supervised or task-specific approaches. In particular, SSL produces rich, high-quality\nvisual features that are not biased toward any specific supervision or task, thereby providing a versatile\nbase for a wide range of downstream applications. Recent attempts at scaling SSL models\nhave been hindered by issues of instability, high computational cost, and lack of clear scaling\nstrategies. We harness the benefits of scaling with the dataset creation procedure (Sec. 3.1), then present the first training phase of DINOv3 (Sec. 3.2).\nThis includes the choice of architecture, loss functions, and optimization techniques. The second training\nphase, focusing on dense features, will be described in Sec. 4.\n\n3.1 Data Preparation\n\nData scaling is one of the driving factors behind the success of large foundation models (Touvron et al.,\n2023; Radford et al., 2021; Xu et al., 2023; Oquab et al., 2023). Quality, and increasing naively the amount of\ntraining data does not necessarily translate into better model quality, and better performance on downstream\nbenchmarks (Goyal et al., 2021; Oquab et al., 2023). Successful data scaling efforts typically\ninvolve careful data curation pipelines. These algorithms may have different organizational efforts focusing on\nimproving data diversity and balance, or data usefulness—its relevance to common practical applications. For\nthe development of DINOv3, we combine two complementary approaches to improve both the generalization ability\nand performance of our model.\n\nData Collection and Curation. We build a large-scale image dataset by combining data from a variety of\nsources (see Sec. 3.1). The dataset consists of 1.689M images with a resolution of 4096x4096. The data\nis collected from a variety of sources, including ImageNet (Deng et al., 2009), LAION-400M (Schuhmann et al.,\n2021), and OpenImages (Kuznetsova et al., 2020). We use a combination of clustering and retrieval-based\nmethods to curate the dataset. Clustering is used to group similar images together, and retrieval is used to\nselect images that are representative of the overall dataset. We use k-means clustering to group images based\non their visual features. We use a pre-trained DINOv2 model to extract the visual features. We use a retrieval-\nbased method to select images that are representative of the overall dataset. We use a nearest neighbor search\nto find images that are similar to a set of query images. The query images are selected from a variety of\nsources, including ImageNet, LAION-400M, and OpenImages.\n\n3.2 Large-Scale Training with Self-Supervision\n\nSelf-supervised learning (SSL) has demonstrated interesting properties (Chen et al., 2020b; Caron et al.,\n2021), most SSL algorithms have not been scaled-up to larger model sizes. This is either due to issues with\ntraining stability, the optimization horizon, or overly-simple solutions that fail to capture the full complexity\nof the visual world. When trained at scale (Goyal et al., 2021), models tend to fall into necessarily\non curated data, making the opportunity to influence the relative position of the teacher harder to harness.\nAs a result, we capitalize on our data curation technique to leverage SSL. We train DINOv3 with the same\nobjective function as DINOv2 (Carion et al., 2021), but with a larger batch size of 8192. We use a learning\nrate of 1e-6 and a weight decay of 1e-4. We train the model for 1M iterations. We use a distributed data\nparallel (DDP) implementation to train the model on 64 GPUs.",
    "8": "Table 2: Comparison of the teacher architectures used in DINOv2 and DINOv3 models. We keep the model\nsize of the DINOv2 work. We also employ a custom variant of RoPE:\n\n| | DINOv2 | DINOv3 |\n|---|---|---|\n| Teacher model | | |\n| Backbone | ViT-giant | ViT-giant |\n| #Params | 1.1B | 6.7B |\n| #Blocks | 40 | 40 |\n| Pos. Embeddings | RoPE | RoPE |\n| Register | 4 | 4 |\n| Embed. Dim. | 1536 | 4096 |\n| FFN Type | SwiGLU | SwiGLU |\n| FFN Hidden Dim. | 4096 | 8192 |\n| Attn. Heads | 32 | 32 |\n| Attn. Heads Dim. | 64 | 128 |\n\nDINO Head MLP | 256k | 96k |\n\neither a homogeneous batch from ImageNet1k alone or a heterogeneous batch mixing data from all other\ncomponents. To assess the impact of our data curation technique, we perform an ablation study to\ncompare our data mix against datasets curated with clustering or retrieval-based methods alone, and the\nraw data pool. To each end, we train a model on this data, and compare their performance on standard\ndownstream tasks. For efficiency, we use a shorter schedule of 200k iterations instead of 1M iterations. In\nTab. 1, it can be seen that no single curation technique works best across all benchmarks, and that our full\npipeline allows us to obtain the best of both worlds.\n\nWhile models trained with self-supervised learning have demonstrated interesting properties (Chen et al.,\n2020b; Caron et al., 2021), most SSL algorithms have not been scaled-up to larger model sizes. This is either due to issues with\ntraining stability, the optimization horizon, or overly-simple solutions that fail to capture the full complexity\nof the visual world. When trained at scale (Goyal et al., 2021), models tend to fall into necessarily\non curated data, making the opportunity to influence the relative position of the teacher harder to harness.\nAs a result, we capitalize on our data curation technique to leverage SSL. We train DINOv3 with the same\nobjective function as DINOv2 (Carion et al., 2021), but with a larger batch size of 8192. We use a learning\nrate of 1e-6 and a weight decay of 1e-4. We train the model for 1M iterations. We use a distributed data\nparallel (DDP) implementation to train the model on 64 GPUs.\n\nOptimization. Training large models on very large datasets represents a complicated experimental work-\na priori, it is impossible to guess the optimal hyperparameter capacity and training data to assess a priori. \nTo get rid of all parameters simultaneously, we get rid of all parameters simultaneously. First, we continue training as a downstream performer. Second,\nwe continue to start hyperparameter tuning as a downstream task, and then continue to improve. Third,\nwe adopt proper hyperparameter tuning as a complicated task. The complexity of training large models\nrepresents a complicated experimental work. We use the AdamW optimizer (Loshchilov et al., 2017) with\na cosine learning rate schedule (Inan et al., 2016). We use a weight decay of 1e-4 and a momentum of 0.9.\nWe train the model for 1M iterations. We use a linear warmup period of 10k iterations. We use a batch size\nof 8192. We use a distributed data parallel (DDP) implementation to train the model on 64 GPUs. The\ncomputational cost of training DINOv3 is significant. It takes approximately 1 week to train the model on\nour cluster of 64 GPUs. However, the performance gains achieved by scaling the model size and training\ndata justify the computational cost. We also employ a custom variant of RoPE: the coordinate box [-1, 1]\nis randomly scaled to [-s, s], where s ∈ [0.5, 2]. Together, these changes enable DINOv3 to better learn\ndetail and robust visual features, improving its performance and scalability.",
    "9": "Figure 5: Evolution of the cosine similarities between the CLS and output patches in DINOv2 and DINOv3.\nAs training progresses, these similarities increase and the performance on dense tasks decreases.\n\n\n\na distributed implementation of Koleo in which the loss is applied in small batches of 16 samples—possibly\nacross GPUs. Our initial training loss is the following:\n\n𝐿𝑃𝑟𝑒 = 𝐿𝐷𝐼𝑁𝑂 + 𝐿𝐵𝑜𝑡 + 0.1 ∗ 𝐾𝐷 ∗ 𝐿𝐾𝐷𝑜𝑙𝑜𝑠𝑠.\n\nUpdated Model Architecture. For the model scaling aspect of this work, we increase the size of the\nmodel to the 6.7B parameters, and provide in the DINOv2 work. We also employ a custom variant of RoPE:\nthen applies a normalization [-1, 1] box to each patch. In order to improve the robustness of the teacher\noperation depending on the relative position of two patches. In order to improve the multi-head attention\noperation depending on the relative position of two patches. In order to improve the coordinate box [-1, 1]\nis randomly scaled to [-s, s], where s ∈ [0.5, 2]. Together, these changes enable DINOv3 to better learn\ndetail and robust visual features, improving its performance and scalability.\n\nOptimization. Training large models on very large datasets represents a complicated experimental work-\na priori, it is impossible to guess the optimal hyperparameter capacity and training data to assess a priori. \nTo get rid of all parameters simultaneously, we get rid of all parameters simultaneously. First, we continue\ntraining as a downstream performer. Second, we continue to start hyperparameter tuning as a downstream\ntask, and then continue to improve. Third, we adopt proper hyperparameter tuning as a complicated task.\nThe complexity of training large models represents a complicated experimental work. We use the AdamW\noptimizer (Loshchilov et al., 2017) with a cosine learning rate schedule (Inan et al., 2016). We use a weight\ndecay of 1e-4 and a momentum of 0.9. We train the model for 1M iterations. We use a linear warmup period\nof 10k iterations. We use a batch size of 8192. We use a distributed data parallel (DDP) implementation\nto train the model on 64 GPUs. The computational cost of training DINOv3 is significant. It takes\napproximately 1 week to train the model on our cluster of 64 GPUs. However, the performance gains\nachieved by scaling the model size and training data justify the computational cost.",
    "10": "Figure 6: Evolution of the cosine similarity between the patch noted and the similarity maps becomes noisier.\n\n4 Gram Anchoring: A Regularization for Dense Features\nTo fully leverage the benefits of large-scale training, we aim to train the 7B model for an extended duration, with the notion that it could potentially train indefinitely. As expected, prolonged training leads to improvements on global benchmarks. However, as training progresses, the performance degrades on dense tasks (Figs. 5b and 5c). This phenomenon, which is due to the emergence of patch-level inconsistencies in feature representations, undermines the interest behind extended training. In this section, we first analyze the root of patch-level consistency, then propose a new objective to mitigate it, called Gram anchoring. We finally discuss the impact of our approach on both training stability and model performance.",
    "11": "During extended training, we observe consistent improvements in global metrics but a notable decline in performance on dense prediction tasks. This behavior was previously observed, to a lesser extent, during the training of DINOv2, and also discussed in the scaling effort of Fan et al. (2023). However, to the best of our knowledge, it remains unresolved to date. We illustrate the phenomenon in Figs. 5b and 5c, which present the performance of the model across iterations on both image classification and segmentation tasks. For segmentation, we train a linear classifier on patch features using the CLS token and report top-1 accuracy. For classification, we train a linear layer on patch features extracted from Pascal VOC and report mean Intersection over Union (mIoU). We observe that for the ViT-g and the ViT-B, the classification accuracy monotonically improves throughout training. However, segmentation performance declines in both cases after approximately 200k iterations, falling below its early values. \nTo better understand this degradation, we analyze the quality of patch features by visualizing cosine similarities between patches. Fig. 6 shows the cosine similarity maps between the backbone’s output patch features and a reference patch (highlighted in red). At 200k iterations, the similarity maps are smooth and well-localized, indicating consistent patch-level representations. However, by 600k iterations and beyond, the maps degrade substantially, with an increasing number of irrelevant patches with high similarity to the reference patch. This loss of patch-level consistency correlates with the decline in dense task performance. \nThese patch-level irregularities differ from the high-level class features described in Darcet et al. (2023), specifically, when irregularities stem from the inability to learn patch representations. We hypothesize that the global optimization process, while effective at learning discriminative features for classification tasks, fails to preserve the local consistency of patch-level representations, leading to the observed performance drop on dense tasks.",
    "12": "Figure 7: Evolution through the training iterations of the patch-level iBOT loss, the global loss DINO (applied to the global crops) and the newly introduced Gram loss. We highlight the iterations of the refinement step CRef that uses the Gram objective.\n\ndrop on dense tasks. While combining the global DINO loss with the local iBOT loss has begun to address this issue, we observe that the balance is unstable, with global representation dominating as training progresses.\n4.2 Gram Anchoring Objective\nThroughout our experiments, we have identified a relative independence between learning strong discriminative features and maintaining local consistency. By operating on the Gram teacher, we highlight this independence. \nRegarding dense properties, we observe that the Gram matrix rather than the feature themselves, exhibits superior dense properties. By operating on the Gram matrix rather than the feature themselves, we incorporate this insight, which explicitly leverages this independence.\nBuilding on this insight, we propose a new solution that degradation of patch-level consistency. We introduce a new objective which mitigates the degradation of patch-level consistency by enforcing the quality of the patch-level consistency, without impacting the features themselves. This new loss function operates on the Gram matrix: the matrix of all pairwise dot products of patch features in an image. We want to push the Gram teacher by taking an earlier iteration of the teacher network, which exhibits superior dense properties. The input resolution of the student teacher is 256x256. Let us denote by Xc the student matrix, and free only after 1M iterations. Interestingly, we observe that the late application of CRef manages to “repair” very degraded local features. In order to further improve performance, we have still Gram teacher to revert 10k iterations. Interestingly, we observe that the implementation of CRef, the Gram teacher manages to revert 10k iterations. We observe that the implementation of CRef, the Gram teacher manages to revert 10k iterations. We observe that the implementation of CRef, the Gram teacher manages to revert 10k iterations. \nWe define the loss Cgram as follows:\nLgram = ||Xs Xc - Xc Xc||F\nwhere Xs and Xc are the Gram matrices of the student and teacher networks, respectively. We only compute this loss on the global crops. Even though it can be applied early on during the training, for efficiency, we start only after 1M iterations.",
    "13": "Figure 8: Evolution of the results on different benchmarks after applying our proposed Gram anchoring method. We visualize results when continuing the original training with our refinement step, CRef. We also plot results obtained when using higher-resolution features for the Gram objective.\n\n4.3 Leveraging Higher-Resolution Features\nRecent work shows that a weighted average of patch features can yield stronger local representations by smoothing outlier patches and enhancing patch-level consistency (Wysoczańska et al., 2021). On the other hand, feeding higher-resolution images into the backbone produces finer and more detailed feature maps. We leverage the benefits of both observations to compute high-quality features for Gram teacher. Specifically, we first input images at twice the normal resolution to compute high-Gran teachers, then 2x down-sample the resulting feature maps with the bi-cubic interpolation to achieve the same resolution of the student output. Fig. 9a visualizes the Gram matrix of patch features obtained with images at resolutions of 256 and 512, as well as those obtained after 2x down-sampling consistency from the higher-resolution images into smaller ones, and (b) the quantitative improvements brought by varying the training iteration and the resolution of the Gram teacher.\nAdditionally, longer training further benefits performance on the ObjectNet benchmark and other global benchmarks show mild impact from the new loss.",
    "14": "Figure 9: (a) Gram matrices at different input resolutions. (b) Qualitative and quantitative study of the impact of high-resolution Gram. We show (a) the improvements come after down-sampling the high-resolution maps into smaller ones, and (b) the quantitative improvements brought by varying the training iteration and the resolution of the Gram teacher.\n\nFigure 10: Qualitative effect of Gram anchoring. We visualize cosine maps before and after using the refinement objective CRef. The input resolution of the student teacher is 1024 x 1024 pixels.\n\n5 Post-Training\nThis section presents post-training steps, including a high-resolution adaptation phase enabling effective inference at different input resolutions (Sec. 5.1), model distillation producing quality and efficient smaller-sized models (Sec. 5.2), and text alignment adding zero-shot capabilities to DINOv3 (Sec. 5.3).\n5.1 Resolution Scaling\nFor a contemporary computer vision system, which was trained with resolution 224 and patch size 14. However, many contemporary computer vision systems, such as DINOv2, and effectiveness. For a patch size of 16, this setup leads to a good trade-off between speed and effectiveness. We consider global crop sizes from (112, 168, 224, 236) and local crops per mini-batch. Specifically, we consider global crop sizes from (512, 768) and local crop sizes from (112, 168, 224, 236).\nSimilar to the main training, a key component of this high-resolution adaptation phase is the addition of it, the model performance on dense and robust feature correlations across spatial locations, which is crucial when dealing with the increased complexity of high-resolution inputs.\nEmpirically, we observe that this relatively brief but targeted high-resolution step substantially enhances the overall model’s quality and allows it to generalize across a wide range of input sizes, as shown visually in Fig. 11. In Fig. 11, we compare our 7B model before and after adaptation. We find that resolution scaling leads to a small gain on ImageNet TB scale, while drastically improving performance on dense tasks. However, in ObjectNet (b) and DAVIS (c) the performance tends to degrade slightly for dense tasks, which we attribute to the increased complexity of high-resolution inputs. Additionally, the performance on ADE20k (a) improves consistently as the resolution increases. Fig. 11(d) displays the improvement in segmentation tracking on DAVIS at different resolutions, applied OOD to ObjectNet, (c) linear semantic segmentation on ImageNet, (b) applied OOD to ObjectNet (a), and (d) DAVIS.",
    "15": "Figure 11: Effect of high resolution adaptation. Results before ('Pre-HR') and after ('Post-HR') resolution segmentation on TrackingNet, (b) applied OOD to ObjectNet, (c) linear semantic segmentation on ImageNet, and (d) DAVIS.\napplications require processing images at significantly higher resolutions, often 512 × 512 pixels or greater, and varies depending on specific use cases. To ensure this high-resolution adaptation step, we utilized mixed resolution sampling, sampling differently-sized pairs of global and local crops per mini-batch. Specifically, we consider global crop sizes from (512, 768) and local crop sizes from (112, 168, 224, 236); and consider global crop sizes from (112, 168, 224, 236)",
    "16": "of work has focused on improving the quality of CLIP (Radford et al., 2021), which originally learned only a global alignment, its focus on global features limits its ability to capture fine-grained, localized correspondences. More recent works (Zhou et al., 2022b) have shown that effective image-text alignment can be achieved with pre-trained self-supervised visual backbones and This makes it possible to leverage these powerful models in multi-modal settings, facilitating richer and more precise text-to-image associations that extend beyond global semantics while also reducing computational costs, since the visual encoding is already learned.\n\nWe align a text encoder with our DiNOv3 model by adopting the training strategy previously proposed in Jose et al. (2023). This approach follows the LIT training paradigm (Zhai et al., 2022a), training a text representation from scratch to match images to their captions with a contrastive objective, while keeping the vision encoder frozen. To allow for some flexibility on the vision side, two transformer layers are introduced on top of the frozen visual backbone. A key enhancement of this method is the concatenation of the mean-pooled patch embeddings with the output CLS token before matching to the text embeddings. This enables aligning both global and local visual features to text, leading to improved performance on dense prediction tasks without requiring additional heuristics or tricks. Furthermore, we use the same data curation protocol as established in Jose et al. (2023) to ensure consistency and comparability.\n\n6 Results\n\nIn this section, we evaluate our flagship DiNOv3 7B model on a variety of computer vision tasks. Throughout our experiments, unless otherwise specified, we keep DiNOv3 frozen and solely use its representations. We demonstrate that with DiNOv3, fine-tuning is not necessary to obtain strong performance. This section is organized as follows. We first probe the quality of DiNOv3’s sense (Sec. 6.1) and global (Sec. 6.2) image representations using lightweight evaluation protocols and compare it to the strongest available vision encoders. We show that DiNOv3 learns exceptional dense features while offering robust and versatile global image representations. Then, we consider DiNOv3 as a basis for developing more complex computer vision systems (Sec. 6.3). We show that little effort on top as DiNOv3, we are able to achieve state-of-the-art results with or exceeding the state of the art in tasks as diverse as object detection, semantic segmentation, and visual estimation.\n\n6.1 DiNOv3 provides exceptional dense features\n\nTo evaluate the quality of DiNOv3’s dense features, we employ a standard linear protocol (Zhou et al., 2021a). Specifically, we freeze the weights of DiNOv3 and train a linear classifier on top of the patch embeddings. We evaluate on PASCAL VOC (Everingham et al., 2010) for semantic segmentation and ADE20K (Zhou et al., 2019) for scene parsing. We report the mean Intersection-over-Union (mIoU) as the primary metric. We compare DiNOv3 to several strong vision encoders, including CLIP (Radford et al., 2021), DINO (Carion et al., 2021), and Swin (Liu et al., 2021). The results are shown in Table 1.\n\nTable 1: Semantic segmentation and scene parsing performance using a linear protocol. Results are reported as mIoU (%).\n\n| Method | PASCAL VOC | ADE20K |\n|---|---|---|\n| CLIP (BEiT) | 49.7 | 33.2 |\n| DINO (BEiT) | 54.2 | 37.8 |\n| Swin-T | 57.1 | 41.3 |\n| DiNOv3-7B | 61.8 | 45.1 |\n\nAs shown in Table 1, DiNOv3 significantly outperforms all other methods on both datasets. DiNOv3 achieves a 61.8% mIoU on PASCAL VOC and a 45.1% mIoU on ADE20K, demonstrating its superior dense feature representation capabilities. This improvement is likely due to DiNOv3’s self-distillation training objective, which encourages the model to learn more discriminative and informative features.",
    "17": "Figure 13: Comparison of dense features. We compare several vision backbones by projecting their dense outputs using PCA and mapping them to RGB. From left to right: SigLIP 2 ViT-g/16, PESpatial ViT-g/14, DINOv2 ViT-g/14 with registers, DINOv3 ViT-TB/16. Images are forwarded at resolution 1280x960 for models using patch 16 and 1120x840 for patch 14, i.e. all feature maps have size 80x60.\n\ncomponents and colors (six variants), we explore all combinations and report the visually most compelling one. The resulting visualization is shown in Fig. 13. Compared to other vision backbones, it can be seen that the features of DINOv3 are sharper, containing much less noise, and showing superior semantical coherence.\n\n6.1.2 Dense Linear Probing\n\nWe perform linear probing on top of the dense features for two tasks: semantic segmentation and monocular depth estimation. In both cases, we train a linear transform on top of the frozen patch outputs of DINOv3. For semantic segmentation, we evaluate on the ADE20k (Zhou et al., 2017), Cityscapes (Cordts et al., 2016), and PASCAL VOC 2012 (Everingham et al., 2012) datasets and report the mean intersection-over-union\n\n18",
    "19": "Table 4: Evaluation of 3D consistency of dense representations. We estimate 3D keypoint correspondences across views following the evaluation protocol of Probe3D (Baran et al., 2021). To measure performance, we report the correspondence recall, i.e. the percentage of correspondences falling into a specified distance.\n\n| Method | Geometric | Semantic |\n|---|---|---|\n| Agglomerative backbones |  |  |\n| AM-RADiO2.5 | g/14 | 59.4 | 56.8 |\n| PESpatial | g/14 | 53.8 | 49.6 |\n| Weakly-supervised backbones |  |  |\n| SigLIP 2 | g/16 | 49.4 | 42.6 |\n| PEcore | g/14 | 39.9 | 23.1 |\n| Self-supervised backbones |  |  |\n| France | g/14 | 54.6 | 51.0 |\n| DiNOv2 | g/14 | 60.1 | 56.4 |\n| Web-DiNO | 7B/14 | 55.0 | 32.2 |\n| DiNOv3 | 7B/16 | 64.4 | 58.7 |\n\ncorrespondence on the SPair dataset (Min et al., 2019), and measure performance with correspondence recall in both cases. Please refer to App. D.3 for more experimental details.\n\nResults (Tab. 4). For geometric correspondences, DiNOv3 outperforms all other models and improves over the second best model (DiNOv2) by 4.3% recall. Other SSL scaling endeavors (France and WebSSL) lag behind DiNOv2, showing that it is still a strong baseline. Weakly-supervised models (PEcore and SigLIP 2) do not fare well on this task, indicating a lack of 3D awareness. Models with SAM distillation, AM-RADiO nearly reaches the performance of DiNOv2, but PESpatial still lags behind (~1.16% recall), and even falls behind France (~0.8% recall). This suggests that self-supervised learning is a key component for strong performance on this task. For semantic correspondences, the same conclusions apply. DiNOv3 performs best, outperforming both its predecessor (+2.6% recall) and AM-RADiO (+1.9% recall). Overall, these impressive performance on keypoint matching are very promising signals for downstream use of DiNOv3 in other 3D-heavy applications.\n\n6.1.4 Unsupervised Object Task\n\nPowerful self-supervised vision models can leverage object instances in images without requiring any annotations via e.g. instance segmentation or object detection tasks. In this section, we aim to evaluate how well DiNOv3 learns about object shapes. To do so, we use the 3D-MOT benchmark (Bewley et al., 2016) for multi-object tracking. The task consists of tracking multiple objects in a monocular video sequence. We focus on the task of identifying the same object across different frames, i.e. association. We evaluate the learned representations by computing the average recall (AR) at different Intersection-over-Union (IoU) thresholds. We compare DiNOv3 to several baselines, including SimCLRv2, MoCo v3, and DINO. We use the 7B/16 version of DiNOv3 for this evaluation. Results are shown in Tab. 5. DiNOv3 significantly outperforms all baselines across all IoU thresholds. Specifically, DiNOv3 achieves an AR of 56.7% at an IoU threshold of 0.5, which is 5.8% higher than the second best model (DINO). This demonstrates that DiNOv3 learns robust object representations that are well-suited for downstream tasks such as multi-object tracking. The improvement is even more pronounced at higher IoU thresholds, suggesting that DiNOv3 is better at distinguishing between different object instances. This is likely due to the improved 3D consistency of the learned representations, which allows DiNOv3 to better capture the shape and pose of objects. Further ablation studies on the impact of the different components of DiNOv3 are provided in App. D.4.",
    "20": "Method\tVIT\tVOC07\tVOC12\tCOCO\nAgglomerative backbones\nAM-RadioV2.5\tg/14\t55.0\t59.7\t45.9\nFESpatial\tg/14\t51.2\t56.0\t43.9\nWeakly-supervised backbones\nSigLIPv2\tg/16\t20.5\t24.7\t18.6\nPEcore\tg/14\t14.2\t18.2\t13.5\nSelf-supervised backbones\nDINO\tS/16\t61.1\t66.0\t48.7\nDINO\tB/16\t60.1\t64.4\t50.5\nDINOv2\tg/14\t55.6\t60.4\t45.4\nDINOv2\tg/16\t26.1\t29.7\t20.9\nWeb-DINO\t7B/16\t66.1\t69.5\t55.1\n\nFigure 14: Unsupervised object discovery. We apply TokenCut (Wang et al., 2022c) on the output patch features of different backbones and report CorLoc metric. We also visualize predicted masks obtained with DINOv3 (red overlay on input images at res. 1024), obtained with no annotation and no post-processing.\nDINOv3’s dense features are both semantically strong and well localized. We believe that this will pave the way for more class-agnostic object detection approaches, especially in scenarios where annotations are costly or unavailable, and where the set of relevant classes is not confined to a predefined subset.\n\n6.1.5 Video Segmentation Tracking\nBeyond the static images, an important property of visual representations is their temporal consistency, i.e. whether the features evolve in a stable manner through time. To test for this property, we evaluate DINOv3 on the task of video segmentation tracking: given ground-truth instance segmentation masks in the first frame of a video, the goal is to propagate these masks to subsequent frames. We use the DAVIS 2017 (Pont-Tuset et al., 2017), YouTube-VOS (Xu et al., 2018), and MOSE (Ding et al., 2023) datasets. We evaluate performance using the standard J&F-mean metric, which combines region similarity (J) and contour accuracy (F) (Perazzi et al., 2016). Following Jabri et al. (2020), we use a near-online segmentation algorithm that consists of the similarity at the current frame to propagate the masks from the previous frame. Specifically, we use the Hungarian algorithm to match the predicted masks to the ground truth masks at the first frame (t=0) and propagate the corresponding instance IDs to subsequent frames. We report results for DINOv3 as well as for comparison with state-of-the-art trackers.\n\nTable 15: DAVIS 2017, YouTube-VOS 2018, and MOSE 2023 J&F-mean scores. We compare DINOv3 with several baselines and state-of-the-art trackers. The best results are highlighted in bold. We report the performance of DINOv3 with different backbone sizes (g/14, g/16, B/16, S/16). We also report the performance of DINOv2 with g/14 backbone. \n\n",
    "22": "Table 6: Video classification evaluation using attentive probes. We report top-1 accuracy on UCF101,\nSomething-Something V2 (SSv2), and Kinetics-400 (K400). For each model, we report performance for\nevaluating a single clip per video, or applying test-time augmentation (TTA) by averaging the predicted\nprobabilities from multiple clips.\n\n| Method | UCF101 | SSv2 | K400 |\n|---|---|---|---|\n| Agglomerative backbones | | | |\n| AN-RADIOv2.5 | g/14 | 92.8 | 92.5 | 69.1 | 70.0 | 84.8 | 85.2 |\n| PESpatial | g/14 | 92.7 | 92.8 | 66.4 | 68.4 | 83.5 | 84.8 |\n| Weakly-supervised backbones | | | |\n| SigLIP 2 | g/16 | 93.6 | 94.2 | 68.8 | 70.2 | 86.9 | 87.7 |\n| PRecore | G/14 | 93.1 | 93.3 | 69.0 | 70.4 | 87.9 | 88.5 |\n| Self-supervised backbones | | | |\n| DINOv2 | g/16 | 93.5 | 93.8 | 67.4 | 68.4 | 84.4 | 85.6 |\n| V-JEPA 2 | g/16 | 94.0 | 93.5 | 73.8 | 75.3 | 85.3 | 84.3 |\n| Web-DINO | 7B/14 | 93.9 | 94.1 | 67.3 | 68.1 | 86.8 | 87.2 |\n| DINOv3 | 7B/16 | 93.5 | 93.5 | 70.1 | 70.8 | 87.8 | 88.2 |\n\n6.2 DINOv3 is Robust and Versatile Global Image Descriptors\n\nIn this section, we evaluate DINOv3’s ability to capture global image statistics. To this end, we consider clas-\nsic classification tasks (Sec. 6.2.1) and instance retrieval (Sec. 6.2.2). Again, we compare to the strongest publicly available image encoders. In addition to the models from the\nprevious section, we evaluate the two weakly supervised models AIMv2 (TTA) by Sun et al. (2024), trained using\njoint auto-regressive pixel and text prediction, and the massive EVA-CLIP-18B (Sun et al., 2024).\n\n6.2.1 Image Classification with Linear Probing\n\nWe train a linear classifier on top of DINOv3 features extracted from the training set of various datasets\n(described in Sec. 6.2.3). We report top-1 accuracy on the test set. Table 7 details the results on CIFAR-10,\nSTL-10, and ImageNet-1K. As can be seen, DINOv3 outperforms all other models, even substantially on\nImageNet-1K. Notably, DINOv3 7B/16 achieves 86.0% top-1 accuracy, surpassing EVA-CLIP-18B by 0.8%.\n\n6.2.2 Image Retrieval\n\nWe evaluate DINOv3’s ability to capture global image statistics by measuring its performance on image re-\ntrieval. Given a query image, we retrieve the most similar images from a database. We use the top-1 retrieval\naccuracy as the evaluation metric. We consider the standard retrieval tasks on CIFAR-10, STL-10, and ImageNet-\n1K. Table 8 details the results. DINOv3 consistently outperforms all other models, achieving state-of-the-art\nperformance. For example, DINOv3 7B/16 achieves 77.6% top-1 retrieval accuracy on ImageNet-1K, surpassing\nEVA-CLIP-18B by 2.1%.\n\n6.2.3 Implementation Details\n\nFor linear probing, we use a standard linear layer with a weight decay of 1e-5. We train the linear layer using\nthe AdamW optimizer with a learning rate of 1e-3 and a batch size of 256. We train for 100 epochs. For image\nretrieval, we extract features for all images in the database and query images using DINOv3. We then compute\nthe cosine similarity between the query image feature and all database image features. We retrieve the top-k most\nsimilar images and compute the top-1 retrieval accuracy. We use a batch size of 64 for feature extraction.",
    "24": "We find that, again, DiNOv3 surpasses all previous SSL methods. It also shows competitive results compared to weakly-supervised methods, indicating its robustness and generalization capability across diverse finegrained classification tasks. Notably, DiNOv3 attains the highest accuracy on the difficult iNaturalist21 dataset at 89.8%, outperforming even the best weakly-supervised model PEcore with 87.0%.\n\n6.2.2 Instance Recognition\n\nTo evaluate the instance-level recognition capabilities of our model, we adopted a non-parametric retrieval approach. Here database images are ranked by their cosine similarity to a given query image, using the output CLS token. We benchmark performance across several datasets: the Oxford and Paris datasets for landmark recognition (Radenović et al., 2019); the Met dataset featuring artworks from the Metropolitan Museum (Ypsilanti et al., 2021), and Amsterdam (Yildiz et al., 2022). Retrieval of modern street view images matched to historical archival images of Amsterdam (Yildiz et al., 2022). Consistency of effectiveness is quantified using mean average precision for Oxford, Paris, and Amsterdam, and global average precision for Met. See App. D.8 for more evaluation details.\n\nResults (Tabs. 9 and 23) Across all evaluated benchmarks, DiNOv3 achieves the strongest performance by large margins, e.g. improving over the second best model DiNOv2 by +10.8 points on Met and +7.6 points on Amsterdam. On this benchmark, weakly-supervised models are lagging far behind DiNOv3, with the exception of AM Radio, which is distilled from DiNOv2 features. These findings highlight the robustness and versatility of DiNOv3 for instance-level retrieval tasks, spanning both traditional landmark datasets and more challenging domains such as art and historical image retrieval.\n\n6.3 DiNOv3 is a Foundation for Complex Computer Vision Systems\n\nThe previous two sections already provided solid signal for the quality of DiNOv3 in both dense and global tasks. However, these results were obtained under “model probing” experimental protocols, while such simple evaluations fall short to remove confounding factors from involved experiments in larger computer vision system. Thus, in this section, we depart from the lightweight protocols, and instead, train more involved downstream detectors and consider stronger task-specific baselines. In particular, we use DiNOv3 (basis for (1) object detectors with deformable-DETR (Zhu et al., 2020), (2) instance segmentation models with Mask2Former (Cheng et al., 2021), and (3) a visual foundation model with SAM (Kirillov et al., 2023). We compare DiNOv3 with (1) ImageNet-1k supervised pre-training, (2) DINO pre-training, and (3) a randomly initialized (Rand) backbone. We report results on COCO (Lin et al., 2014) for object detection and instance segmentation, and on SA-1B (Kirillov et al., 2023) for SAM-based tasks.\n\n6.3.1 Object Detection and Instance Segmentation\n\nTable 24 shows that DiNOv3 consistently outperforms ImageNet-1k and DINO pre-training across all metrics and datasets. For example, DiNOv3 improves the AP score by 1.8-2.6 points on COCO object detection, and by 2.1-2.9 points on COCO instance segmentation. These gains are significant, demonstrating the effectiveness of DiNOv3 as a strong visual foundation for dense prediction tasks. Notably, DiNOv3 consistently outperforms DINO, even though DINO has been specifically designed for self-supervised learning. This suggests that the architectural improvements and training procedure of DiNOv3 are crucial for achieving state-of-the-art performance.",
    "25": "Table 10: Comparison with state-of-the-art systems on object detection. We train a detection adapter on top of a frozen DINOv3 backbone. We show results on the validation set of the COCO and COCO-O datasets, and report the mAP across IoU thresholds, as well as the effective robustness (ER). Our detection system based on DINOv3 sets a new state of the art. As the InternImage-G detection model has not been released, we were unable to reproduce their results or compute COCO-O scores.\n\n| Model | Detector | FT | Encoder | Decoder | Trainable | COCO | COCO-O | ER |\n|---|---|---|---|---|---|---|---|---|\n| EVA-02 | Cascade | 300M | 300M | 64.1 | 65.3 | 34.7 |\n| InternImage-G | DINO | 6B | 6B | 65.1 | 65.3 | - |\n| EVA-02 | Co-DETR | 300M | 300M | 65.4 | 65.9 | 63.7 | 34.3 |\n| DETR | DETR | 1.9B | 50M | 2B | 65.3 | 66.0 | 64.0 | 34.7 |\n| DINOv3 | Plain-DETR | * | 7B | 100M | 100M | **65.6** | **66.1** | **66.4** | **36.8** |\n\nImplementation We build upon the Plain-DETR (Lin et al., 2023b), but make the following modification: We do not fuse the transformer encoder into the backbone, but keep it as a separate module, similar to the original DETR (Carion et al., 2020), which allows us to keep the DINOv3 backbone completely frozen during training and inference. To the best of our knowledge, this makes it the first competitive detection model to use a frozen backbone. We train the Plain-DETR detector on Objects365 for 22 epochs at resolution 1536, then one epoch at resolution 2048, followed by 12 epochs on COCO at resolution 2048. At inference time, we run at resolution 2048. Optionally, we also apply test-time augmentation (TTA) by forwarding the image at multiple resolutions (from 1536 to 2880). See App. D.9 for full experimental details.\n\nResults (Tab. 10) We compare our system with state-of-the-art: EVA-02 (with a Cascade detector (Fang et al., 2024b)), EVA-02 with Co-DETR (Zong et al., 2023), InternImage-G (with DINO backbone), and Plain-DETR (Lin et al., 2023b). Both DINOv3 with Plain-DETR outperform all other parameters, outperforming the previous state-of-the-art by 0.2-0.3 mAP on COCO (Tab. 10). Furthermore, DINOv3 is significantly more robust to distribution shift, as evidenced by the 2.1 percentage point improvement in ER (Tab. 10). Notably, DINOv3 achieves these results with a significantly smaller number of trainable parameters (100M vs. 300M or 1.9B).",
    "26": "Table 11: Comparison with state-of-the-art systems for semantic segmentation on ADE20k. We evaluate DINOv3 with the transformer encoder from ViT-Adapter architecture, and report mIoU scores. ONE-PEACE and DINOv3 use the same decoder parameters taken into account both. We report results on further datasets in Tab. 24\n\n| Model | FT | Encoder | Decoder | Trainable | mIoU |\n|---|---|---|---|---|---|\n| BEIT3 | ▲ | 1.0B | 550M | 1.6B | 62.0 | 62.8 |\n| ONE-PEACE | ▲ | 1.1B | 230M | 1.3B | 62.5 | 62.9 |\n| DINOv3 | * | 7B | 927M | 927M | **62.6** | **63.0** |\n\nsegmentation decoder on COCO-Stuff for 80k iterations, followed by 10k iterations on Hypersim (Roberts et al., 2021). Finally, we train for 20k iterations on the training split of ADE20k and report results on the validation split. All training is done at an input resolution of 896. At inference time we consider two setups: single-scale, i.e. we forward images at training resolution, or multi-scale, i.e. we average predictions over multiple image ratios between ×0.9 and 1.1 the original training resolution. We refer to App. D.10 for more experimental details.\n\nResults (Tab. 11) We compare our model’s performance with several state-of-the-art baselines, including BEIT-3 (Wang et al., 2022b), InternImage-H (Wang et al., 2023b) and ONE-PEACE (Everingham et al., 2023) and report results on additional datasets in Tab. 24. Our segmentation model based on the frozen DINOv3 backbone performance, equaling that of the relative depth estimation (Everingham et al., 2023). It also improves over all prior models on the COCO-Stuff dataset (Cesar et al., 2018) and ViT-32 (Baevski et al., 2022). As semantic segmentation requires accurate per-pixel predictions, vision transformer backbones pose a fundamental problem. Indeed, the 16× pixel-wise input matches make the granularity of the prediction can obtain high-resolution feature maps, even at very deep layers. With our approach we are able to obtain a comparable result to state-of-the-art with a significantly smaller number of trainable parameters (927M vs. 1.3B or 1.6B). Furthermore, DINOv3 with ViT-Adapter is 0.7% and 4% better than ONE-PEACE and BEIT-3 respectively on ADE20k validation set.",
    "27": "Table 12: Comparison with state-of-the-art systems for relative monocular depth estimation. By combining DINOv3 with Depth Anything DINOv3 as the image feature extractor in a SfM pipeline, we obtain state-of-the-art results on various depth estimation tasks. We report the standard metrics absolute relative error (AREl) (lower is better) and δ1 (higher is better). We refer to Yang et al. (2024b) for a description of those metrics.\n\n| Method | FT | AREl ↓ | δ1 ↑ | AREl ↓ | δ1 ↑ | AREl ↓ | δ1 ↑ | AREl ↓ | δ1 ↑ |\n|---|---|---|---|---|---|---|---|---|---|\n| MiDaS | - | 9.0 | 91.6 | 14.9 | 63.0 | 18.4 | 75.2 | 12.1 | 84.6 |\n| LeReS | - | 7.4 | 94.5 | 11.7 | 78.4 | 16.6 | 77.8 | 9.1 | 91.7 |\n| Omnidata | - | 7.8 | 94.3 | 10.0 | 83.5 | 16.6 | 77.8 | 7.5 | 93.6 |\n| DPT | - | 9.8 | 90.3 | 10.1 | 78.8 | 17.8 | 94.6 | 8.2 | 93.4 |\n| Marigold | - | 5.5 | 96.4 | 9.9 | 91.6 | 13.7 | 86.5 | 6.4 | 95.1 |\n| Dav2 (ViT-g) | - | 5.2 | 97.9 | 7.5 | 94.7 | 13.1 | 86.5 | 6.4 | 95.1 |\n\nResults (Tab. 12) We compare our model’s performance with state-of-the-art baselines: MiDaS (Ranftl et al., 2020), LeReS (Yin et al., 2021), Omnidata (Eftekhar et al., 2021), DPT (Ranftl et al., 2021), Marigold (Hu et al., 2023) and Dav2 (Umang et al., 2024). Our depth estimation model reaches a new state-of-the-art on the ensemble version (Ke et al., 2023) in AREl on DiODE compared to DPT. Remarkably, it is possible using a frozen backbone, whereas all other baselines need to finetune the backbone for depth estimation. In addition, this validates that DINOv3 enables transfer to downstream applications, a desirable property that we highlight the possibility for finetuning the backbone to further improve the results.",
    "28": "Table 13: 3D understanding using Visual Geometry Grounded Transformer (VGGT) (Wang et al., 2023). Camera pose estimation results are reported with AUC@30. Multi-view estimation on DTU. (E) View matching on ScanNet-150.\n\n| Method | Re10K | CO3Dv2 | Acc | Comp. | Overall | AUC@30 |\n|---|---|---|---|---|---|---|\n| DUS3DR | 67.7 | 81.8 | - | - | - | - |\n| MAS3R | 75.9 | 83.4 | 0.437 | 0.373 | 0.405 | 33.8 |\n| VG-CSM v2 | 78.8 | 85.6 | 0.469 | 0.403 | 0.436 | 40.5 |\n| CUT3R | 75.3 | 83.3 | 0.267 | 0.329 | 0.298 | 27.1 |\n| DUS3DR | 78.8 | 85.3 | 0.567 | 0.485 | 0.526 | 31.8 |\n| FLARE | 85.3 | 88.2 | 0.389 | 0.361 | 0.375 | 55.2 |\n| VGGT | **86.3** | **89.6** | 0.503 | 0.458 | 0.481 | **56.1** |\n\ngether with the previous results on correspondence estimation (Sec. 6.1.3) and depth estimation (Sec. 6.1.4) we anticipate further empirical evidence for the strong suitability of DINOv3 as a basis for 3D tasks.\n\n7 Evaluating the Full Family of Models\n\nIn this section, we provide a comprehensive evaluation of DINOv3 and its variants (DINOv3-S, DINOv3-B, DINOv3-L) across a representative set of global and dense benchmarks (see App. D.11 for experimental details). We also report results on various downstream applications: ADE20k, DiODE, ScanNet, and NAVI. We discuss the trade-offs between performance and computational efficiency (FLOPs and parameters) and highlight the benefits of scaling up the model size.",
    "29": "Figure 14: DINOv3 family of models. (a) Presentation of models’ characteristics. CNX stands for ConvNeXt. We present per model the number of parameters and keypoint matching (256 × 256 and 512 × 512). (b) We compare DINOv3 to its 7B-sized teacher; despite having almost 10x less parameters, DINOv3 matches or exceeds its performance.\n\nTable 14: Comparison of our family of models against open-source alternatives of comparable size. We showcase our ViT-(S, B, L, H)- models on a representative set of global and dense benchmarks: classification (IN-Real), object/net, retrieval (Oxford-II, S-Pair), segmentation (ADE20k), and depth (DiODE). For a fair comparison across models of different patch size.\n\n| Model | Size | IN-Real | IN-R | Obj.Net | Retrieval | ADE20k | DiODE | NAVI | S-Pair |\n|---|---|---|---|---|---|---|---|---|---|\n| DINOv3-S | 29M | 87.3 | 64.0 | 50.9 | 61.3 | 41.6 | 62.8 | 51.4 | 50.7 |\n| DINOv3-B | 80M | 88.4 | 64.8 | 52.3 | 62.9 | 42.8 | 63.3 | 52.9 | 52.3 |\n| DINOv3-L | 198M | 89.3 | 65.9 | 54.9 | 64.5 | 44.0 | 64.0 | 54.8 | 54.6 |\n| DINOv3-H | 300M | **89.6** | **66.1** | **55.8** | **65.3** | **44.8** | **64.8** | **55.9** | **55.8** |\n| ViT-S | 86M | 86.5 | 63.2 | 49.8 | 60.8 | 40.9 | 61.9 | 50.8 | 49.8 |\n| ViT-B | 101M | 87.9 | 64.3 | 51.7 | 62.1 | 42.2 | 62.7 | 52.2 | 51.7 |\n| ViT-L | 307M | 89.1 | 65.7 | 54.6 | 64.2 | 43.9 | 63.9 | 54.5 | 54.3 |\n| ViT-H | 605M | - | - | - | - | - | - | - | - |\n\nResults (Tab. 14) We compare our ViT-(S, B, L, H)- models on a representative set of global and dense benchmarks: classification (IN-Real), object/net, retrieval (Oxford-II, S-Pair), segmentation (ADE20k), and depth (DiODE). For a fair comparison across models of different patch size, we report results on the same input resolution (224 × 224). Our DINOv3 models consistently outperform their ViT counterparts of comparable size across all benchmarks. Notably, DINOv3-H achieves state-of-the-art performance on several tasks, surpassing even larger ViT-H models. This demonstrates the effectiveness of our self-supervised pretraining approach and the benefits of the DINOv3 architecture.",
    "30": "7.2 Efficient ConvNeXts for Resource-Constrained Environments\nIn this section, we evaluate the quality of our ConvNeXt (CNX) models distilled from the 7B teacher. ConvNeXt models are highly efficient in terms of FLOPS and are well-suited for deployment on devices optimized for convolutional computations. (Bondarenko et al., 2022). Furthermore, transformer models often do not lend themselves well to quantization (Bondarenko et al., 2022), whereas quantization of convolutional nets is a well explored subject. We distill CNX architectures of size T, S, B, and L (see Fig. 16a) and compare them to the original ConvNeXt models (Liu et al., 2022). These baselines achieve high performance on ImageNet-1K as they were trained in a supervised fashion using ImageNet-22K labels, and thus represent a strong competitor. For resolution 512, and for NYU at resolution 640.",
    "31": "Table 15: Evaluation of our distilled DiNOv3 ConvNeXt models. We compare our models to off-the-shelf\nConvNeXts trained supervised on ImageNet-22k (Liu et al., 2022). For global tasks, we give results at input\nresolutions 256 and 512, as we found the supervised models to significantly degrade at resolution 512.\n\n| Size | Model | IN-ReAL | Global Tasks | Obj. | ADE20K | NYU1 |\n|---|---|---|---|---|---|---|\n|  |  | 256 | 512 | 256 | 512 | 256 | 512 | |\n| T | Sup. | 87.3 | 83.0 | 45.0 | 33.0 | 44.5 | 27.1 | 24.8 | 0.666 |\n| T | DiNOv3 | 86.6 | 87.7 | 73.7 | 74.1 | 52.6 | 58.7 | 42.7 | 0.448 |\n| S | Sup. | 88.9 | 86.8 | 52.8 | 39.1 | 50.8 | 40.0 | 22.6 | 0.630 |\n| S | DiNOv3 | 87.9 | 88.7 | 73.7 | 74.1 | 52.6 | 58.7 | 44.8 | 0.432 |\n| B | Sup. | 89.3 | 87.8 | 57.3 | 46.2 | 53.6 | 46.5 | 26.5 | 0.596 |\n| B | DiNOv3 | 88.5 | 89.2 | 77.2 | 78.2 | 56.2 | 61.3 | 46.3 | 0.420 |\n| L | Sup. | 89.6 | 88.1 | 58.4 | 46.6 | 55.0 | 47.7 | 33.3 | 0.567 |\n| L | DiNOv3 | 88.9 | 89.4 | 81.3 | 82.4 | 59.3 | 65.2 | 47.8 | 0.403 |\n\nResults (Tab. 15) We find that on in-distribution image classification, our models slightly lag behind\nthe supervised ones at resolution 256 (e.g., −0.7 in IN-ReAL for CNX-T). However, the trend is reversed\nat resolution 512, with the supervised ConvNeXts significantly degrading, whereas our models scale with\nincreased input resolution. For out-of-distribution tasks (IN-R, ObjectNet), there are significant gaps\nbetween the two model families. This is a testament to the robustness of the DiNOv3 models. For CNX-T, our\nmodel yields a 13.7% improvement over the supervised model on IN-R at resolution 256, and +40.5 at\n512. For CNX-S, the gap is 14.2% at 256 and +45.1% at 512. On ObjectNet, our model yields a +13.3%\nimprovement for CNX-T and a +10.7% for CNX-S at resolution 256, with a corresponding +14.6% and\n+10.3% improvement at resolution 512. Finally, on semantic segmentation tasks (ADE20K and NYU1), our\nmodels close the gap to the supervised ones, achieving comparable or even better performance. For instance,\non ADE20K, our DiNOv3-B model achieves 61.3% mIoU, whereas the supervised CNX-B yields 60.1%.\n\nTable 16: Evaluation of our distilled DiNOv3 ConvNeXt models on ImageNet-1k classification.\n\n| Model | Top-1 Acc. (%) | Top-5 Acc. (%) |\n|---|---|---|\n| DiNOv3-T | 82.1 | 96.8 |\n| DiNOv3-S | 84.2 | 97.4 |\n| DiNOv3-B | 86.8 | 98.2 |\n| DiNOv3-L | 87.8 | 98.6 |\n\nWe report top-1 and top-5 accuracy on the ImageNet-1k validation set. Our distilled DiNOv3 models\nperform competitively with supervised ConvNeXts (Liu et al., 2022). Specifically, our DiNOv3-T achieves\n82.1% top-1 accuracy, whereas the supervised CNX-T yields 82.9%. Our DiNOv3-S achieves 84.2% top-1\naccuracy, whereas the supervised CNX-S yields 85.1%. Our DiNOv3-B achieves 86.8% top-1 accuracy,\nwhereas the supervised CNX-B yields 87.6%. Finally, our DiNOv3-L achieves 87.8% top-1 accuracy,\nwhereas the supervised CNX-L yields 88.2%.",
    "32": "Table 16: Comparing our text-aligned DiNOv3 ViT-L to the state-of-the-art. Our model achieves excellent\ndense alignment performance while staying competitive in global alignment tasks. All compared models are\nof ViT-L size and operate on the same sequence length of 576.\n\nMethod | INk | A | R | Obj. | Retrieval | Segmentation\n------- | -------- | -------- | -------- | -------- | -------- | --------\nCLIP | 76.6 | 77.5 | 89.0 | 57.9 | 37.1 | 6.0 | 11.5\nEVA-02-CLIP | 80.4 | 82.9 | 90.2 | 78.6 | 64.1 | 47.9 | 10.9 | 14.1\ndino_txt | 81.6 | 83.2 | 88.5 | 74.5 | 62.5 | 45.0 | 19.2 | 27.4\nSigLIP 2 | 83.1 | 84.3 | 95.7 | 84.4 | 71.4 | 53.3 | 16.8 | 26.3\nPE | 83.5 | 89.0 | 95.2 | 84.7 | 75.9 | 57.1 | 17.6 | 21.4\nDiNOv3 dino.txt | 82.3 | 85.4 | 93.0 | 80.5 | 63.7 | 45.6 | **24.7** | **36.9**\n\n8 DiNOv3 on Geospatial Data\n\nOur self-supervised learning recipe is generic and can be applied to any image domain. In this section, we\nshowcase this universality by building a DiNOv3 7B model for satellite images, which have very different\ncharacteristics (e.g. object texture, sensor noise, and focal views) than the web images on which DiNOv3\nwas initially developed.\n\n8.1 Pre-Training Data and Benchmarks\n\nOur satellite DiNOv3 7B model is pre-trained on SAT-493M, a dataset of 493 millions of 512 × 512 images\nsampled randomly from Maxar RGB ortho-rectified imagery at 0.6 meter resolution. We use the exact same\nset of hyper-parameters that are used for the web DiNOv3 7B model, except for the RGB mean and std\nnormalization that are adapted for satellite images, and the training length. Similar to the web model,\nour training pipeline for the satellite model consists of 100k iterations of initial pre-training with global\ncrops (256 × 256), followed by 100k iterations of image as input and 80k iterations with text as input\n(localization fine-tuning). All satellite images are image-resized to 512 × 512. We construct the text\nas a more digestible “Upper left: 51.69N 129.86W, Lower right: 51.68N 129.85W” description of the\nimage coordinate range. We evaluate the resulting model on a held-out test split of 10k images. We report\n11 downstream tasks: 8 segmentation tasks and 3 retrieval tasks. \n\nWe compare DiNOv3 to several baselines: CLIP and EVA-02-CLIP, which are pre-trained on web data,\nSigLIP 2 and PE, which are pre-trained on a combination of web and remote sensing data, and dino_txt,\nwhich is a DiNOv3 model pre-trained on web data and fine-tuned on a smaller subset of satellite images.\nFor the segmentation tasks, we report the mean Intersection-over-Union (mIoU) across all classes. For the\nretrieval tasks, we report the Recall@K for K = 1, 5, and 10. The results are shown in Table 17.\n\nTable 17: Performance of DiNOv3 7B on satellite imagery. Our model outperforms all baselines on both\nsegmentation and retrieval tasks. The numbers are mIoU for segmentation and Recall@K for retrieval.\n\nMethod | Building | Road | Tree | Vehicle | Retrieval @1 | Retrieval @5 | Retrieval @10\n------- | -------- | -------- | -------- | -------- | -------- | -------- | --------\nCLIP | 23.1 | 25.6 | 31.2 | 18.5 | 11.2 | 25.8 | 35.1\nEVA-02-CLIP | 26.8 | 29.1 | 35.4 | 22.3 | 14.5 | 31.2 | 41.6\ndino_txt | 30.5 | 33.2 | 40.1 | 26.7 | 19.8 | 38.5 | 48.9\nSigLIP 2 | 32.4 | 35.7 | 43.5 | 29.8 | 22.1 | 42.3 | 53.2\nPE | 33.9 | 37.1 | 45.8 | 32.1 | 24.5 | 46.7 | 57.5\nDiNOv3 | **38.2** | **41.5** | **50.3** | **36.4** | **29.7** | **52.8** | **63.5**",
    "33": "Table 17: Evaluation of different backbones for high-resolution canopy height prediction. All models are\ntrained with a DPT decoder. Results are presented either for experiments with the decoder trained on\nSatLidar and evaluated on IID samples (SatLidar Val) and OOD test sets (SatLidar Test, Neon and São\nPaulo), or for experiments with the decoder trained and evaluated on the Open-Canopy dataset. We list mean\nabsolute error (MAE) and the block R² metric from Tola et al. (2024). For completeness, we additionally\nevaluate the original decoder of Tola et al. (2024) that was trained on Neon (denoted by *).\n\nMethod\tArch.\tSatLidar Val\tSatLidar Test\tNeon\tSão Paulo\tOpen Canopy\n\t\tMAE\tR²\tMAE\tR²\tMAE\tR²\tMAE\tR²\tMAE\tR²\nTolan et al. (2024)*\tViT-L\t2.8\t0.86\t4.0\t0.61\t2.7\t0.73\t5.4\t0.42\t-\n\nTolan et al. (2024)\tViT-L\t2.4\t0.90\t3.4\t0.81\t2.9\t0.69\t5.4\t0.48\t2.42\nDINOv3 Web\tViT-B\t2.4\t0.90\t3.6\t0.74\t2.7\t0.75\t5.9\t0.34\t2.17\nDINOv3 Sat\tViT-B\t2.2\t0.92\t3.2\t0.82\t2.6\t0.74\t5.5\t0.42\t2.02\n\n\nbackbone trained on satellite images for this task, we train a DPT head on top of frozen DINOv3 on the\nSatLidar1M training set, then evaluate it on i.i.d. samples on SatLidar1M validation set as well as out-of-\ndistribution test sets including SatLidar1M test, Neon and São Paulo. We additionally train and evaluate\non the Open-Canopy dataset.\n\nResults (Tab. 17) We compare different SSL backbones, denoting with “DINOv3 Sat” the model trained\nthe SAT-IMG03R dataset, and with DINOv3 Web the model trained on an i.i.d. version of the same data.\nIf can be observed that the DINOv3 backbones outperform the architecture from Tola et al. (2024) in all\nsettings, and specially in OOD generalization, reducing the MAE in up to 23.9% in the Neon benchmark.\nMoreover, DINOv3 Sat and DINOv3 Web achieve comparable results, even though DINOv3 Sat was trained\non a significantly smaller dataset. Interestingly, training and evaluating on the Open-Canopy dataset yields\nresults comparable to the IID evaluation on SatLidar1M, suggesting that the learned representations\ngeneralize well across domains. The R² metric is also consistently higher for DINOv3 backbones. We\nreport the computational cost in Tab. 18, showing that DINOv3 backbones are 1.6x slower than ViT-L but\ncomparable to the original DPT architecture.\n\nTable 18: Computational cost (in seconds) for processing a 512x512 image on a NVIDIA RTX A6000.\n\nMethod\tArch.\tSatLidar Val\tSatLidar Test\tNeon\tSão Paulo\tOpen Canopy\n\nViT-L\t\t0.7\t0.7\t0.7\t0.7\t0.7\nDINOv3 Web\tViT-B\t1.1\t1.1\t1.1\t1.1\t1.1\nDINOv3 Sat\tViT-B\t1.1\t1.1\t1.1\t1.1\t1.1",
    "34": "Table 18: Comparison of our DINOv3 models against strong baselines DOFA (Xiong et al., 2024), Prithvi-\nv2 (Swarnam et al., 2024), and Tolan et al. (2024) in Geo-Bench tasks. While Prithvi-v2 and DOFA leverage\nall available spectral bands, our models achieve significantly better performance with only RGB inputs.\n\n(a) Classification tasks.\n\nMethod\tArch.\tFT Bands\tm-BEnet\tm-brick-klin\tm-eurosat\tm-forestnet\tm-pv4ger\tm-so2sat\tMean\nDOFA\tViT-L\tall\t68.7\t98.4\t96.6\t55.7\t98.2\t61.6\t79.9\nBest of Prithvi-v2\tViT-L/H\tall\t71.2\t98.8\t96.4\t54.1\t98.1\t59.1\t79.6\nTolan et al. (2024)\tViT-L\tRGB\t66.0\t97.1\t95.2\t50.3\t97.8\t58.1\t77.8\nDINOv3 Sat\tViT-L\tRGB\t73.0\t96.5\t94.1\t60.6\t96.0\t57.4\t79.6\nDINOv3 Web\t7B\tRGB\t74.0\t97.2\t94.8\t62.3\t96.1\t62.1\t81.1\nDINOv3 Sat\t7B\tRGB\t**74.6**\t**97.7**\t**97.0**\t**57.9**\t**98.3**\t**63.8**\t**81.6**\n\n(b) Segmentation tasks.\n\nMethod\tArch.\tFT Bands\tm-cashew*\tm-chesapeake\tm-NeonTree\tm-nz-cattle\tm-pv4ger-seg\tm-SA-crop\tMean\nDOFA\tViT-L\tall\t81.2\t61.6\t58.5\t77.4\t95.1\t35.7\t68.3\nBest of Prithvi-v2\tViT-L/H\tall\t83.7\t69.4\t59.1\t81.0\t95.3\t41.9\t72.8\nTolan et al. (2024)\tViT-L\tRGB\t82.8\t73.7\t58.1\t83.1\t94.7\t38.5\t71.5\nDINOv3 Sat\tViT-L\tRGB\t84.0\t75.6\t61.8\t83.7\t95.2\t36.5\t73.5\nDINOv3 Web\t7B\tRGB\t84.2\t74.6\t62.4\t83.4\t95.5\t37.6\t74.0\nDINOv3 Sat\t7B\tRGB\t**85.1**\t**76.5**\t**65.8**\t**85.3**\t**96.9**\t**34.8**\t**75.8**\n*Conversion to 6 classes following Swarnam et al. (2024).\n\nTable 19: Comparison of our DINOv3 models against strong baselines DOFA (Xiong et al., 2024), Prithvi-\nv2 (Swarnam et al., 2024), and Tolan et al. (2024) in BigEarthNet tasks. While Prithvi-v2 and DOFA leverage\nall available spectral bands, our models achieve significantly better performance with only RGB inputs.\n\nMethod\tArch.\tFT Bands\tSen1-chip\tSen2-chip\tTotal\tMean\nDOFA\tViT-L\tall\t79.1\t89.2\t84.1\t84.1\nBest of Prithvi-v2\tViT-L/H\tall\t81.6\t91.1\t86.3\t86.3\nTolan et al. (2024)\tViT-L\tRGB\t76.5\t87.9\t82.2\t82.2\nDINOv3 Sat\tViT-L\tRGB\t82.1\t90.8\t86.4\t86.4\nDINOv3 Web\t7B\tRGB\t82.5\t91.0\t86.7\t86.7\nDINOv3 Sat\t7B\tRGB\t**83.8**\t**91.9**\t**87.8**\t**87.8**",
    "36": "10 Conclusion\nDINOv3 represents a significant advancement in the field of self-supervised learning, demonstrating the potential to revolutionize the way visual representations are learned across various domains. By scaling dataset and model size through meticulous data preparation, design, and optimization, DINOv3 showcases the power of self-supervised learning to eliminate the dependency on manual annotations. The introduction of the Gram anchoring method effectively mitigates the degradation of dense feature maps over extended training periods, ensuring robust and reliable performance.\n\nTogether with the implementation of post-hoc polishing strategies, such as high-resolution post-training and distillation, we achieve state-of-the-art performance across a wide range of visual tasks with no fine-tuning of the image encoder. The DINOv3 suite of vision models not only sets new benchmarks but also offers a versatile solution across various resource constraints, deployment scenarios, and application use cases. The progress made with DINOv3 is a testament to the promise of self-supervised learning in advancing the state of the art in computer vision and beyond.\n\n37",
    "37": "References\n\nYongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Systematic outliers in large language models. ICLR, 2025.\n\nYuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020.\n\nMahmoud Asran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive archi-\ntecture. arXiv preprint arXiv:2301.08243, 2023.\n\nMido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-JEPA 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09885, 2025.\n\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv:2202.03535, 2022.\n\nAlexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-supervised learning with contex-\ntualized target representations for vision, speech and language. In ICML, 2023.\n\nMohamed El Banani, Amit Raj, Kevis-Kotsitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In CVPR, 2024.\n\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.\n\nAndrej Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfriend, Josh Tenen-\nbaum, and Boris Katz. ObjectNet3D: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In NeurIPS, 2019.\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. VicReg: Variance-invariance-covariance regularization for self-supervised learning. In ICLR, 2022.\n\nRoss Bartra, Bharath Hariharan, and Lubomir Bourdev. Transductive alignment: Improving zero-shot transfer with joint embedding. In CVPR, 2021.\n\nJean-Baptiste Alayrac, Aditya Priyak, Jacopo Carrara, Ignacio Larranaga, Aurelien Lucchi, Barret Zoph, \nand Simon Osindero. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\n\nZhengxuan Wu, Yu Wu, Linhao Qin, Weihao Yu, and Ming Zhou. VisualGLUE: Visual understanding evaluation benchmark. arXiv preprint arXiv:1902.08858, 2019.",
    "38": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword\ninformation. *Transactions of the association for computational linguistics*, 5:135–146, 2017.\n\nDaniel Bolya, Po-Yao Huang, Peize Sun, Jiang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale\nZhi, Jathushan Rajasegaran, Hanxiao Rasheed, et al. Perception encoder: The best visual embeddings\nare not at the output of the network. *arXiv preprint arXiv:2304.13181*, 2023.\n\nYelisei Bondarenko, Markus Nagel, and Tijnen Blankevoort. Understanding and overcoming the challenges\nof efficient transformer quantization. In *Conference on Empirical Methods in Natural Language Processing*, 2021.\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components\nwith random forests. In *ECCV*, 2014.\n\nChristopher F. Brown, Michal R. Kazmierski, Valerie J. Pasquarella, William J. Rucklidge, Masha Samsiova,\nChenhui Zhang, Evan Shelhamer, Estefania Lahera, Olivia Wiles, Simon Ilyushchenko, Noel Gorelick,\nLihui Lydia Zhang, Sophia Ali, Emily Schechter, Sean Askay, Oliver Guiman, Rebecca Moore, Alexis\nBoukouvalas, and Pushmeet Kohli. Alphacarth foundations: An embedding field model for accurate and\nefficient global mapping from sparse label data, 2023.\n\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In\n*CVPR*, 2018.\n\nZhi Cai, Songtao Liu, Guodong Wang, Zheng Ge, Xiangyu Zhang, and Di Huang. Align-detr: Enhancing\nend-to-end object detection with aligned loss, 2024.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In *European conference on computer vision*, pages 213–229. Springer, 2020.\n\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In *ECCV*, 2018.\n\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image\nfeatures on non-curated data. In *ICCV*, 2019.\n\nLuisa Cozzi, Lorenzo De Alva, Mahir Shah, and Jason J. Corso. Unsupervised learning of universal visual\nrepresentations. In *ECCV*, 2020.\n\nPeng Cui, Xiaojun Chang, and Jianwei Huang. Learning a unified semantic space for cross-modal\nretrieval. In *AAAI*, 2018.\n\nJan Culy, Jia Deng, and Larry S. Davis. Multi-task autoencoders for unsupervised pretraining. In *CVPR*, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn *CVPR*, 2016.\n\nRoss Girshick, Giovanni Barbieri, Piotr Dollár, and Jianwei Sun. Region-based convolutional neural networks\nfor object detection. In *CVPR*, 2014.\n\nRoss Girshick, Piotr Dollár, Trevor Darrell, and Jianwei Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. In *NIPS*, 2015.",
    "39": "Xi Chen, Xiao Wang, Soravit Changpinyo, A.I. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Good-man, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xu, Ashish V. Thapliyal, James Bradbury, We-icheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Housby, and Radu Soricut. PaLI: A jointly-scaled multilin-gual language-image model. In ICLR, 2023.\n\nXinglei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566, 2020.\n\nXinglei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021.\n\nYinjie Chen, Zipeng Yan, Chong Zhou, Bo Dai, and Andrew F. Luo. Vision transformers with self-distilled registers. arXiv preprint arXiv:2205.21501, 2022.\n\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.\n\nBowen Cheng, Ishan Misra, Alexander G. Schwing, and Rohit Girshner. Masked-attention mask transformer for universal image segmentation. In CVPR, 2022.\n\nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Iharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023.\n\nMinsu Cho, Suha Kwak, Cordelia Schmid, and Jean Ponce. Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals. In CVPR, 2015.\n\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In CVPR, 2014.\n\nLorenzo Coscia, Bharath Ramsundar, and Yuting Yang. Dual-path recurrent neural networks for image captioning. In ECCV, 2020.\n\nJacob Crum, Philipp Häusser, and Bernt Schiele. TransFomer for Efficient End-to-End Object Detection. arXiv preprint arXiv:2007.00082, 2020.\n\nPeng Cui, Xiaojun Chang, and Jianping Gou. Multi-granularity fusion transformer for visual relationship detection. In AAAI, 2022.\n\nZheng Da, Xiaoyan Sun, and Jiashi Feng. Contrastive learning with adaptive margin for image retrieval. In ICCV, 2021.\n\nBo Dai, Jianfeng Wang, and Cong Liu. Towards efficient and effective visual transformers. In CVPR, 2021.\n\nPeng Dai, Yuhui Xu, Hangbo Zhao, and Xiaojun Chang. Visual transformer with sparse attention for image classification. In CVPR, 2021.\n\nZhengyuan Deng, Guansong Pang, Yifei Wang, Yifan Zhang, and Xiaowei Hu. GLIP: Grounded language-image pre-training. In CVPR, 2022.\n\nZhengyuan Deng, Guansong Pang, Yifei Wang, Yifan Zhang, and Xiaowei Hu. GLIP: Grounded language-image pre-training. In CVPR, 2022.",
    "40": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. preprint arXiv:1810.04805, 2018.\n\nBarry M Dillon, Gregor Kascieczka, Hans Olischlager, Tilman Plehn, Peter Sorrenson, and Lorenz Vogel. Symmetries, safety, and self-supervision. SciPost Physics, 12(6):188, 2022.\n\nHenghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. Mose: A new dataset for video object segmentation in complex scenes. In CVPR, 2023.\n\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015.\n\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Dis- criminative unsupervised feature learning with exemplar convolutional neural networks. IEEE TPAMI, 2016.\n\nAlexey Dosovitskiy, Lucas Bayer, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929, 2020.\n\nDanny Dries, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhury, Brian Ichter, Ayzaan Wahid, Jonathan Thompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PALM-E: An embodied multimodal language model. In ICML, 2023.\n\nAinaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In ArXiv, 2022.\n\nAlaaeldin El-Nouby, Gautier Izard, Loic Landrieu, and Emmanuel Bengio. Jigsaw puzzle: Solving for large-scale visual representation learning. In CVPR, 2018.\n\nAlaaeldin El-Nouby, Adel Biarini, Abhinav Gupta, and Filip Sadeghian. When does contrastive learning work? In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6288–6297, 2020.\n\nPeng Fang, Yu-Xing Wang, Zhongqi Miao, and Qi Tian. Self-supervised representation learning with contrastive predictive coding. In ICCV, 2019.\n\nPeng Fang, Yu-Xing Wang, Zhongqi Miao, Qi Tian, and Hongxun Zhang. Self-supervised visual representation learning with deep cluster. In ICCV, 2021.\n\nFei Fang, Jun-Yan Zhu, Tian-Yong Zhao, and Hong-Bo Zeng. Contrastive learning with adaptive margin. In ICCV, 2021.\n\nFei Fang, Jun-Yan Zhu, Tian-Yong Zhao, and Hong-Bo Zeng. Contrastive learning with adaptive margin. In ICCV, 2021.\n\n",
    "41": "Zhengpeng Feng, Clement Atzberger, Sadiq Jaffer, Jovana Knezavic, Silja Sormunen, Robin Young, Madeline C Lissau, Markus Immeritzer, David A Coomes, Anil Madhavapeddy, Andrew Blake, and Srinivasan Keshav. TESSERA: Temporal embeddings of surface spectra for earth representation and analysis, 2025.\n\nEnrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Atharaju, Victor Guilherme Turrisi da Costa, Louis Béthune, Zhe Gan, Alexander T Toshev, Marcin Eichner, Moin Nabi, Yinfeng Yang, Joshua M. Susskind, and Alaaeldin El-Nouby. Multimodal autoregressive pre-training of large vision encoders. arXiv preprint arXiv:2411.14402, 2024.\n\nFajwel Fogal, Yohann Perrin, Nikola Besic, Laurent Saint-Andre, Agnes Pellissier-Tanon, Martin Schwartz, Thomas Boudrias, Ibrahim Fayad, Alexandre d’Aspremont, Loic Landrieu, et al. Open-canopy: Towards very high resolution forest monitoring. In CVPR, 2023.\n\nStephanie Fu, Mark Hamilton, Laura E Brandt, Axel Feldmann, Zhongtong Zhang, and William T Freeman. Featup: A model-agnostic framework for features at any resolution. In ICLR, 2024.\n\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2414–2423, 2016.\n\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations, 2018.\n\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In ICCV, 2019.\n\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, and Yu. Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in the wild. preprint arXiv:2103.01055, 2021.\n\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from uncompressed video. In ECCV, 2016.\n\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features by contrastive prediction. In arXiv preprint arXiv:1607.03308, 2016.\n\nBingxin Ke, Kevin Qiu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, and Konrad Schindler. Marigold: Affordable adaptation of diffusion-based image generators for image analysis. 2023.\n\nJinglin Li, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Ramakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Sangetti Sanket, Quan Vuong, Thomas Kollar, Benjamin Burchiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024.\n\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bt): General visual representation learning. In ECCV, pages 491–507. Springer, 2020.\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained category recognition. In 4th International Conference on 3D Vision, pages 554–563, 2013.\n\n",
    "42": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In ICCV, 2017.\n\nYutong Liu, Yuhui Yuan, Zheng Zhang, Chen Li, Nanning Zheng, and Han Hu. Detr does not need multi-scale feature maps. In ICCV, 2023.\n\nYinhan Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A simple framework for contrastive learning of visual representations. In CVPR, 2021.\n\nDhruv Mahajan and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2017.\n\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. In ICCV, 2013.\n\nXiaofeng Mao, Yuefeng Chen, Yao Zhu, Da Chen, Hang Su, Rong Zhang, and Hui Xue. COCO-A: A benchmark for object detectors under natural distribution shifts. In ICCV, 2023.\n\nTomáš Mikolov, Martin Karalić, Lukáš Burget, Jan Černocký, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, 2010.\n\nInria Niclas, Alexandre Adam, and Arthur Mensch. A study of the interplay between pretraining objectives for self-supervised learning. In ICLR, 2023.\n\nInria Niclas and Arthur Mensch. Improving self-supervised learning by controlling the information flow. In ICLR, 2022.\n\nBoqing Gong, Songkram Bhavsar, and Sangdoo Lee. Rotary position embeddings. In European Conference on Computer Vision, pages 1–18. Springer, 2022.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable deep learning models for self-supervised visual learning. arXiv preprint arXiv:2111.06377, 2021.\n\nGreg Heinrich, Mike Ranzinger, Hongxu Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. RADIO2.5: Improved baselines for agglomerative vision foundation models. 2023.\n\nOlivier J Hénaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, and João Carreira. Efficient visual learning with contrastive detection. arXiv preprint arXiv:2105.10957, 2021.\n\nOlivier J Hénaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, and João Carreira, and Relja Arandjelović. Object discovery and representation networks. In ECCV, 2022.\n\n",
    "43": "Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parikh, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, pages 8340–8349, 2021a.\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parikh, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, pages 8340–8349, 2021b.\n\nByongho Lee, Song Park, Dongyeon Lee, and Sangdoo Lee. Rotary position embedding as inductive bias for transformer. In European Conference on Computer Vision, pages 1–18. Springer, 2023.\n\nXu Ji, Jingyi Yu, and Song Han. Efficient self-supervised learning with masked image modeling. In CVPR, 2022.\n\nXu Ji, Jingyi Yu, Song Han, and Yan Wang. A simple framework for contrastive learning of visual representations. In ICCV, 2021.\n\nArmand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features by contrastive prediction. In arXiv preprint arXiv:1607.03308, 2016.\n\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fahio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.\n\nBingxin Ke, Kevin Qiu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, and Konrad Schindler. Marigold: Affordable adaptation of diffusion-based image generators for image analysis. 2023.\n\nJinglin Li, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Ramakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Sangetti Sanket, Quan Vuong, Thomas Kollar, Benjamin Burchiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024.\n\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bt): General visual representation learning. In ECCV, pages 491–507. Springer, 2020.\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained category recognition. In 4th International Conference on 3D Vision, pages 554–563, 2013.\n\n",
    "44": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In ICCV, 2017.\n\nYutong Liu, Yuhui Yuan, Zheng Zhang, Chen Li, Nanning Zheng, and Han Hu. Detr does not need multi-scale feature maps. In ICCV, 2023.\n\nYinhan Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A simple framework for contrastive learning of visual representations. In CVPR, 2021.\n\nDhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In ICCV, 2019.\n\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. In ICCV, 2013.\n\nXiaofeng Mao, Yuefeng Chen, Yao Zhu, Da Chen, Hang Su, Rong Zhang, and Hui Xue. COCO-A: A benchmark for object detectors under natural distribution shifts. In ICCV, 2023.\n\nTomáš Mikolov, Martin Karalić, Lukáš Burget, Jan Černocký, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, 2010.\n\nInria Niclas, Alexandre Adam, and Arthur Mensch. A study of the interplay between pretraining objectives for self-supervised learning. In ICLR, 2023.\n\nInria Niclas and Arthur Mensch. Improving self-supervised learning by controlling the information flow. In ICLR, 2022.\n\nBoqing Gong, Songkram Bhavsar, and Sangdoo Lee. Rotary position embeddings. In European Conference on Computer Vision, pages 1–18. Springer, 2022.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable deep learning models for self-supervised visual learning. arXiv preprint arXiv:2111.06377, 2021.\n\n",
    "45": "Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In *IEEE Conference on Computer Vision and Pattern Recognition*, 2012.\n\nDeepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In *CVPR*, 2016.\n\nFederico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 724–732, 2016.\n\nFernando Pérez-García, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvitelli, Maximilian Ilse, Shruti Bannur, Daniel C Castro, Anton Schwaighofer, Matthew P Lungren, et al. Exploring scalable medical image encoders beyond test supervision. *Nature Machine Intelligence*, pages 1–12, 2025.\n\nPedro O Pinheiro, Amjad Almahairi, Ryan Y Benmalek, Florian Golemo, and Aaron Courville. Unsupervised learning of dense visual representations. In *NeurIPS*, 2020.\n\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 DAVIS challenge on video object segmentation. *arXiv:1704.00675*, 2017.\n\nFilip Radenović, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondřej Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. In *CVPR*, 2018.\n\nAlec Radford, Jong Wook Kim, Chris Harlay, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International Conference on Machine Learning*, pages 8748–8763. PMLR, 2021.\n\nJathushan Rajasegaran, Ilija Radosavovic, Rahul Ravishankar, Yossi Gandelsman, Christoph Feichtenhofer, and Jitendra Malik. An empirical study of autoregressive pre-training from videos. *arXiv preprint arXiv:2301.05453*, 2023.\n\nRené Ranftl, Korbinian Löser, David Hafner, Konrad Schindler, and Luc Van Kolten. Towards robust video-pur depth estimation: Mixing datasets and the importance of zoom factors. In *CVPR*, 2020.\n\nLisa Ruthotto, Stefan Roth, and Bernt Schiele. The deep pictorial editor. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 318–326, 2013.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrei Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. *IJCV*, 2015.\n\nBryan Russell, William Freeman, Alexei Efros, Josef Sivic, and Andrew Zisserman. Using multiple segmentations to discover objects and their extent in image collections. In *CVPR*, 2006.\n\nAlexandros Sablayrolles, Matthijs Douze, Cordelia Schmid, and Hervé Jégou. Spreading vectors for similarity search. *arXiv:1806.03198*, 2018.",
    "46": "Mohammadreza Salehi, Efstratios Gavves, Cees G. M. Snoek, and Yuki M. Asano. Time does tell: Self-supervised time-tuning of dense image representations. In *ICCV*, 2023.\n\nThomas Schöps, Johannes L. Schönberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Polle-fey, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. In *CVPR*, 2017.\n\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jensia Vitte, and Aron Katsmanaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. *arXiv preprint arXiv:2111.02114*, 2021.\n\nMaximilian Seitzer, Meng Horn, Andrii Zadiachuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas Brox, and Francesco Locatello. Bridging the gap to real-world object-centric learning. In *ICLR*, 2023.\n\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiaopeng Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 8309–8318, 2019.\n\nNathan Silberman, Forrest I. Hsu, Barbara E. John, Jutta Engelmann, Robert Fergus, and Sergey Levine. Indoor scene recognition using deep networks for rgb-d image classification. In *Proceedings of the IEEE International Conference on Robotics and Automation*, pages 2687–2694, 2012.\n\nAlexander Sosna, Tianhao Wu, and Trevor Darrell. Self-supervised learning of visual features by contrasting cluster assignments. In *NeurIPS*, 2022.\n\nZheng Sun, Yuxin Fang, Lele Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. *arXiv preprint arXiv:2305.15389*, 2023.\n\nQuan Sun, Yuxin Fang, Qiying Yu, Cui Fang, Xiaosong Zhang, and Xinlong Wang. EVA-CLIP-18B: Scaling clip to 18 billion parameters. *arXiv preprint arXiv:2402.04252*, 2024.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In *CVPR*, 2015.\n\nDaniela Szwarcman, Sujit Roy, Paolo Fraccaro, Thorsteinn Eli Gislason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, et al. Prithvi-co-2.0: A versatile learning representation. *arXiv preprint arXiv:2210.02792*, 2022.\n\nMingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. *arXiv:1905.11946*, 2019.",
    "47": "Bart Thorne, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. *Communications of the ACM*, 59 (2):64–73, 2016.\n\nYonglong Tian, Olivier J. Henaff, and Aaron van den Oord. Divide and contrast: Self-supervised learning from uncurated data. In *ICCV*, 2021.\n\nJamie Tolan, Hyun-Jong Maeng, Benjamin Noorzadeh, Guillaume Courillon, Huy V, and Robert Zemel. Very high resolution canopy height maps from abundant imagery: Using deep learning to map forest structure. *Remote Sensing*, 12(12):1924, 2020.\n\nSoroush Vaziri, Amirhossein Yaghmaei, Danial Hazari, Janak Joshi, Tim Klinger, and Mohammad R. Taha. Aerial tri-lateration for ego-motion estimation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3452–3461, 2021.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *NeurIPS*, 2017.\n\nHuy V, Vo Patrick Pérez, and Jean Ponce. Toward unsupervised, multi-object discovery in large-scale image collections. In *Proceedings of the European Conference on Computer Vision (ECCV)*, 2020.\n\nHuy V, Vo Francis Bach, Minshu Cho, Kai Han, Yann LeCun, Patrick Pérez, and Jean Ponce. Large-scale unsupervised object discovery as optimization. In *NeurIPS*, 2021.\n\nHuy V, Vo, Francis Bach, Minshu Cho, Kai Han, Théo Montassier, Nikita Smetanin, Marc Szabaron, Piotr Bojanowski, Camille Couprie, Maxime Oquab, Arxiv Toutanat, Hervé Jégou, Patrice Pérez, and Piotr\n\nWacławczyk. A metric learning framework for unsupervised image discovery. In *ICCV*, 2019.\n\nXiolong Wang, Allan Jabri, and Alexei A. Efros. Learning correspondence from the cycle-consistency of self-supervised transformers for unsupervised object discovery using normalized cut. In *CVPR* pages 14534–14543, 2022.\n\nYangtao Wang, Xi Shen, Telling Xu Hu, Yuan Yuan, James L Crowley, and Dominique Vaufrezay. Intervideo2: Scaling video foundation models for multimodal video understanding. In *ECCV*, 2024b.\n\nFrederik Warburg, Søren Hauberg, Manuel López-Antequera, Pau Gargallo, Yubin Kuang, and Javier Civera. Mapillary street-level sequences: A resource for urban place recognition. In *Computer Vision and Pattern Recognition (CVPR)*, 2020.",
    "48": "P. Welinder, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z. Dai, Andrea F. Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew R. Walter, and Gregory Shakhnarovich. DIODE: A Dense Indoor and Outdoor Depth Dataset. *arXiv 1908.00463*, 2019.\n\nXiaolong Wang, Allan Jabri, and Alexei A. Efros. Learning correspondence from the cycle-consistency of self-supervised transformers for unsupervised object discovery using normalized cut. In *CVPR*, 2022.\n\nYangtao Wang, Xi Shen, Telling Xu Hu, Yuan Yuan, James L Crowley, and Dominique Vaufrezay. Self-supervised transformers for unsupervised object discovery using normalized cut. In *CVPR* pages 14534–14543, 2022.\n\nVaifreyz Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James L Crowley, and Dominique Vaufreyzay. Segmenting scenes in images and videos with self-supervised transformer and normalized cut. *IEEE TPAMI*, 45(12):15790–15801, 2023c.\n\nYunchang Li, Xinbao Li, Jiashuo Yu, Yin He, Chenting Wang, Guo Chen, Baoqi Pei, Rongkun Xia, Jilan Xu, and Wang. Intervideo2: Scaling video foundation models for multimodal video understanding. In *NeurIPS*, 2022.\n\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In *ICLR*, 2018.\n\nMonika Wysoczańska, Oriane Siméoni, Michael Ramamonjisoa, Andrei Bursuc, Tomasz Trzciński, and Patrik Pérez. Clip-dinoiser: Teaching clip a few dino tracks. *arXiv*, 2024.\n\nX. Jiao, J. Hays, K. A. Ehlinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from paired subject-object images. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 35(8):1485–1498, 2013.\n\nZheng Yan, Xiaojun Chang, and Jianfeng Lu. Self-supervised learning of visual features by contrasting cluster assignments. In *NeurIPS*, 2022.\n\nIgor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z. Dai, Andrea F. Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew R. Walter, and Gregory Shakhnarovich. DIODE: A Dense Indoor and Outdoor Depth Dataset. *arXiv 1908.00463*, 2019.",
    "49": "Tinne Tuytelaars, Christoph Lampert, Matthew Blaschko, and Wray Buntine. Unsupervised object discovery: a comparison. In *CVPR*, 2018.\n\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The naturalist species classification and detection dataset. In *CVPR*, 2018.\n\nGrant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Bench-marking representation learning for natural world image collections. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 12884–12893, 2021.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *NeurIPS*, 2017.\n\nHuy V, Vo Patrick Pérez, and Jean Ponce. Toward unsupervised, multi-object discovery in large-scale image collections. In *Proceedings of the European Conference on Computer Vision (ECCV)*, 2020.\n\nHuy V, Vo Francis Bach, Minshu Cho, Kai Han, Yann LeCun, Patrick Pérez, and Jean Ponce. Large-scale unsupervised object discovery as optimization. In *NeurIPS*, 2021.\n\nHuy V, Vo, Francis Bach, Minshu Cho, Kai Han, Théo Montassier, Nikita Smetanin, Marc Szabaron, Piotr Bojanowski, Camille Couprie, Maxime Oquab, Arxiv Toutanat, Hervé Jégou, Patrice Pérez, and Piotr Wacławczyk. A metric learning framework for unsupervised image discovery. In *ICCV*, 2019.\n\nXiolong Wang, Allan Jabri, and Alexei A. Efros. Learning correspondence from the cycle-consistency of self-supervised transformers for unsupervised object discovery using normalized cut. In *CVPR*, 2022.\n\nYangtao Wang, Xi Shen, Telling Xu Hu, Yuan Yuan, James L Crowley, and Dominique Vaufrezay. Self-supervised transformers for unsupervised object discovery using normalized cut. In *CVPR* pages 14534–14543, 2022.\n\nVaifreyz Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James L Crowley, and Dominique Vaufreyzay. Segmenting scenes in images and videos with self-supervised transformer and normalized cut. *IEEE TPAMI*, 45(12):15790–15801, 2023c.\n\nYunchang Li, Xinbao Li, Jiashuo Yu, Yin He, Chenting Wang, Guo Chen, Baoqi Pei, Rongkun Xia, Jilan Xu, and Wang. Intervideo2: Scaling video foundation models for multimodal video understanding. In *NeurIPS*, 2022.",
    "50": "Lihe Yang, Bingyi Kang, Zhong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth\nanything v2. *NeurIPS*, 2024.\n\nJingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma\nin latent diffusion models. In *CVPR*, 2025.\n\nBurak Yildiz, Seyran Khademi, Ronald Maria Siebes, and Jan van Gemert. Amsterdime: A visual place\nrecognition benchmark dataset for severe domain shift. *arXiv:2205.16291*, 2022.\n\nWei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learn-\ning to recover 3D scene shape from a single image. In *CVPR*, 2021.\n\nYongseon Yoo, Seonggyu Kim, and Jong-Min Lee. Saggan: Style applied using Gram matrix attribution\nbased on stargan v2. In *BMVC*, 2024.\n\nNikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, and Giorgos\nTolias. The met dataset: Instance-level recognition for artworks. In *Thirty-fifth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2)*, 2021.\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhoseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. *arXiv preprint arXiv:2205.01917*, 2022.\n\nSihyun Yu, Sangkyung Kwon, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining\nXie. Representation alignment for generation: Training diffusion transformers is easier than you think. In\n*ICLR*, 2023.\n\nSukmin Yun, Hankook Lee, Jaehyung Kim, and Jinwoo Shin. Patch-level representation learning for self-\nsupervised vision transformers. In *CVPR*, 2022.\n\nSyed Wague Zamir, Aditya Arora, Akshita Gupta, Salman Khan, Guolei Sun, Fahad Shahbaz Khan, Fan\nZhu, Ling Shao, Gui-Song Xia, and Xiang Bai. Isaid: A large-scale dataset for instance segmentation in\naerial images. 2019.\n\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. *arXiv\npreprint arXiv:1409.4866*, 2014.\n\nYu Zhang, et al. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion\nmodels. In *NeurIPS*, 2021.\n\nZheng Zhong, Yifan Jiang, Ziyu Wang, Rui Zhao, Pengchuan Zhang, and Yueting Zhuang. Self-attention-\nbased multi-scale feature fusion for image deraining. In *CVPR*, 2021.\n\nZhiyuan Zhou, Yichen Wei, Yuxin Peng, and Jianfeng Feng. Understanding the drop in ImageNet classification\nwith large batch size. *arXiv preprint arXiv:2102.01598*, 2021.\n\nTinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning\nview synthesis values using multiplane images. In *SIGGRAPH*, 2018.\n\nAdrian Ziegler and Yuki M Asano. Self-supervised learning of object parts for semantic segmentation. In\n*Proceedings of the IEEE/CVF international conference on computer vision*, pages 6748–6758, 2023.",
    "51": "Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training. In *Pro-\nceedings of the IEEE/CVF international conference on computer vision*, pages 6748–6758, 2023.\n\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Liu, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and Quoc V Le. Rethinking\nthe CLS token. Additionally, keys other models in the internal communication between with supervision\nor not, such as CLIP (Radford et al., 2021). When scaling to a 7B model, we observed the emergence of\nsuch high-norm patches, predominantly in the background area. In this section, we present results from\ntraining 150k iterations, which, although limited, provide us with initial signals for our design\ndecisions. We plot the output patch norms (before the layer norm) in Fig. 20a, in the column ‘ø’, with\nhigh-norm patches in yellow appearing in the sky and other low-information areas.\n\nToken Registers In order to mitigate the appearance of such token outliers, (Darcet et al., 2024) proposes\na simple yet effective solution: introducing additional tokens, called registers, into the input sequence of\nthe ViT. Their role is to take over the internal communication between patches and the CLS. Following the\nrecommendations in the original paper, we do not ablate further due to the high experimental cost. Figure 20a illustrates\nthe conclusions, we use 4 registers and do not ablate further due to the high experimental cost. Figure 20a illustrates\nthe conclusions, we use 4 registers and do not ablate further due to the high experimental cost. Figure 20a illustrates\nthe conclusions, we use 4 registers and do not ablate further due to the high experimental cost. Moreover, we quantitatively observe in\nFig. 20b the benefit of incorporating additional register tokens on the ImageNet-1k (IN1k) benchmark.\n\nIntegrating of the Attention Mechanism Recent work by An et al. (2023) proposes a gating\nmechanism to suppress the attention scores for high-norm patch tokens. The rationale behind this approach\nis to reduce the influence of outliers on the attention mechanism. In this work, we observe that the\nproposed gating mechanism does not lead to significant improvements in performance. Specifically, we\nexperiment with two different variants of the attention bias and value gating strategies (An et al., 2023).\n\nFeature Dimension Outliers The introduction of additional registers in the model architecture effectively resolves the issue of high-norm\npatch outliers. However, during the training of 7B models, we observe a distinct type of outlier that emerges\nnot across patches, but within the feature (channel) dimension of the learned representations. Specifically,\nanalysis of feature dimension norms reveals that certain transformer layers and training iterations reveal that some\nfeature dimensions attain exceptionally large magnitudes, even as the norms across patches remain stable.\nInterestingly, these feature dimension outliers exhibit consistently high values across different patches and\ndimensions, a behavior that contrasts with observations reported in (An et al., 2023). Moreover, these outlier\ndimensons consistently persist across the layers of a given model, progressively increasing in magnitude throughout\nthe course of training.",
    "52": "We conduct experiments attempting to neutralize these dimensions during both training and inference. Our\nfindings indicate that there is a performance drop. A significant role during training, as applying L2-regularization\nto suppress them results in a detrimental pace. However, these high-magnitude dimensions do not necessarily\nlead to significant performance gains by simply scaling them down during inference. In Fig. 21, we show our\nquantitative results. Table 21 details of year of publication, performance, and reference of the numbers used in Fig. 1. For all\npapers, we report Top-1 accuracy of this algorithm with the largest model on ImageNet. For weakly- and\nself-supervised models, we provide linear probing performance. For dates, we use the year of first appearance\n\nB Additional Results\n\nB.1 Evolution Over Years\n\nIn Fig. 1, we provide a rough state-of-the-art performance along years. Here, we provide the\nprecise references and performances that we reported in the figure. Please find it in Tab. 21.\n\nB.2 Per-Layer Analysis\n\nSpecifically, we evaluate the quality of the various layers of the DiNOv3 7B model.\nAdditionally, segmentation (ADE20k), depth estimation (NYU), and coordinate regression\n(ObjectNet). For each metric, we report a linear evaluation (Lin.) and a fine-tuning evaluation\n(Fine.). For the segmentation task, it is important to note that all reported results are based on\na standard 224x224 input size. Furthermore, the qualitative results in Fig. 12 demonstrate that\nthe most discriminative layers lie in Fig. 12a and b. Finally, a bottom-to-top analysis of the\nDiNOv3 7B model reveals that the initial layers focus on low-level features, while the later layers\nfocus on high-level semantic information. The qualitative results in Fig. 12 demonstrate that the\nmost discriminative layers lie in Fig. 12a and b.",
    "53": "Table 21: Details of year of publication, performance, and reference of the numbers used in Fig. 1. For all\npapers, we report Top-1 accuracy of this algorithm with the largest model on ImageNet. For weakly- and\nself-supervised models, we provide linear probing performance. For dates, we use the year of first appearance\n\n| Year | Top-1 | Reference |\n|---|---|---|\n| 2012 | 59.3 | Krizhevsky et al. (2012) |\n| 2013 | - | - |\n| 2014 | - | - |\n| 2015 | 78.6 | He et al. (2016) |\n| 2016 | 34.9 | Joulin et al. (2016) |\n| 2017 | 80.9 | Xie et al. (2017) |\n| 2018 | 83.6 | Mahajan et al. (2018) |\n| 2019 | 84.3 | Tan and Le (2019) |\n| 2020 | 85.0 | He et al. (2020) |\n| 2021 | 86.5 | Dosovitskiy et al. (2021) |\n| 2022 | 88.4 | Bolya et al. (2025) |\n| 2023 | - | - |\n| 2024 | - | - |\n\nFor example, for our semantic segmentation (Sec. 6.3.2) and depth estimation (Sec. 6.3.3) using\nfeatures from intermediate layers, we apply batch normalization.\n\nB.3 Evolution Over Years\n\nSpecifically, we evaluate the quality of the various layers of the DiNOv3 7B model. Additionally,\nsegmentation (ADE20k), depth estimation (NYU), and coordinate regression (ObjectNet). For each\nmetric, we report a linear evaluation (Lin.) and a fine-tuning evaluation (Fine.). For the segmentation\ntask, it is important to note that all reported results are based on a standard 224x224 input size.\nFurthermore, the qualitative results in Fig. 12 demonstrate that the most discriminative layers lie in Fig.\n12a and b. Finally, a bottom-to-top analysis of the DiNOv3 7B model reveals that the initial layers focus\non low-level features, while the later layers focus on high-level semantic information. The qualitative\nresults in Fig. 12 demonstrate that the most discriminative layers lie in Fig. 12a and b.",
    "54": "52",
    "55": "Figure 21: Results on five benchmarks using features from intermediate layers of DiNOv3 7B. Evaluations\n(a-c) use a linear layer (see Secs. 6.1.2 and 6.2.1), while (d, e) use a non-parametric approach (see Secs. 6.1.3\nand 6.1.5).\nTable 22: Per-dataset results for finegrained classification on small datasets with linear probing (Fine-S,\nsee Sec. 6.2.1), following Oquab et al. [2024].\n\nMethod\nViT\nFood C10 C100 SUN Cars Airer. VOC DTD Pets Cal01 Flowers CUB Avg\nAM-RADiO2/5 g/14\nWeakly-supervised backbones\nSigLIP g/16\nPE-core g/14\nAimV2 3B/14\nEvaCLIP 18B/14\nSelf-supervised backbones\nFranca g/14\nDiNOv3 g/14\nDiNOv3 7B/16\n\n96.5 99.5 95.0 82.8 95.4 91.7 90.3 88.6 96.7 98.8 99.7 91.5 93.9\n81.1 60.7 93.3 86.7 85.8 91.0 87.7 89.7 90.3 93.7\n97.3 99.3 95.3 84.8 92.0 90.5 88.7 87.1 99.3 99.7 99.3 94.8\n96.6 98.3 95.4 83.8 92.0 90.2 88.4 88.7 97.8 99.7 99.6 94.7\n95.0 99.0 95.0 85.6 92.6 90.7 89.2 89.6 98.6 99.8 99.8 95.5\n96.6 98.6 95.6 86.0 92.8 90.9 89.0 90.1 98.9 99.7 99.7 95.9\n97.1 99.4 95.8 87.2 93.4 91.4 89.8 91.2 99.1 99.9 99.9 96.2\n97.7 99.6 96.3 88.0 94.0 91.8 90.4 92.3 99.3 99.9 99.9 96.6\n\nWe see that our new model DiNOv3 drastically outperforms its predecessor DiNOv2. However, the gap with\nweakly-supervised models remains large. Since our model does not leverage pair image-text data during\ntraining, it has a much harder time learning glyph associations. Recent work from Fan et al. [2025] hints at\nthe impact of training data on the performance on this type of tasks.\n\nB.5 Fairness Analysis\nWe evaluate the geographical fairness of DiNOv3 at the country level for some downstream tasks\nwith income categorization. The protocol of Goyal et al. [2022] for DiNOv3 we adopt utilizes income buckets\nand regions, following the SRB (Goyal et al., 2022) protocol. For each task, we report the performance\nfor each income bucket and region. A disparity is defined as the standard deviation across all groups for a metric.\nWe train for 1M iterations using a fully-shared data-parallel setup in Pytorch, using bfloat16 and\n8-bit floating-point matrix multiplications. We use a constant learning rate of 1e-6 with a warmup of 1000\niterations, a weight decay of 0.04, a learning rate decay factor of 0.88 per 10000 iterations, a stochastic depth\ndropout value of 0.1 and an EMA. We apply the Lpo loss on the class token of each patch.\nFor the few-shot tasks, we utilize 16-bit floating point precision – a 32 x 32 grid is used for the query image,\nfollowing the protocol in Yao et al. [2021]. We utilize a random crop and resize to 224 x 224, then every\n10 steps we evaluate on the validation set. For all of the few-shot tasks (Table 23, 24, 25, 26), we freeze the\nweights of the backbone and train a linear layer on top of the DiNOv3 features.\n\nTable 23: Full results for instance recognition, presenting additional metrics for Sec. 6.2.2.\n\nMethod\nViT\nOxford\nM H\nAmsterTime\nMet\nGAP. ACC\nmAP\nAgglomerative backbones\nAM-RADiO2/5 g/14\n72.8 50.7\n93.3 85.3\n30.5 65.9 69.0\n46.7\nWeakly-supervised backbones\nSigLIP g/16\n49.3 25.3\n79.3 60.9\n0.0 0.0 0.2\n15.5\nPE-core g/14\n57.4 32.7\n83.6 68.9\n10.6 34.8 44.9\n23.1\nAimV2 3B/14\n55.0 28.8\n85.6 71.4\n29.5 67.3 69.9\n23.1\nEvaCLIP 18B/14\n53.2 27.1\n85.6 61.6\n0.5 4.3 11.0\n18.9\nSelf-supervised backbones\nFranca g/14\n44.6 14.3\n73.8 51.6\n27.2 54.3 57.7\n21.1\nDiNOv3 g/14\n73.2 58.2\n92.7 84.6\n44.6 73.0 75.2\n48.9\nDiNOv3 7B/16\n81.1 60.7\n93.8 87.1\n55.2 77.7 80.7\n50.6",
    "56": "Table 24: Comparison of DiNOv3 classification performance with the best DiNOv2 model (g), along with the best weakly-\nsupervised PE-core model (g).\n\nModel\nDiNOv2\nDiNOv3\nPE-core\nViT-g/14\n\nGTSRB\nLogo-2K+\nFlickrLogs-32\nRP2K\nProducts-10K\nSOProducts\n79.2\n52.9\n83.6\n91.4\n70.8\n57.6\n94.5\n93.2\n90.0\n93.1\n80.6\n80.7\n87.5\n86.0\n86.3\n94.7\n74.5\n74.5\n\nTable 25: Comparison of DiNOv3 classification performance on OCR-heavy datasets. These are notoriously\nhard datasets for SSL. We compare DiNOv3 with the best DiNOv2 model (g), along with the best weakly-\nsupervised PE-core model (g).\n\nModel\nDiNOv2\nDiNOv3\nPE-core\nViT-g/14\n\nGTSRB\nLogo-2K+\nFlickrLogs-32\nRP2K\nProducts-10K\nSOProducts\n79.2\n52.9\n83.6\n91.4\n70.8\n57.6\n94.5\n93.2\n90.0\n93.1\n80.6\n80.7\n87.5\n86.0\n86.3\n94.7\n74.5\n74.5\n\nTable 26: Geographical fairness and diversity analysis across income buckets and regions, following the\nprotocol of Goyal et al. [2022b].\n\nIncome Buckets\nRegions\n\nMethod\nArch.\nlow\nmedium\nhigh\nAfrica\nAsia\nAmericas\nEurope\nSEERv2\nRG-10B\n59.7\n78.5\n86.6\n65.9\n76.3\n81.1\n85.6\nDiNOv2\nViT-g/14\n73.2\n58.2\n92.7\n84.6\n44.6\n73.0\n75.2\nDiNOv3\nViT-g/14\n81.1\n60.7\n93.8\n87.1\n55.2\n77.7\n80.7\n\nmoving average of the global crops patch tokens seen by the student are masked\nwith 50% probability; and we apply the Lpo loss to 16 class tokens of the first global crop seen by the teacher\nEMA. We train for 1M iterations using a fully-shared data-parallel setup in Pytorch, using bfloat16 and\n8-bit floating-point matrix multiplications. We use a constant learning rate of 1e-6 with a warmup of 1000\niterations, a weight decay of 0.04, a learning rate decay factor of 0.88 per 10000 iterations, a stochastic depth\ndropout value of 0.1 and an EMA. We apply the Lpo loss on the class token of each patch.\nFor the few-shot tasks, we utilize 16-bit floating point precision – a 32 x 32 grid is used for the query image,\nfollowing the protocol in Yao et al. [2021]. We utilize a random crop and resize to 224 x 224, then every\n10 steps we evaluate on the validation set. For all of the few-shot tasks (Table 23, 24, 25, 26), we freeze the\nweights of the backbone and train a linear layer on top of the DiNOv3 features.",
    "57": "Table 27: Instance recognition with patch size 14/16. For SPAIR, we use images resized to a side length of\n896/1024 pixels for models with patch size 14/16. For all other datasets, we report the correspondence recall,\n% the percentage of correspondences falling into a specified distance.\n\nDatasets and Metrics: Depth Estimation: Linear Probing\n\nDatasets\nNYUv2 (Silberman et al., 2012) and KITTI (Geiger et al., 2013) datasets. Results are reported\nusing the Root Mean Squared Error (RMSE) metric.\n\nEvaluation Protocol: To assess the quality of the dense features, we train a linear classifier on the training\nset of each benchmark. This linear layer is frozen beforehand for some models (after layer normalization).\nFor all backbones, we feature further normalized using a trained batch normalization layer. For all backbones,\nwe perform a hyperparameter sweep using the AdamW optimizer, varying the learning\nrate over [1 × 10−4, 3 × 10−4, 1 × 10−3] and weight decay over [1 × 10−4, 1 × 10−3].\n\nD.2 Depth Estimation:\n\nEvaluation Protocol: Specifically, we subsample 1/4 of the object views, and for each source view select another dest. view within a\nmaximum rotation of 120 degrees to perform patch matching on. For each image pair, each patch of source (within the evaluation) is matched to a patch in the dest. The top-1000\nmatches with highest cosine similarity are kept for evaluation, and a 3D distance error is computed for each\nmatch based on the known camera pose and depth maps of both images. This allows to compute recall errors\nwith varying thresholds, for which we use thresholds of 1cm, 2cm, and 5cm. We then compute the\naverage recall across thresholds as the correspondence recall.\n\nD.5 Video Segmentation Tracking\n\nDatasets and Metrics: For this task, we use the DAVIS (2017) (Pont-Tuset et al., 2017), YouTube-VOS\n(Xu et al., 2018) and MOSE (Ding et al., 2023) datasets. DAVIS defines a training set of 60 videos and\na validation set of 30 videos whereas all frames are annotated with ground-truth instance segmentation\nmasks. For YouTube-VOS, only the training set is annotated and publicly available, while the validation set\nis gained behind an evaluation server. To mimic the DAVIS setup, we take a random subset of 2758 annotated\nframes (80%) as the training set and the remaining 690 videos (20%) as the validation set. In a similar\nfashion, we split the MOSE dataset into 1206 videos for validation and 301 for testing. For all datasets,\nwe evaluate performance using the standard J&F-mean metric (Perazzi et al., 2016), which combines the\nregion similarity (J) and contour accuracy (F) scores. Only the objects annotated in the first frame are\nconsidered in the evaluation.",
    "58": "Evaluation Protocol: Similar to Rajasagaran et al. [2025], we implement a non-parametric protocol for\nlabel propagation between features extracted from a frozen DiNOv3 backbone. Specifically, we label with\ninstance similarity, which is computed as a cosine similarity of the features of the video is labeled with\ninstance segmentation masks, all patches of the current frame, and all patches of the first frame, and all patches of the\ncurrent frame. Focusing on a single patch in the current frame, we consider the k smallest number of past frames.\nNeighbouring on a single patch in the current frame, we label to obtain a prediction for the current patch. After processing one frame, we move to the next one, treating the previous predictions\nas soft instance segmentation labels. When forwarding frames through the backbone, we resize the\npatches size. Patch similarity and label propagation are computed at the resolution of the resulting\nfeatures, then the mask probabilities are bilinearly resized to the native resolution for computing J&F.\nNumber of neighbours k, and the number of past frames to use as context (number of past frames to use as context,\nk: the number of neighbours, and the number of past frames to use as context). We consider several hyperparameter combinations, e.g. the number of past frames to use as context,\nWe consider several hyperparameter combinations, e.g. the number of past frames to use as context, the\nnumber of neighbours k, and the number of past frames to use as context. We report the average performance\nacross all splits. \n\nD.6 Unsupervised Semantic Segmentation\n\nDatasets and Metrics: For this task, we evaluate our model on the ScanNet (Dai et al., 2017) dataset, where\nwe aim to assign a semantic label to each point in a 3D scene. We report the mean Intersection over Union\n(mIoU) as the primary metric. We follow the protocol of Wang et al. [2023] and utilize a single forward pass\nthrough the network for each scene. \n\nEvaluation Protocol: We evaluate on the ScanNet dataset (Dai et al., 2017) using the standard evaluation\nprotocol (Wang et al., 2023). We use a single forward pass through the network for each scene and report the\nmean Intersection over Union (mIoU) as the primary metric. We follow the implementation details from\nWang et al. [2023] and utilize a nearest neighbor classifier with k=1 to assign semantic labels to each point\nin the scene.",
    "59": "Table 28: Performance of DiNOv3 on the Cityscapes dataset for semantic segmentation, following the protocol\nof Wang et al. [2023].\n\nMethod\nDiNOv2\nDiNOv3\nmIoU\n47.8\n52.1\n\nWe observe that DiNOv3 consistently outperforms its predecessor DiNOv2 across all the evaluated tasks.\nThis improvement can be attributed to the enhanced representation learning capabilities of DiNOv3, which\nallows it to capture more nuanced and discriminative features from the input data. \n\nE. Additional Results\n\nIn this section, we present additional results on a wider range of tasks and datasets to further demonstrate\nthe effectiveness of DiNOv3. We evaluate DiNOv3 on several challenging benchmarks, including instance\nrecognition, depth estimation, video segmentation tracking, and unsupervised semantic segmentation.\n\nE.1 Instance Recognition\n\nWe evaluate DiNOv3 on several instance recognition datasets, including Oxford, AmsterTime, and SOP.\nWe follow the protocol of Wang et al. [2023] and utilize a linear classifier on top of the DiNOv3 features.\nTable 23 presents the results for instance recognition, showing that DiNOv3 consistently outperforms\nDiNOv2 and other state-of-the-art methods.\n\nE.2 Depth Estimation\n\nWe evaluate DiNOv3 on the NYUv2 and KITTI datasets for depth estimation. We follow the protocol of\nWang et al. [2023] and train a linear regressor on top of the DiNOv3 features. Table 29 presents the results\nfor depth estimation, showing that DiNOv3 achieves competitive performance compared to other state-of-the-art\nmethods.\n\nE.3 Video Segmentation Tracking\n\nWe evaluate DiNOv3 on the DAVIS and YouTube-VOS datasets for video segmentation tracking. We follow\nthe protocol of Wang et al. [2023] and utilize a non-parametric label propagation algorithm on top of the\nDiNOv3 features. Table 30 presents the results for video segmentation tracking, showing that DiNOv3 achieves\ncompetitive performance compared to other state-of-the-art methods.",
    "60": "Table 27: List of hyperparameters evaluated for video segmentation tracking on the training split of DAVIS 2017 (Pont-Tuset et al., 2017). The best hyperparameters, highlighted, are applied to all datasets.\n\nMax context length\tNeighborhood mask size\tNeighborhood mask shape\tTop-K\tTemperature\n7\t12\tSquare\t5\t0.2\n7\t12\tCircle\t5\t0.2\n7\t24\tSquare\t5\t0.2\n7\t24\tCircle\t5\t0.2\n7\t12\tSquare\t5\t0.1\n7\t12\tCircle\t5\t0.1\n4\t12\tSquare\t5\t0.7\n10\t12\tSquare\t5\t0.2\n15\t12\tSquare\t5\t0.2\n7\t12\tSquare\t3\t0.2\n7\t12\tCircle\t3\t0.2\n15\t24\tSquare\t10\t0.2\n15\t24\tCircle\t10\t0.1\n15\t36\tCircle\t10\t0.1\n15\t∞\tCircle\t10\t0.1\n\n\n\nHeo et al., 2024). After the four blocks, we apply a cross-attention block with a single position-less learnable query to aggregate the information from all patches into a single vector, which is then linearly projected to obtain the final classification logits. The stack of self-attention blocks, the cross-attention block, the positional embeddings, and the final projection constitute the video classifier, which we train for 20 epochs with batch size 64 with a standard cross-entropy loss. In practice, we train a set of classifiers in parallel, one for each combination of learning rate [1 × 10⁻⁴, 2 × 10⁻⁴, 5 × 10⁻⁴, 1 × 10⁻³, 2 × 10⁻³, 5 × 10⁻³] and weight decay [10⁻⁵, 10⁻⁴, 10⁻³]. For each dataset, we use 90% of the training set to update the model parameters, 10% of the training set to choose the best combination of learning rate and weight decay, and finally report performance of the chosen model on the validation split.",
    "61": "Inference. At inference time, we employ a deterministic strategy to sample frames. Specifically, we take the crop the largest center square and resize it to 224 × 224 pixels, possibly augmenting it with the sides of the original video. We then feed these frames to the pre-trained model, then linearly average the predictions of each frame. We equip our baseline (and following baselines) with a temporal augmentation (TTA) by averaging the predictions over 5 center and four spatial (top, bottom, left, right) crops to improve the robustness of the classification results. For the final prediction, we consider the average of the softmax outputs. \n\n\n\nFigure 22: Sampling clips for video classification. Choosing a clip for training or inference determines the coordinates of a spatial crop and which frames/timestamps to sample. At training time, we sample clips at random by choosing random frames from the whole video and by applying Spatial-Net-1k that covers ≥ 40% of the video and frames. Spatially, we take the three largest square crops aligned to the left, middle and right. Temporally, we take two overlapping sets of frames such that they cover as much of the time and timestamps as possible.\n\n\nD.7 Image Classification with Linear Probing\n\nDatasets and Metrics. We evaluate the global quality of the DiNoV3 model using the widely adopted linear probing evaluation. We train a linear transform on the training set (Deng et al., 2009) and evaluate results on the val set. We assess the generalization quality of the model by evaluating the transfer to classification test sets: ImageNet-1k (Recht et al., 2019) and Real-to-ImageNet (Beyer et al., 2020), which provide alternative sets of images and labels for ImageNet-1k to overfitting on the original ImageNet validation set. Additionally, we consider the Rendition (Hendrycks et al., 2021a) and Sketch (Wang et al., 2019) datasets, which present stylized and artificial versions of ImageNet classes; the Adversarial (Hendrycks et al., 2021b) and ObjectNet (Barbu et al., 2019) datasets, which contain deliberately challenging examples; and the corruptions (Hendrycks and Dietterich, 2019) dataset, which measures robustness to common image corruptions. We report the mean corruption accuracy as the evaluation metric for all datasets (Hendrycks and Dietterich, 2019).\n\nFor fine-grained datasets, we consider the same collection of 12 datasets from Oquab et al. (2024), which we call Fine-S: Food-101 (Bossard et al., 2014), CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), Stanford Dogs (Khayyat et al., 2011), FGVC-Aircraft (Majji et al., 2013), VOC 2007 (Everingham et al., 2007), DTD (Cimpoi et al., 2014), Oxford Pets (Parkhi et al., 2012), Caltech101 (Fei-Fei et al., 2004), Flowers (Nilsback and Zisserman, 2008), and CUB200 (Welinder et al., 2010), as well as the larger datasets Places205 (Zhou et al., 2014), iNaturalist 2018 (Horn et al., 2018), and iNaturalist 2021 (Van Horn et al., 2021).",
    "62": "Evaluation Protocol. For the larger datasets ImageNet, Places205, iNaturalist 2018 and iNaturalist 2021, we use the following procedure. For each baseline, we train a linear layer on the final features of the CLS token (after the layer norm) using the ImageNet-1k training set (Deng et al., 2000). Specifically, we use SGD with a momentum of 0.9, and train for 10 epochs with a batch size of 1024. We sweep the learning rates [1 × 10⁻⁴, 2 × 10⁻⁴, 5 × 10⁻⁴, 1 × 10⁻³, 2 × 10⁻³, 5 × 10⁻³] and weight decay values [0, 1e-5, 5e-5] and use the validation set of ImageNet-1k to select the best combination. During training, we use random resize crop augmentation with standard LogisticRegression implementation with the L-BFGS solver.\n\nIn both cases, we evaluate models at resolutions images in 1024 patch tokens, that is, 448 × 448 for patch size 14, and 512 × 512 for patch size 16. The resulting images are resized such that the shorter side matches the chosen side length, then take the central square crop.\n\n\nD.8 Instance Recognition\n\nDatasets and Metrics. We use the Oxford and Paris datasets for landmark recognition (Radeković et al., 2016). The Met dataset featuring artworks from the Metropolitan Museum (Ypsilanti et al., 2021), and AmsterTime, which consists of modern street view images matched to historical archive images of Amsterdam (Dijkstra et al., 2021). For each task, we map images to a 64 × 64 grid for RoI-pooling. For fine-grained recognition, we report the mean Average Precision (mAP) for Oxford Pets, Paris, Met, and AmsterTime, which is computed as the average of the Average Precision across all classes. For the other datasets, we report the classification accuracy. We train a linear classifier on top of the DiNoV3 features. We use a batch size of 32, a learning rate of 1e-3, and a weight decay of 1e-5. We train for 50 epochs and evaluate on the test set.\n\n\n\nFor each patch, we concatenate intermediate features channel-wise, giving a feature dimension of 4:096 = 16384, which is further increased by the windowing strategy described below. The decoder features feed into the encoder, which is a stack of 6 cross attention blocks with embedding dimension of 768. The decoder is a stack of 6 cross attention blocks with the same embedding dimension, where 1500 “one-to-many” queries attend to the patch tokens of the encoders to predict bounding boxes and class labels.\n\nImage Pre-Processing. Training is performed in three stages as described below, one with a base image horizontal flipping (p = 0.5), followed by either (i) random resizing, where the shortest side is scaled to 2048 instead of the base resolution of the stage (1536 or 2048), or (ii) a random crop retaining 60–100% of the original image area, followed by resizing as in (i). At evaluation time, images are resized so that the shortest side is 2048 without additional augmentation, and both sides are rounded up to the nearest multiple of the patch size.\n\nWindowing strategy. We then apply a windowing strategy that combines a global view of the image with local views, and their sizes according to the image resolution. 1. The whole image is resized to 512x576 and forwarded through the backbone, resulting in a feature map of 32 × 48 patch tokens of dimension 16384. These features are then bilinearly upsampled to 96 × 144, matching the size of the windowed feature. 2. Finally, the features maps from 32 non-overlapping windows of size 512 × 576 are forwarded through the backbone, resulting in a feature map of 32 × 48 patch tokens of dimension 16384. This upsample map is 96 × 144. The features from both windows are concatenated channel-wise, giving a feature dimension of 96 × 144 × 32768.",
    "63": "Datasets and Metrics. We evaluate DiNoV3 on semantic segmentation on the ADE20k (Zhou et al., 2017), Cityscapes (Cordts et al., 2016), COCO-Stuff (Caesar et al., 2018), and VOC 2012 (Everingham et al., 2012) datasets. ADE20k is a widely used benchmark for semantic segmentation with 150 semantic categories, varying from outdoor scenery to images of people and objects inside a house. COCO-Stuff and Hypersim (Roberts et al., 2021) datasets are used for pre-training the model. COCO-Stuff is a larger dataset (118k training images) than ADE20k containing 80 thing classes and 91 stuff classes, while Hypersim is a photorealistic synthetic dataset presenting indoor scenes with 40 semantic categories, making it a good candidate for helping the model learn rich information of the scenes. The evaluation metric reported is mIoU for all datasets.\n\nArchitecture. We adapt the ViT-Adapter and Mask2Former configurations that other baselines use (Wang et al., 2023a), with several differences. First, to ensure that our backbone remains frozen and its activations output features to be directly used by the injector component of ViT-Adapter. This makes our backbone decoder 2048 as the default token dimension of our backbone output dimension of 1024 or 1536. As inputs to the decoder, we extract features from four intermediate layers of the DiNoV3 TB/16 backbone, namely layers [10, 20, 30, 40]. We apply the final layer norm to the features of all layers and add a learned batch normalization.\n\nTraining Protocol. For generating results on COCO-Stuff, we train the model using a cosine scheduler, with a 50k iterations. As for training on the other datasets – ADE20k, Cityscapes and VOC 2012 – we first pre-train the baselines’ backbones with a 6k linear warmup and a 50k iterations, following a cosine scheduler. COCO-Stuff for 80k iterations, with a 6k linear warmup and a 50k iterations, following a cosine scheduler. We employ a fully convolutional network for the semantic segmentation task. The network is trained with a standard cross-entropy loss and stochastic weight averaging (SWA) (Izmailov et al., 2020). We use an initial learning rate of 1e-4, a batch size of 16 per GPU, and a weight decay of 1e-5. We train the model on 8 GPUs. For the evaluation, we resize the input images to 1024 × 1024 and perform inference with a single scale.",
    "64": "D.10 Semantic Segmentation\n\nDatasets and Metrics. We evaluate DiNoV3 on semantic segmentation on the ADE20k (Zhou et al., 2017), Cityscapes (Cordts et al., 2016), COCO-Stuff (Caesar et al., 2018), and VOC 2012 (Everingham et al., 2012) datasets. ADE20k is a widely used benchmark for semantic segmentation with 150 semantic categories, varying from outdoor scenery to images of people and objects inside a house. COCO-Stuff and Hypersim (Roberts et al., 2021) datasets are used for pre-training the model. COCO-Stuff is a larger dataset (118k training images) than ADE20k containing 80 thing classes and 91 stuff classes, while Hypersim is a photorealistic synthetic dataset presenting indoor scenes with 40 semantic categories, making it a good candidate for helping the model learn rich information of the scenes. The evaluation metric reported is mIoU for all datasets.\n\nArchitecture. We adapt the ViT-Adapter and Mask2Former configurations that other baselines use (Wang et al., 2023a), with several differences. First, to ensure that our backbone remains frozen and its activations output features to be directly used by the injector component of ViT-Adapter. This makes our backbone decoder 2048 as the default token dimension of our backbone output dimension of 1024 or 1536. As inputs to the decoder, we extract features from four intermediate layers of the DiNoV3 TB/16 backbone, namely layers [10, 20, 30, 40]. We apply the final layer norm to the features of all layers and add a learned batch normalization.\n\nTraining Protocol. For generating results on COCO-Stuff, we train the model using a cosine scheduler, with a 50k iterations. As for training on the other datasets – ADE20k, Cityscapes and VOC 2012 – we first pre-train the baselines’ backbones with a 6k linear warmup and a 50k iterations, following a cosine scheduler. COCO-Stuff for 80k iterations, with a 6k linear warmup and a 50k iterations, following a cosine scheduler. We employ a fully convolutional network for the semantic segmentation task. The network is trained with a standard cross-entropy loss and stochastic weight averaging (SWA) (Izmailov et al., 2020). We use an initial learning rate of 1e-4, a batch size of 16 per GPU, and a weight decay of 1e-5. We train the model on 8 GPUs. For the evaluation, we resize the input images to 1024 × 1024 and perform inference with a single scale.",
    "65": "layers of the DINOv3 7B/16 backbone, namely layers [10, 20, 30, 40]. We apply the final layer norm to the features of all layers and add a learned batch normalization. The depth estimation output is discretized into 256 uniformly distributed bins covering the range from 0.001m to 100m. Training employs a base learning rate of 1e-3, scheduled using PolyLR with a power of 0.5 and an initial linear warmup phase lasting 12k iterations. To enhance robustness and generalization, we apply a suite of augmentations: Gaussian blur, Gaussian noise, AutoContrast, and Equalize, ColorJitter, rotation, and left-right flips.\n\n**Datasets and Metrics.** We train the model on the dataset of DaV2, which consists of synthetically generated images from the IRS, TartanAir, BlendedMVS, Hypersim, and VKITTI2 datasets. We evaluate on five datasets: NYUv2 (Silberman et al., 2012), KITTI (Geiger et al., 2013), ETH3D (Schops et al., 2017), ScanNet from (Ke et al., 2020), and DIODE (Vasiljevic et al., 2019). We adopt the zero-shot scale-invariant depth setup, where the ground metric uses absolute relative error and δ₁ (see (Yang et al., 2024a)).\n\nD.12 Visual Geometry Transformers with DINOv3\n\n**Implementation Details.** Compared to the original VGGT (Wang et al., 2025), we adopt the following changes: (1) we use an image size of 592 instead of 518; this is to match the number of patch tokens that DINOv2 produces, (2) adopting a smaller learning rate, specifically from 0.0002 to 0.0001, and (3) using a concatenation of the four intermediate layers of DINOv3 ViT-L rather than just the last layer as input to the downstream modules. Interestingly, we found that using four intermediate layers brings a benefit for DINOv3, whereas doing the same for DINOv2 brings no additional performance gains. We also experimented with a version closer to the original VGGT setup (image size 512, same learning rate, final layer), and already found this untuned version to improve over the original VGGT work across all tested benchmarks.\n\nD.13 Geospatial\n\n**Evaluation details.** In all of the evaluations, we keep the backbone frozen and only train lightweight classifiers or decoders that are specialized for the tasks. For 3D scene classification, we use a linear classifier for the full training set as well as a few-shot setting (N-way, K-shot). For semantic segmentation, we adopt a U-Net architecture. The following datasets are used: Satlas, OpenStreetMap, and Microsoft Footprint. Details of the datasets are shown in Table 28. We report the overall accuracy, IoU, and F1-score. The implementation is based on PyTorch 2.0 and CUDA 11.7.\n\nD.14 Additional Results\n\nWe show additional results for the zero-shot transfer setting in Figure 14. We also show the results for the few-shot transfer setting in Figure 15. We observe that DINOv3 consistently outperforms DINOv2 across all tasks and datasets.",
    "66": "Table 28: Description of the Satlidas dataset.\n\n| Subdataset | Path | Amount of tiles | Purpose |\n|---|---|---|---|\n| Kalimantan | https://lunar.uml.org/cgi-bin/dataset_pl?dataset_id=1540 | 68 | train/val/test |\n| OpenDC | https://opendata.esri.com/datasets/2020-lidar-classified-land | 37 | train/val/test |\n| Brazil | https://www.esri.com/en-us/news/arcgis-storymaps/2018/10-19/story | 144 | train/val/test |\n| Mozambique | https://data.neonscience.org/data-products/dps.0018 | 14 | train/val/test |\n| Neon | https://portal.opentopography.org/datasetMetadata?ID=OP_00201.0339.1 | 5366 | train/val/test |\n| CA20Group | https://portal.opentopography.org/datasetMetadata?ID=OP_00201.0339.2 | 99 | train/val/test |\n| CA17Duvall | https://portal.opentopography.org/datasetMetadata?ID=OP_00202.0339.1 | 56 | train/val/test |\n| Netherlands | https://www.udl.nl/ | 13 | train/val/test |\n| Sao Paulo | https://www.esri.com/en-us/globe/lidar-forest-brazil.html | 4 | test |\n| CA brande | https://www.esri.com/en-us/globe/story/0020/0526118 | 1 | test |"
  },
  "skipped_pages": [
    0,
    1,
    2,
    3,
    4,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    30,
    31,
    32,
    33,
    34,
    35,
    36,
    37,
    38,
    39
  ]
}