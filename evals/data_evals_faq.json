{
  "response": "## Key Points from the Text: AI Evals & LLM Evaluation FAQ\n\nHere's a breakdown of the key points, organized for clarity:\n\n**I. Core Principles of LLM Evaluation (Evals)**\n\n*   **Error Analysis is Paramount:**  The most important activity. It identifies unique failure modes for *your* application and data, guiding which evals to write.\n*   **Application-Specific Metrics:** Focus on metrics that emerge from your data and failures, not generic ones (like \"hallucination\") which often miss crucial domain-specific issues.\n*   **Iterative Process:** Evals are not a one-time thing. Regularly revisit error analysis, especially after significant changes to your system.\n*   **Don't Skip Error Analysis:** It ensures your evaluations are grounded in real-world behavior, not misleading generic metrics.\n\n**II. Error Analysis Process (Detailed)**\n\n1.  **Dataset Creation:** Gather representative traces (user interactions with the LLM). Generate synthetic data if needed, but prioritize real data.\n2.  **Open Coding:** Human annotators (ideally a \"benevolent dictator\" - a domain expert) review traces and write free-form notes about issues. Focus on the first failure observed.\n3.  **Axial Coding:** Categorize the open-ended notes into a \"failure taxonomy\" (group similar failures). Count failures in each category. LLMs can *help* with this step.\n4.  **Iterative Refinement:**  Continue reviewing traces until you reach \"theoretical saturation\" (no new failure modes emerge). Aim for at least 100 traces.\n\n**III. Practical Implementation & Tools**\n\n*   **Custom Annotation Tools:** Build your own! They are highly valuable for showing context, streamlining workflow, and coordinating teams.\n*   **Smart Sampling:** Don't review traces randomly. Use outlier detection, user feedback, and metric-based sorting to prioritize potentially problematic traces.\n*   **Human Annotation:**  For most teams, a single, knowledgeable \"benevolent dictator\" is more effective than multiple annotators.  If multiple annotators are needed, measure agreement (Cohen's Kappa).\n*   **Outsourcing Caution:** Avoid outsourcing error analysis. It breaks the feedback loop and requires domain expertise.\n*   **LLM Assistance:** LLMs can *help* with axial coding, suggesting prompt improvements, and analyzing data, but *not* replace human judgment.\n*   **CI/CD vs. Production Evaluation:** CI uses small, curated datasets. Production uses sampling of live traffic.\n\n**IV. Specific Evaluation Scenarios**\n\n*   **RAG Evaluation:** Evaluate retrieval (using IR metrics) *separately* from generation.\n*   **Multi-Turn Conversations:** Simplify to isolate failures. Log the entire trace.\n*   **Human Handoffs:**  Capture the *complete* user journey, including the handoff. Evaluate handoff quality and rate.\n*   **Agentic Workflows:** Evaluate end-to-end task success *and* individual step performance.\n*   **Synthetic Data:** Use cautiously.  It's best for supplementing real data, not replacing it.  Start with defining dimensions and failure hypotheses.\n\n**V. General Advice**\n\n*   **Binary Evaluations:** Prefer pass/fail over Likert scales for clarity and consistency.\n*   **Eval-Driven Development:** Generally avoid. Start with error analysis, then build evaluators for discovered failures.\n*   **Ready-to-Use Metrics:**  Be skeptical. They often don't reflect your specific needs.\n*   **Model Selection:** Don't overemphasize model selection. Focus on error analysis and data.\n*   **Budget Allocation:** Evaluation is part of the development process, not a separate line item.\n\n\n\n**Discount Code:**  `bit.ly/evals-ai` (35% off AI Evals Course)",
  "extracted_text": {
    "1": "AI Evals Course: 35% off at bit.ly/evals-ai\nQ: Can I use the same model for both the main task and evaluation? . . . . . . . . . 15\nHuman Annotation & Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nQ: How many people should annotate my LLM outputs? . . . . . . . . . . . . . . . . . 16\nQ: Should product managers and engineers collaborate on error analysis? How? . . . 16\nQ: Should I outsource annotation & labeling to a third party? . . . . . . . . . . . . . 17\nQ: What parts of evals can be automated with LLMs? . . . . . . . . . . . . . . . . . . 19\nQ: Should I stop writing prompts manually in favor of automated tools? . . . . . . . 20\nTools & Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nQ: Should I build a custom annotation tool or use something off-the-shelf? . . . . 21\nQ: What makes a good custom interface for reviewing LLM outputs? . . . . . . . . . 21\nQ: What gaps in eval tooling should I be prepared to fill myself? . . . . . . . . . . . 25\nQ: Seriously Hamel. Stop the bullshit. What’s your favorite eval vendor? . . . . . . 27\nProduction & Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nQ: How are evaluations used differently in CI/CD vs. monitoring production? . . . 27\nQ: What’s the difference between guardrails & evaluators? . . . . . . . . . . . . . . . 28\nQ: Can my evaluators also be used to automatically fix or correct outputs in production? 28\nQ: How much time should I spend on model selection? . . . . . . . . . . . . . . . . . 29\nDomain-Specific Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nQ: Is RAG dead? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nQ: How should I approach evaluating my RAG system? . . . . . . . . . . . . . . . . . 30\nQ: How do I choose the right chunk size for my document processing tasks? . . . . . 31\nQ: How do I debug multi-turn conversation traces? . . . . . . . . . . . . . . . . . . . 33\nQ: How do I evaluate sessions with human handoffs? . . . . . . . . . . . . . . . . . . . 34\nQ: How do I evaluate complex multi-step workflows? . . . . . . . . . . . . . . . . . . 34\nQ: How do I evaluate agentic workflows? . . . . . . . . . . . . . . . . . . . . . . . . . 300\n\nThis document curates the most common questions Shreya and I received while teaching AI\nengineers & PMs AI Evals. Warning: AI Evals are sharp opinions about what works in most\ncases. They are not universal truths. Use your judgment.\n\nIf you want to learn more about AI Evals, check out our AI Evals course. Here is a 35% discount code for readers.\n2",
    "2": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nListen to the audio version of this FAQ\nIf you prefer to listen to the audio version (narrated by AI), you can play it here.\n\nGetting Started & Fundamentals\n\nQ: What are LLM Evals?\nIf you are completely new to product-specific LLM evals (not foundation model benchmarks), see these posts: part 1, part 2 and part 3. Otherwise, keep reading.\n\nFocus view\n\nQ: What is a trace?\nA trace is the complete record of all actions, messages, tool calls, and data retrievals from a single initial user query through to the final response. It includes every step across all agents, tools, and system components in a session: multiple user messages, assistant responses, retrieved documents, and intermediate tool interactions.\n\nNote on terminology: Different observability vendors use varying definitions of traces and spans. Alex Strick van Linschoten’s analysis highlights these differences (screenshot below).\n\n3",
    "4": "Q: How much of my development budget should I allocate to evals?\n\nIt’s important to recognize that evaluation is part of the development process rather than a\ndistinct line item, similar to how debugging is part of software development.\n\nYou should always be doing error analysis. When you discover issues through error analysis,\nmany will be straightforward bugs you’ll fix immediately. These fixes don’t require separate\nevaluation infrastructure as they’re just part of development.\n\nThe decision to build automated evaluators comes down to cost-benefit analysis. If you can\ncatch an error with a simple assertion or regex check, the cost is minimal and probably worth\nit. But if you need to align an LLM-as-judge evaluator, consider whether the failure mode\nwarrants that investment.\n\nIn the projects we’ve worked on, we’ve spent 60-80% of our development time on errors\n(i.e. looking at data) rather than building automated checks.\n\nBe wary of optimizing for high eval pass rates. If you’re passing 100% of your evals, you’re\nlikely not challenging your system enough. A 70% pass rate might indicate a more meaningful\nevaluation that’s actually stress-testing your application. Focus on evals that help you catch\nreal issues, not ones that make your metrics look good.\n\nFocus view\n\nQ: Will today’s evaluation methods still be relevant in 5-10 years given how fast\nAI is changing?\n\nYes. Even with perfect models, you still need to verify you’re solving the right problem.\nThe need for systematic error analysis, domain-specific testing, and monitoring will still be\nimportant.\n\nToday’s prompt engineering tricks might become obsolete, but you’ll still need to understand\nfailure modes. Additionally, a LLM cannot read your mind, and research shows that people\nneed to observe the LLM’s behavior in order to properly externalize their requirements.\n\nFor deeper perspective on this debate, see these two viewpoints: “The model is the product”\nversus “The model is NOT the product”.\n\nFocus view",
    "5": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nError Analysis & Data Collection\nQ: Why is “error analysis” so important in LLM evals, and how is it performed?\nError analysis is the most important activity in evals. Error analysis helps you decide what evals to write in the first place. It allows you to identify failure modes unique to your application and data. The process involves:\n\n1. Creating a Dataset\nGathering representative traces to focus on interactions with the LLM. If you do not have any data, you can generate synthetic data to get started.\n\n2. Open Coding\nHuman annotator(s) (ideally a benevolent dictator) review and write open-ended notes about traces, noting any issues. This process is akin to “journaling,” and is adapted from qualitative research methodologies. When beginning, it is recommended to noting the first failure observed in a trace, as upstream errors can cause downstream issues, though you can also tag all independent failures if feasible. A domain expert should be performing this step.\n\n3. Axial Coding\nCategorize the open-ended notes into a “failure taxonomy.” In other words, group similar failures into distinct categories. This is the most important step. At the end, count the number of failures in each category. You can use a LLM to help with this step.\n\n4. Iterative Refinement\nKeep iterating on more traces until you reach theoretical saturation, meaning new traces do not seem to reveal new failure modes or information to you. As a rule of thumb, you should aim to review at least 100 traces.\nYou should frequently revisit this process. There are advanced ways to sample data more efficiently, like clustering, sorting by user feedback, and sorting by high probability failure patterns. Over time, you’ll develop a “nose” for where to look for failures in your data.\nDo not skip error analysis. It ensures that the evaluation metrics you develop are supported by real application behaviors instead of counter-productive generic metrics (which most platforms nudge you to use). For examples of how error analysis can be helpful, see this video, or this blog post.\n\n6",
    "6": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nHere is a visualization of the error analysis process by one of our students, Pawel Huryn - including how it fits into the overall evaluation process:\n\n7",
    "7": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nHow to Perform Error Analysis\nGeneric Metrics Don’t Work Application-specific metrics\n“Generic metrics, such as “hallucination” or “toxicity” often miss domain-specific issues. Their abuse is endemic in the industry. The most successful teams\nfocus on data, identify failure modes, and let app-specific metrics emerge.\"\n\nHamel Husain and Shreya Shankar\nML Engineers, 2+ years of experience\nbuilding & evaluating AI systems.\nStep 2: Read and Open Code Traces\nOpen Coding: Write brief, descriptive notes with problems, surprises, and incorrect behaviors.\nTrace 01:\nBullet points\nformatting error\nTrace 02:\nIncorrect\ndoesn’t exist\nTrace 03:\nHallucinates\nthat LLM\nStep 3: Axial Coding, Refine Failure Modes (categories)\nCluster similar notes and refine definitions and merge categories. You naturally emerge if you use an LLM, always review signals to identify traces worth reviewing, even if they don’t directly measure quality.\n1 Add New Traces\n(Real or Synthetic)\n2 Read And Open\nCode Traces\n3\nAxial Coding\nRefine Failure Modes\n4\nRe-Code Traces\nWith Failure Modes\nAs a rule of thumb, you need ~100 high-quality, diverse traces.\nThese can be real data, synthetic data, or both, focused on\nspecific failure modes.\nStep 1: (Optional) Generate Synthetic Traces\nDon’t generate synthetic data without hypotheses about where\nlook for failure. Guarantee coverage of all dimensions. Synthetic data is a\nsupplement to real data.\n\nFailure mode\nRequest\nResponse\nMy name is ()\nI’m sorry ()\nDo you name ()\nI’m sorry ()\nRequest should be inspected\nFail\nInvalid request categorization\n\nOnce you perform initial error analysis, you can mount automated\nevals, for single or monthly models, and fail fast with automated\nchecks.\n\n8",
    "8": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nQ: How do I surface problematic traces for review beyond user feedback?\nWhile user feedback is a good way to home in on problematic traces, other methods are also useful. Here are three complementary approaches:\n\nStart with random sampling\nThe simplest approach is reviewing a random sample of traces. If you find few issues, escalate to stress testing: create prompts that deliberately test your prompt constraints to see if the AI follows your rules.\n\nUse evals for initial screening\nUse existing evals to find problematic traces and potential issues. Once you’ve identified these, you can proceed with the typical evaluation process starting with error analysis.\n\nLeverage efficient sampling strategies\nFor more sophisticated trace discovery, use outlier detection, metric-based sorting, and stratified sampling to find interesting traces. Generic metrics can serve as exploration starting points, even if they don’t directly measure quality.\n\nFocus view\n\nQ: How often should I re-run error analysis on my production system?\nRe-run error analysis when making significant changes: new features, prompt updates, model switches, or major bug fixes. A useful heuristic is to set a goal for reviewing at least 100+ fresh traces each review cycle. Typical review cycles lie we’ve seen range from 2-4 weeks. See this FAQ on how to sample traces effectively.\n\nBetween major analyses, review 10-20 traces weekly, focusing on outliers: unusually long conversations, sessions with multiple retries, or traces flagged by automated monitoring. Adjust frequency based on system stability and usage growth. New systems need weekly analysis until failure patterns stabilize. Mature systems might need only monthly analysis unless usage patterns change. Always analyze after incidents, user complaint spikes, or metric drift. Scaling usage introduces new edge cases.\n\nFocus view\n\n9",
    "9": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nQ: What is the best approach for generating synthetic data?\nCommon mistake is prompting an LLM to “give me test queries” without structure, resulting in generic, repetitive outputs. A structured approach using dimensions produces far better synthetic data for testing LLM applications.\n\nStart by defining dimensions: categories that describe different aspects of user queries.\n\n• For a recipe search, dimensions might include Dietary Restriction (vegan, gluten-free, none),\nCuisine Type (Italian, Asian, comfort food), and Query Complexity (simple recipe,\nmulti-step, edge case).\n• For a customer support bot, dimensions could be Issue Type (billing, technical, general),\nCustomer Mood (frustrated, neutral, happy), and Prior Context (new issue, follow-up,\nresolved).\n\nStart with failure hypotheses. If you lack intuition about failure modes, use your application extensively or recruit friends to use it. Choose dimensions targeting those likely failures.\n\nCreate tuples manually first: Create tuples by hand—specifically combinations selecting one value from each dimension. Example:\n(Vegan, Italian, Multi-step). This manual work helps you understand your problem space.\n\nScale with two-step generation:\n1. Generate structured tuples: Have the LLM create more combinations like (Gluten-\nfree, Asian, Simple)\n2. Convert tuples to queries: In a separate prompt, transform each tuple into natural\nlanguage\nThis separation avoids repetitive phrasing. The (Vegan, Italian, Multi-step) tuple becomes: “I\nneed a dairy-free lasagna recipe that I can prep the day before.”\n\nGeneration approaches\nCross product then filter: Generate all dimension combinations, then filter with an LLM.\nGuarantee coverage including edge cases. Use when most combinations are valid.\nDirect LLM generation: Ask the LLM to generate tuples directly. More realistic but tends\ntoward generic outputs and misses rare scenarios. Use when many dimension combinations\nare invalid.\n\n10",
    "10": "AI Evals Course: 35% off at bit.ly/evals-ai\nFix obvious problems first: Don’t generate synthetic data for issues you can fix immediately.\nIf your prompt doesn’t mention dietary restrictions, fix the prompt rather than generating\nspecialized test queries.\nAfter iterating on your tuples and prompts, run these synthetic queries through your\nactual system to capture full traces. Sample 100 traces for error analysis. This num-\nber provides enough traces to manually review and identify failure patterns without being\noverwhelming.\nFocus view\nQ: Are there scenarios where synthetic data may not be reliable?\nYes: synthetic data can mislead or mask issues. For guidance on generating synthetic data\nwhen appropriate, see What is the best approach for generating synthetic data?\nCommon scenarios where synthetic data fails:\n1. Complex domain-specific content: LLMs often miss the structure, nuance, or quirks\nof specialized documents (e.g., legal filings, medical records, technical forms). Without\nreal examples, critical edge cases are missed.\n2. Low-resource languages or dialects: For low-resource languages or dialects, LLM-\ngenerated samples are often unrealistic. Evaluations based on them won’t reflect actual\nperformance.\n3. When validation is impossible: If you can’t verify synthetic sample realism (due\nto domain complexity or lack of ground truth), real data is important for accurate\nevaluation.\n4. High-stakes domains: In high-stakes domains (medicine, law, emergency response),\nsynthetic data often lacks subtlety and edge cases. Errors here have serious consequences,\nand manual validation is difficult.\n5. Underrepresented user groups: For underrepresented user groups, LLMs may mis-\nrepresent context, values, or challenges. Synthetic data can reinforce biases in the train-\ning data of the LLM.\nFocus view\n11",
    "11": "Q: How do I approach evaluation when my system handles diverse queries?\n\nComplex applications often support vastly different query patterns—from “What’s the return policy?” to “Compare pricing trends across regions for products matching specific criteria.” Each query type exercises different system capabilities, leading to confusion on how to design eval criteria.\n\nError Analysis is all you need. Your evaluation strategy should emerge from observed failure patterns (e.g. error rates, not predetermined query classifications. Rather than creating a massive evaluation matrix covering every query type you can imagine, let your system’s actual behavior guide where you invest evaluation effort.\n\nDuring error analysis, you’ll likely discover that certain query categories share failure patterns. For instance, all queries requiring temporal reasoning might struggle regardless of whether they’re simple lookups or complex aggregations. Similarly, queries that need to combine information from multiple sources might fail in consistent ways. These patterns discovered through error analysis drive your evaluation priorities. It could be that query category is a fine way to group failures, but you don’t know that until you’ve analyzed your data.\n\nTo see an example of basic error analysis in action, see this video.\n\nFocus view\n\nQ: How can I efficiently sample traces for review?\n\nIt can be cumbersome to review traces randomly, especially when most traces don’t have an error. These sampling strategies help you find traces more likely to reveal problems:\n\n• Outlier detection: Sort by any metric (response length, latency, tool calls) and review extremes.\n• User feedback signals: Prioritize traces with negative feedback, support tickets, or escalations.\n• Metric-based sorting: Generic metrics can serve as exploration signals to find interest- ing traces. Review both high and low scores and treat them as exploration clues. Based on what you learn, you can build custom evaluators for the feature modes you find.\n• Stratified sampling: Group traces by key dimensions (user type, feature, query cate- gory) and sample from each group.\n• Embedding clustering: Generate embeddings of queries and cluster them to reveal natural groupings. Sample proportionally from each cluster, but oversample small clus- ters for edge cases. There’s no right answer for clustering—it’s an exploration technique to surface patterns you might miss manually.\n\n12",
    "12": "As you get more sophisticated with how you sample, you can incorporate these tactics into the design of your annotation tools.\nFocus view\nEvaluation Design & Methodology\nQ: Why do you recommend binary (pass/fail) evaluations instead of 1-5 ratings (Likert scales)?\nEngineers often believe that Likert scales (1-5 ratings) provide more information than binary evaluations, allowing them to track gradual improvements. However, this added complexity often creates more problems than it solves in practice.\nBinary evaluations force clearer thinking and more consistent labeling. Likert scales introduce significant challenges: the difference between adjacent points (like 3 vs 4) is subjective and inconsistent across annotators, detecting statistical differences requires larger sample sizes, and annotators often default to middle values to avoid making hard decisions.\nHaving binary options forces people to make a decision rather than hiding uncertainty in middle values. Binary decisions are also faster to make during error analysis – you don’t waste time debating whether something is a 3 or 4.\nFor tracking gradual improvements, consider measuring specific sub-components with their own binary checks rather than using a scale. For example, instead of rating factual accuracy 1-5, you could track “4 out of 5 expected facts included” as separate binary checks. This preserves the ability to measure progress while maintaining clear, objective criteria and usually not necessary.\nStart with binary labels to understand what ‘bad’ looks like. Numeric labels are advanced and usually not necessary.\nFocus view\nQ: Should I practice eval-driven development?\nGenerally no. Eval-driven development (writing evaluators before implementing features) sounds appealing but creates more problems than it solves. Unlike traditional software where failure modes are predictable, LLMs have infinite surface area for potential failures. You can’t anticipate what will break.\nA better approach is to start with error analysis. Write evaluators for errors you discover, not errors you imagine. This avoids getting blocked on what to evaluate and prevents wasted effort on metrics that have no impact on actual system quality.\n\n13",
    "13": "Exception: Eval-driven development may work for specific constraints where you know ex-\nactly what success looks like. If adding \"never mention competitors,\" writing that evaluator\nearly may be acceptable.\n\nThe most importantly, always do a cost-benefit analysis before implementing an eval. Ask whether\nthe failure mode justifies the investment. Error analysis reveals which failures actually matter\nfor your users.\n\nFocus view\n\nQ: Should I build automated evaluators for every failure mode I find?\n\nFocus automated evaluators on failures that persist after fixing your prompts. Many teams\ndiscover their LLM doesn’t meet preferences they never actually specified - like wanting short\nresponses, specific formatting, or step-by-step reasoning. Fix these obvious gaps first before\nbuilding complex evaluation infrastructure.\n\nConsider the cost hierarchy of different evaluator types. Simple assertions and reference-\nbased checks (comparing against known correct answers) are cheap to build and maintain.\nLLM-as-Judge evaluators require 100+ labeled examples, ongoing weekly maintenance, and\ncoordination between developers, PMs, and domain experts. This cost difference should shape\nyour evaluation strategy.\n\nOnly build expensive evaluators for problems you’ll iterate on repeatedly. Since LLM-as-Judge\ncomes with significant overhead, save it for persistent generalization failures - not issues you\ncan fix trivially. Start with cheap code-based checks where possible: regex patterns, structural\nvalidation, or execution tests. Reserve complex evaluation for subjective qualities that can’t\nbe captured by simple rules.\n\nFocus view\n\nQ: Should I use “ready-to-use” evaluation metrics?\n\nNo. Generic evaluations waste time and create false confidence. (Unless you’re using\nthem for exploration).\n\nOne instructor noted:\n\n“All you get from using these prefab evals is you don’t know what they actually\ndo and in the best case they waste your time and in the worst case they create an\nillusion of confidence that is unjustified.”¹\n\n¹Eleanor Berger, our wonderful TA.\n\n14",
    "15": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nNegative Rate (TNR) with your judge on a held out labeled test set rather than avoiding the same model family. You can use these metrics on the test set to understand how well your judge is doing.\n\nWhen selecting judge models, start with the most capable models available to establish strong alignment with human judgments. You can optimize for cost later once you’ve established reliable evaluation criteria. We do not recommend using the same model for open ended preferences or response quality (but we don’t recommend building judges this way in the first place!).\n\nFocus view\n\nHuman Annotation & Process\n\nQ: How many people should annotate my LLM outputs?\n\nFor most small to medium-sized companies, appointing a “benevolent dictator” as a “benevolent dictator” is the most effective approach. Empower domain experts to evaluate actual health chatbot, a lawyer for legal document analysis, or a customer service director for support automation—becomes the definitive voice on quality standards.\n\nA single expert eliminates annotation conflicts and prevents the paralysis that comes from “too many cooks in the kitchen”. The benevolent dictator can incorporate input and feedback from others, but they drive the process. If you feel like you need five subject matter experts to judge a single interaction, it’s a sign your product scope might be too broad.\n\nHowever, larger organizations or those operating across multiple domains (like a multinational company with different cultural contexts) may need multiple annotators. When you do use multiple people, you’ll need to measure their agreement using metrics like Cohen’s Kappa, which accounts for agreement beyond chance. However, use your judgment. Even in larger companies, a single expert is often enough.\n\nStart with a benevolent dictator whenever feasible. Only add complexity when your domain demands it.\n\nFocus view",
    "16": "Q: Should product managers and engineers collaborate on error analysis? How?\n\nAs time goes on you should learn to collaborate to establish shared context. Engineers catch technical issues like retrieval issues and tool errors. PMs identify product failures like unmet user expectations, confusing responses, or missing features users expect.\n\n16",
    "17": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nQ: Should I outsource annotation & labeling to a third party?\n\nOutsourcing error analysis is usually a big mistake (with some exceptions). The core of evaluation is building the product intuition that only comes from systematically analyzing your system’s failures. You should be skeptical of this process being delegated.\n\nThe Dangers of Outsourcing\n\nWhen you outsource annotation, you often break the feedback loop between observing a failure and understanding how to improve the product. Problems with outsourcing include:\n\n* Superficial Labeling: Even well-defined metrics require nuanced judgment that external teams lack. A critical misstep in error analysis is excluding domain experts from the labeling process. Outsourcing this task to those without domain expertise, like general developers or IT staff, often leads to superficial or incorrect labeling.\n* Loss of Unspoken Knowledge: A principal domain expert possesses tacit knowledge and user understanding that cannot be fully captured in a rubric. Involving these experts helps uncover their preferences and expectations, which they might not be able to fully articulate upfront.\n* Annotation Conflicts and Misalignment: Without a shared context, external annotators can create more disagreement than they resolve. Achieving alignment is a challenge even for internal teams, which means you will spend even more time on this process.\n\nInstead of outsourcing, focus on building an internal evaluation process.\n\n1. Appoint a “Benevolent Dictator”. For most teams, the most effective strategy is to appoint a single, internal domain expert as the final decision-maker on quality. This individual sets the standard, ensures consistency, and develops a sense of ownership.\n\n17",
    "18": "AI Evals Course: 35% off at bit.ly/evals-ai\n\n2. Use a collaborative workflow for multiple annotators. If multiple annotators are necessary, follow a structured process to ensure alignment: * Draft an initial rubric with clear Pass/Fail definitions and examples. * Have each annotator label a shared set of traces independently. * Measure Inter-Annotator Agreement (IAA) using Cohen’s Kappa. * Facilitate alignment sessions to discuss disagreements and refine the rubric. * Iterate on this process until agreement is consistently high.\n\nHow to Handle Capacity Constraints\n\nBuilding internal capacity does not mean you have to label every trace. Use these strategies to manage the workload:\n\n* Smart Sampling: Review a small, representative sample of traces thoroughly. It is more effective to analyze 100 traces than to superficially label thousands.\n* The “Think-Aloud” Protocol: To make the most of limited expert time, use this technique from usability testing. Ask an expert to verbalize their thought process while reviewing a handful of traces. This method can uncover deep insights in a single one-hour session.\n* Build Lightweight Custom Tools: Build custom annotation tools to streamline the review process, increasing throughput.\n\nExceptions for External Help\n\nWhile outsourcing the core error analysis process is not recommended, there are some scenarios where external help is appropriate:\n\n* Purely Mechanical Tasks: For highly objective, unambiguous tasks like identifying a phone number or validating an email address, external annotators can be used after a rigorous internal process has defined the rubric.\n* Tasks Without Product Context: Well-defined tasks that don’t require understanding your product’s specific requirements can be outsourced. Translation is a good example: it requires linguistic expertise but not deep product knowledge.\n* Engaging Subject Matter Experts: Hiring external SMEs to act as your internal domain experts is not outsourcing; it’s hiring the necessary expertise into your evaluation process. For example, Ankith hired 4hr-year medical students to evaluate their RAG systems for medical content rather than outsourcing to generic annotators.\n\n18",
    "19": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nQ: What parts of evals can LLMs help with?\n\nLLMs can speed up parts of your eval workflow, but they can’t replace human judgment (i.e., reviewing and annotating traces). If you let an LLM handle all of error analysis your product. Users keep mentioning “app crashes after login” and “login takes too long” under a single “login issues” category, even though one is a stability problem and the other is a performance problem. Without your intervention, you’d miss that these issues require different fixes.\n\nThat said, LLMs are valuable tools for accelerating certain parts of the evaluation workflow when used with oversight.\n\nHere are some areas where LLMs can help:\n\n* First-pass axial coding: After you’ve opened 30–50 traces yourself, use an LLM to organize your raw failure notes into proposed groupings. This helps you quickly spot patterns, but always review and refine the clusters yourself. Note: If you aren’t familiar with axial and open coding, see this faq.\n* Mapping annotations to failure categories: Once you’ve defined failure categories, you can ask an LLM to suggest which categories apply to each new trace (e.g., “Given this annotation: [open_annotation] which apply?”).\n* Suggesting prompt improvements: When you notice recurring problems, have the LLM propose concrete changes to your prompts. Review these suggestions before adopting any changes.\n* Analyzing annotation data: Use LLMs or AI-powered notebooks to find patterns in your labels, such as “reports of lag increase 3x during peak usage hours” or “slow response times are mostly reported from users on mobile devices.”\n\nHowever, you shouldn’t outsource these activities to an LLM:\n\n* Initial open coding: Always tread through the raw traces yourself at the start. This is how you discover new types of failures, understand user pain points, and build intuition about your data. Never skip this or delegate it.\n\n19",
    "20": "Tools & Infrastructure\nQ: Should I build a custom annotation tool or use something off-the-shelf?\nBuild a custom annotation tool. This is the single most impactful investment you can make for your AI evaluation workflow. With AI-assisted development tools like Cursor or Lovable, you can build a tailored interface in ~10x faster.\nCustom tools excel because:\n\n• They show all your context in multiple systems in one place\n• They can render your data in a product specific way (images, widgets, markdown, buttons, etc.)\n• They’re designed for your specific workflow (custom filters, sorting, progress bars, etc.)\n• They’re designed to coordinate teams.\nOff-the-shelf tools may need to justify when you need to coordinate dozens of distributed annotators with enterprise access controls. Even then, many teams find the configuration overhead and limitations aren’t worth it.\nIsaac’s Anki flashcard annotation app shows the power of custom tools—handling 400+ results per query with keyboard navigation and domain-specific evaluation criteria that would be nearly impossible to configure in a generic tool.\nFocus view",
    "21": "Q: What makes a good custom interface for reviewing LLM outputs?\nGreat interfaces make human review fast, clear, and motivating. We recommend building your own annotation tool customized to your domain. The following features are possible enhancements we’ve seen work well, but you don’t need all of them. The screenshots shown are illustrative examples to clarify concepts. In practice, I rarely implement all these features in a single app. It’s ultimately a judgment call based on your specific needs and constraints.\n1. Render Traces Intelligently, Not Generically:\nPresent the trace in a way that’s intuitive for the domain. If you’re evaluating generated emails, render them to look like emails. If the output is code, use syntax highlighting. Allow the reviewer to see the full trace (user input, tool calls, and LLM reasoning), but keep less important details in collapsed sections that can be expanded. Here is an example of a custom annotation tool for reviewing real estate assistant emails:",
    "22": "Figure 2: A custom interface for reviewing emails for a real estate assistant.\n2. Show Progress and Support Keyboard Navigation:\nKeep reviewers in a state of flow by minimizing friction and motivating completion. Progress indicators (e.g., “Trace 45 of 100”) help the reviewer understand where they are. Keyboard shortcuts (e.g., “Press ‘?’ for a complete list”) are essential and encourage usage. The image below is an example of a progress bar and keyboard shortcuts.",
    "23": "Figure 3: An annotation interface with a progress bar and hotkey guide.\n3. Trace navigation through clustering, filtering, and search:\nAllow reviewers to filter traces by metadata or search by keywords. Semantic search helps find conceptually similar problems. Clustering similar traces (like grouping by user persona) lets reviewers spot recurring issues and explore hypotheses. Below is an illustration of these features:\n\nFigure 4: Cluster view showing groups of emails, such as property-focused or client-focused\n4. Prioritize labeling traces you think might be problematic:\nSurface traces flagged by guardrails, CI failures, or automated evaluators for review. Provide buttons to take actions like adding to datasets, filing bugs, or re-running pipeline tests. Dis-play relevant context (pipeline version, eval scores, reviewer info) directly in the interface. Below is an illustration of these ideas:",
    "25": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nbuilding capabilities that use AI to suggest groupings, rewrite your observations into clearer failure taxonomies, help find similar cases through semantic search, etc.\n\n2. AI-Powered Assistance Throughout the Workflow\n\nThe most effective workflows use AI to accelerate every stage of evaluation. During error analysis, you want an LLM helping to conduct your open-ended observations into coherent failure modes. For example, you might annotate several traces with notes like “wrong tone for investor,” “too casual for luxury buyer,” etc. Your tooling should recognize these as the same underlying pattern and suggest a unified “persona-tone mismatch” category.\nYou’ll also want AI assistance in proposing fixes. After identifying 20 cases where your as-sistant omits pet policies from property summaries, can your workflow analyze these failures and suggest specific prompt modifications? Can it draft refinements to your SQL generation instructions when it notices patterns of missing WHERE clauses?\n\nAdditionally, good workflows help you conduct data analysis of your annotations and traces. I like using notebooks with AI in-the-loop like Julius, Hex or Solveff. These help me discover insights like “location ambiguity errors spike 3x when users mention neighborhood names” or “tone mismatches occur 80% more often in email generation than other modalities.”\n\n3. Custom Evaluators Over Generic Metrics\n\nBe prepared to build most of your evaluators from scratch. Generic metrics like “hallucination score” or “helpfulness rating” rarely capture what actually matters for your application—like proposing unavailable showing times or omitting budget constraints from emails. In our experience, successful teams spend most of their effort on application-specific metrics.\n\n4. APIs That Support Custom Annotation Apps\n\nCustom annotation interfaces work best for most teams. This requires observability platforms with thoughtful APIs. I often have to build my own libraries and abstractions just to make bulk data export manageable. You shouldn’t have to paginate through thousands of requests or handle timeout-prone endpoints just to get your data. Look for platforms that provide true focus view capabilities and, crucially, APIs that let you write annotations back efficiently.\n\nFocus view",
    "26": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nQ: Seriously Hamel. Stop the bullshit. What’s your favorite eval vendor?\n\nEval tools are in an intensely competitive space. It would be futile to compare their features. If I tried to do such an analysis, it would be invalidated in a week! Vendors I encounter the most organically in my work are: Langsmith, Arize and BrainTrust.\n\nWhen I help clients with vendor selection, the decision weighs heavily towards who can offer the best support, as opposed to purely features. This changes depending on size of client, use case, etc. Yes - it’s mainly the human factor that matters, and dare I say, vibes.\n\nI have no favorite vendor. At the core, their features are very similar - and I often build custom tools on top to fit my needs.\n\nMy suggestion is to explore the vendors and see which one you like the most.\n\nFocus view",
    "27": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nQ: How are evaluations used differently in CI/CD vs. monitoring production?\n\nThe most important difference between CI vs. production evaluation is the data used for testing.\n\nTest datasets for CI are small (in many cases 100+ examples) and purpose-built. Examples cover core features, regression tests for past bugs, and known edge cases. Since CI tests are run frequently, the cost of each test has to be carefully considered (that’s why you carefully curate the dataset). Favor assertions or other deterministic checks over LLM-as-judge evaluators.\n\nFor evaluating production traffic, you can sample live traces and run evaluators against them asynchronously. Since you usually lack reference outputs on production data, you might rely more on more expensive reference-free evaluators like LLM-as-judge. Additionally, track confidence intervals for production metrics. If the lower bound crosses your threshold, investigate further.\n\nThese two systems are complementary: when production monitoring reveals new failure patterns through error analysis and evals, add representative examples to your CI dataset. This mitigates regressions on new issues.\n\nFocus view",
    "28": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nQ: What’s the difference between guardrails & evaluators?\n\nGuardrails are inline safety checks that sit directly in the request/response path: they validate inputs or outputs before anything reaches a user, so they typically are:\n\n• Fast and deterministic – typically a few milliseconds of latency budget.\n• Simple and explainable – regexes, keyword block-lists, schema or type validators.\n• Targeted at clear-cut, high-impact failures – PII leaks, profanity, disallowed instructions, SQL injection, malformed JSON, invalid code syntax, etc.\n\nIf a guardrail triggers, the system can redact, refuse, or regenerate the response. Because these checks are user-visible when they fire, false positives are treated as production bugs; teams version guardrail rules, log every trigger, and monitor rates to keep them conservative.\n\nEvaluators typically run after a response has been generated. Evaluators measure qualities that cannot, and model-improvement loops, but they do not block the original answer.\n\nEvaluators are usually run asynchronously or in batch to afford heavier computation such as a LLM-as-a-Judge. Inline use of an LLM-as-Judge is possible only when the latency budget and reliability targets allow it. Slow LLM judges might be feasible in a cascade that runs on the minority of borderline cases.\n\nApply guardrails for immediate protection against objective failures requiring intervention. Use evaluators for monitoring and improving subjective or nuanced criteria. Together, they create layered protection.\n\nWord of caution: Do not use llm guardrails off the shelf blindly. Always look at the prompt.\n\nQ: Can my evaluators also be used to automatically fix or correct outputs in production?\n\nYes, but only a specific subset of them. This is the distinction between an evaluator and a guardrail that we previously discussed. As a reminder:\n\n• Evaluators typically run asynchronously after a response has been generated. They measure quality but do not interfere with the user’s immediate experience.\n• Guardrails run synchronously in the critical path of the request, before the output is shown to the user. Their job is to prevent high-impact failures in real-time.",
    "29": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nThere are two important decision criteria for deciding whether to use an evaluator as a guardrail:\n\n1. Latency & Cost: Can the evaluator run fast enough and without degrading user experience?\n2. Error Rate Trade-offs: What’s the cost-benefit balance between false positives (blocking good outputs and frustrating users) versus false negatives (letting bad outputs reach users and causing harm)? In high-stakes domains like medical advice, false negatives may be more costly than false positives. In creative applications, false positives may block legitimate creativity may be more harmful than occasional quality issues.\n\nMost guardrails are designed to be fast (to avoid harming user experience) and have a very low false positive rate (to avoid blocking valid responses). For this reason, you would almost never use a slow or non-deterministic LLM-as-Judge as a synchronous guardrail. However, these tradeoffs might be different for your use case.\n\nFocus view\n\nQ: How much time should I spend on model selection?\n\nMany developers fixate on model selection as the primary way to improve their LLM applications. As Hamel noted in office hours, “I suggest not thinking of switching model as the main axes of how to improve your application.” Does error analysis suggest that your model is the problem?\n\nQ: Is RAG dead?\n\nQuestion: Should I avoid using RAG for my AI application after reading that “RAG is dead” for coding agents?\n\nMany developers are confused about when and how to use RAG after reading articles claiming “RAG is dead.” Understanding what RAG actually means versus the narrow marketing definitions will help you make better architectural decisions for your AI applications.\n\nThe viral article claiming RAG is dead specifically argues against using naive vector database retrieval for autonomous coding agents, not RAG as a whole. This is a crucial distinction that many developers miss due to misleading marketing.\n\nRAG simply means Retrieval-Augmented Generation - using retrieval to provide relevant context that improves your model’s output. The core principle remains: your LLM needs the right context to generate accurate answers. The question isn’t whether to use retrieval, but how to retrieve effectively.\n\nFor coding applications, naive vector similarity search often fails because code relationships are complex and contextual. Instead of abandoning retrieval entirely, modern coding assistants like Claude Code still uses retrieval —they just employ agentic search instead of relying solely on vector databases, similar to how human developers work.\n\nYou have multiple retrieval strategies available, ranging from simple keyword matching to embedding similarity to LLM-powered relevance filtering. The optimal approach depends on your specific use case, data characteristics, and performance requirements. Many production systems combine multiple strategies or use multi-hop retrieval guided by LLM agents.\n\nUnfortunately, “RAG” has become a buzzword with no shared definition. Some people use it to mean any retrieval system, others restrict it to vector databases. Focus on the ultimate goal: getting your LLM the context it needs to succeed. Whether that’s through vector search, agentic exploration, or hybrid approaches is a product and engineering decision.\n\nFocus view\n\nQ: How should I approach evaluating my RAG system?\n\nRAG systems have two distinct components that require different evaluation approaches: retrieval and generation.\n\nThe retrieval component is a search problem. Evaluate it using traditional information retrieval (IR) metrics. Common examples include Recall@k (of all relevant documents, how many did you retrieve in the top k?), Precision@k (of the k documents retrieved, how many were relevant?), or MRR (how high up was the first relevant document?). The specific metrics you choose depend on your use case. These metrics are pure search metrics that require whether you are finding the right documents. (more on this below).\n\nTo evaluate retrieval, create a dataset of queries paired with their relevant documents. Generate this synthetically by taking documents from your corpus, extracting key facts, then creating queries that those facts would answer. This reverse process gives you query-document pairs for measuring retrieval performance which best annotator.",
    "30": "AI Evals Course: 35% off at bit.ly/evals-ai\n\nFor the generation component—how well the LLM uses retrieved context, whether it halluci-\nnates, whether it answers the question—the same evaluation procedures covered through-\nout this course: error analysis to identify failure modes, collecting human labels, building\nLLM-as-judge evaluators, and validating those judges against human annotations.\nJason Liu’s “There Are Only 6 RAG Evals” provides a framework that maps well to this\nseparation. His Tier 1 covers traditional IR metrics for retrieval. Tiers 2 and 3 evaluate\nrelationships between Question, Context, and Answer—like whether the context is relevant\n(C(Q), whether the answer is faithful to context (A(C), and whether the answer addresses the\nquestion (A(Q).\n\nIn addition to Jason’s six evals, error analysis on your specific data may reveal domain-specific\nfailure modes that warrant their own metrics. For example, a medical RAG system might\nconsistently fail to distinguish between drug dosages for adults versus children, or a legal\nRAG might confuse jurisdictional boundaries. These patterns emerge only through systematic\nreview of actual failures. Once identified, you can create targeted evaluators for these specific\nissues beyond the general framework.\n\nFinally, when implementing Jason’s Tier 2 and 3 metrics, don’t just use prompts off the shelf.\nThe standard LLM-as-judge process requires several steps: error analysis, prompt iteration,\ncreating labeled examples, and measuring your judge’s accuracy against human labels. Once\nyou know your judge’s True Positive and True Negative rates, you can correct its estimates\nto determine the actual failure rate in your system. Skip this validation and your judges may\nnot reflect your actual quality criteria.\n\nIn summary, debug retrieval first using IR metrics, then tackle generation quality without being\noverwhelmed. Even if a document fits within the context window, it might be better to break\nit up. Long inputs can degrade performance due to attention bottlenecks, especially in the\nmiddle of the context. Two tasks require different strategies:\n\nFocus view",
    "31": "Q: How do I choose the right chunk size for my document processing tasks?\n\nUse the largest chunk (with caveats) that likely contains the answer. This reduces the number\nof queries and avoids context fragmentation. However, avoid adding irrelevant text. Models are\nsensitive to distraction, especially with large inputs. The middle parts of a long input might\nbe under-attended. Furthermore, if cost and latency are a bottleneck, you should consider\npreprocessing or filtering the document (via keyword search or a lightweight retriever) to\nisolate relevant sections before feeding a huge chunk.\n\n1. Fixed-Output Tasks → Large Chunks\n\nThese are tasks where the output length doesn’t strongly correlate with input: extracting a number,\nswering a specific question, classifying a section. For example:\n\n• “What was the CEO’s salary in 2023?”\n\n2. Expansive-Output Tasks → Smaller Chunks\n\nThese include summarization, exhaustive extraction, or any task where output grows with\ninput. For example:\n\n• “Summarize each section”\n• “List all customer complaints”\n\nIn these cases, smaller chunks help preserve reasoning quality and output completeness. The\nstandard approach is to process each chunk independently, then aggregate results (e.g., map-\nreduce). When sizing your chunks, try to respect content boundaries like paragraphs, sections,\nor chapters. Chunking also helps mitigate output limits. By breaking the task into pieces,\neach piece’s output can stay within limits.\n\nGeneral Guidance\n\nIt’s important to recognize why chunk size affects results. A larger chunk means the model\nhas to reason over more information in one go - essentially, a heavier cognitive load. LLMs\nhave limited capacity to retain and correlate details across a long text. If too much\ninformation in the model might prioritize certain parts (commonly the beginning or end) and\noverlook or “forget” details in the middle. This can lead to overly coarse summaries or missed\nfacts. In contrast, a smaller chunk bounds the problem: the model can pay full attention to\nthat section. You are trading off global context for local focus.\n\nNo rule of thumb can perfectly determine the best chunk size for your use case – you should\nvalidate with experiments. The optimal chunk size can vary by domain and model. I treat\nchunk size as a hyperparameter to tune.\n\nFocus view",
    "32": "Q: How do I debug multi-turn conversation traces?\n\nStart simple, agent-flows, assign a session or trace ID to each user request and log every message\nwith its source (which agent or tool), trace ID, and position in the sequence. This lets you\nreconstruct the full path from initial query to final result across all agents.\n\nMulti-agent trace logging\n\nFor multi-agent cases cascade from the first upstream failure. Do not resolve downstream failures\nsince these often cascade from the first upstream failure. You can annotate independent\ndownstream failures automatically. As you gain experience, you can annotate independent\nfailure modes within the same trace to speed up overall error analysis.\n\nSimplify when possible\n\nWhen you find a failure, reproduce it with the simplest possible test case. Here’s an example:\nsuppose a shopping bot gives the wrong return policy on turn 4 of a conversation. Before\ndiving into the full multi-turn complexity, simplify it to a single turn: “What is the return\nwindow for product X1000?” If it still fails, you’ve proven the error isn’t about conversation\ncontext - it’s likely a retrieval or knowledge issue you can debug more easily.\n\nTest case generation\n\nYou have two main approaches. First, simulate users with another LLM to create realistic\nmulti-turn conversations. Second, use “N-1 testing” where you provide the first N-1 turns of\na real conversation and test what happens next. The N-1 approach often works better since it\nuses actual conversation prefixes rather than fully synthetic interactions, but is less flexible.\nThe key is balancing thoroughness with efficiency. Not every multi-turn failure requires multi-\nturn analysis.",
    "33": "Q: How do I evaluate sessions with human handoffs?\n\nCapture the complete user journey in your traces, including human handoffs. The trace\ncontinues until the user’s goal is resolved or the session ends, not when AI hands off to a\nhuman. Log the handoff decision, why it occurred, context transferred, wait time, human\nactions, final resolution, and whether the human had sufficient context. Many failures occur\nat handoff boundaries where AI hands off too early, too late, or without proper context.\n\nEvaluate handoffs as potential failure modes during error analysis. Ask: Was the handoff\nnecessary? Did the AI provide adequate context? Track both handoff quality and handoff\nrate. Sometimes the best improvement reduces handoffs entirely rather than improving handoff\ntime.\n\nLog the entire workflow from initial trigger to final business outcome. Include LLM calls, tool\nusage, human approvals, and database writes in your traces. You will need this visibility to\nproperly diagnose failures.\n\nUse both outcome and process metrics. Outcome metrics verify the final result meets require-\nments: Was the business case complete? Accurate? Properly formatted? Process metrics\nevaluate efficiency: step count, time taken, resource usage. Process failures are often easier to\nsegment your error analysis by workflow stages. Early stage failures (understanding user input)\ndiffer from middle stage failures (data processing), and late stage failures (formatting output).\n\nEarly stage improvements have more impact since errors cascade in LLM chains.\n\nUse transition failure matrices to analyze where workflows break. Create a matrix showing\nthe last successful state versus where the first failure occurred. This reveals failure hotspots\nand guides where to invest debugging effort.\n\nFocus view",
    "34": "Q: How do I evaluate agentic workflows?\n\nWe recommend evaluating agentic workflows in two phases:\n\n1. End-to-end task reviews. Treat the agent as a black box and ask “did we meet the\nuser’s goal?”. Define a precise success rule per task (exact answer, corrected side-effect, etc.)\nand measure with human or aligned LLM judges. Take note of the first upstream failure, when\nconducting error analysis.\n\nOnce error analysis reveals which workflows fail most often, move to step-level diagnostics to\nunderstand why they’re failing.\n\n2. Step-level diagnostics. Assuming that you have sufficiently instrumented your system\nwith details of tool calls and responses, you can score individual components such as: - Tool\nchoice: was the selected tool appropriate? - Parameter extraction: were inputs complete and\nwell-formed? - Error handling: did the agent recover from empty results or API failures? -\nContext retention: did the agent recover for long workflows and many steps, seconds, and\ntokens were spent? - Goal checkpoints: for long workflows verify key milestones.\n\nExample: “Find Berkeley homes under $1M and schedule viewings” breaks into: parameters\nextracted correctly, relevant listings retrieved, availability checked, and calendar invites sent.\n\nEach checkpoint can pass or fail independently, making debugging tractable.\n\nUse transition failure matrices to understand error patterns. Create a matrix where\nrows represent the last successful state and columns represent where the first failure occurred.\n\nFigure 6: Transition failure matrix showing hotspots in text-to-SQL agent workflow\n\nTransition matrices transform overwhelming agent complexity into actionable insights. Instead of\ndrowning in individual trace reviews, you can immediately see that GenSQL → ExecSQL\ntransitions cause 12 failures while DecideTool → PlanCal causes only 2. This data-driven\napproach guides where to invest debugging effort. Here is another example from Bryan Bischof,\n\nFocus view"
  },
  "skipped_pages": [
    0,
    1,
    2,
    3,
    4,
    10,
    11,
    12,
    13,
    14,
    35
  ]
}