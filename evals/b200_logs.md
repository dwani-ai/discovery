(VllmWorker rank=6 pid=274251) INFO 08-15 15:39:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_306e8ac3'), local_subscribe_addr='ipc:///tmp/d7ac6afa-bd0e-462a-adbf-6fe584b020a1', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=2 pid=274247) INFO 08-15 15:39:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9a878cbe'), local_subscribe_addr='ipc:///tmp/6262b6c3-7d5f-4667-a73d-c8cca07248a4', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=3 pid=274248) INFO 08-15 15:39:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2a158b32'), local_subscribe_addr='ipc:///tmp/6bd18bae-7a73-4f9c-8d12-8246c658564b', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=7 pid=274252) INFO 08-15 15:39:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_158aa017'), local_subscribe_addr='ipc:///tmp/2a28c846-5309-44bc-81e7-1f6e189fe971', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=1 pid=274246) INFO 08-15 15:39:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dc77b0f4'), local_subscribe_addr='ipc:///tmp/1fee5e22-bff6-43f3-b4ae-557b8ae1ca6f', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=0 pid=274245) INFO 08-15 15:39:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a30b6408'), local_subscribe_addr='ipc:///tmp/f473b646-34f9-4195-96a3-675dd1db1426', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=4 pid=274249) INFO 08-15 15:39:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f52eb131'), local_subscribe_addr='ipc:///tmp/e748a4b1-f425-474f-b199-9f523b37b68a', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=5 pid=274250) INFO 08-15 15:39:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e9b4d250'), local_subscribe_addr='ipc:///tmp/e05871d7-88b0-43a4-8168-440891e29d72', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=0 pid=274245) INFO 08-15 15:39:39 [__init__.py:1375] Found nccl from library libnccl.so.2
(VllmWorker rank=6 pid=274251) INFO 08-15 15:39:39 [__init__.py:1375] Found nccl from library libnccl.so.2
(VllmWorker rank=0 pid=274245) INFO 08-15 15:39:39 [pynccl.py:70] vLLM is using nccl==2.26.2
(VllmWorker rank=6 pid=274251) INFO 08-15 15:39:39 [pynccl.py:70] vLLM is using nccl==2.26.2
(VllmWorker rank=5 pid=274250) INFO 08-15 15:39:39 [__init__.py:1375] Found nccl from library libnccl.so.2
(VllmWorker rank=1 pid=274246) INFO 08-15 15:39:39 [__init__.py:1375] Found nccl from library libnccl.so.2
(VllmWorker rank=5 pid=274250) INFO 08-15 15:39:39 [pynccl.py:70] vLLM is using nccl==2.26.2
(VllmWorker rank=1 pid=274246) INFO 08-15 15:39:39 [pynccl.py:70] vLLM is using nccl==2.26.2
(VllmWorker rank=7 pid=274252) INFO 08-15 15:39:39 [__init__.py:1375] Found nccl from library libnccl.so.2
(VllmWorker rank=7 pid=274252) INFO 08-15 15:39:39 [pynccl.py:70] vLLM is using nccl==2.26.2
(VllmWorker rank=2 pid=274247) INFO 08-15 15:39:39 [__init__.py:1375] Found nccl from library libnccl.so.2
(VllmWorker rank=2 pid=274247) INFO 08-15 15:39:39 [pynccl.py:70] vLLM is using nccl==2.26.2
(VllmWorker rank=4 pid=274249) INFO 08-15 15:39:39 [__init__.py:1375] Found nccl from library libnccl.so.2
(VllmWorker rank=3 pid=274248) INFO 08-15 15:39:39 [__init__.py:1375] Found nccl from library libnccl.so.2
(VllmWorker rank=4 pid=274249) INFO 08-15 15:39:39 [pynccl.py:70] vLLM is using nccl==2.26.2
(VllmWorker rank=3 pid=274248) INFO 08-15 15:39:39 [pynccl.py:70] vLLM is using nccl==2.26.2





(VllmWorker rank=0 pid=274245) INFO 08-15 15:40:15 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:14 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:14 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:14 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:14 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:14 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:14 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:14 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:14 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_1335a4e8'), local_subscribe_addr='ipc:///tmp/9c0e92d6-704f-46f3-8075-edeb248d8b59', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:15 [parallel_state.py:1102] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:15 [parallel_state.py:1102] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:15 [parallel_state.py:1102] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:15 [parallel_state.py:1102] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:15 [parallel_state.py:1102] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:15 [parallel_state.py:1102] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:15 [parallel_state.py:1102] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:15 [parallel_state.py:1102] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
(VllmWorker rank=6 pid=274251) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(VllmWorker rank=5 pid=274250) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(VllmWorker rank=3 pid=274248) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(VllmWorker rank=4 pid=274249) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(VllmWorker rank=0 pid=274245) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(VllmWorker rank=1 pid=274246) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(VllmWorker rank=7 pid=274252) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(VllmWorker rank=2 pid=274247) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
(VllmWorker rank=6 pid=274251) WARNING 08-15 15:41:21 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:21 [gpu_model_runner.py:1843] Starting to load model RedHatAI/gemma-3-27b-it-FP8-dynamic...
(VllmWorker rank=5 pid=274250) WARNING 08-15 15:41:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(VllmWorker rank=7 pid=274252) WARNING 08-15 15:41:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(VllmWorker rank=1 pid=274246) WARNING 08-15 15:41:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:22 [gpu_model_runner.py:1875] Loading model from scratch...
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:22 [gpu_model_runner.py:1843] Starting to load model RedHatAI/gemma-3-27b-it-FP8-dynamic...
(VllmWorker rank=6 pid=274251) WARNING 08-15 15:41:22 [cuda.py:280] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:22 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:22 [gpu_model_runner.py:1843] Starting to load model RedHatAI/gemma-3-27b-it-FP8-dynamic...
(VllmWorker rank=6 pid=274251) WARNING 08-15 15:41:22 [layer.py:51] Xformers is not available, falling back.
(VllmWorker rank=0 pid=274245) WARNING 08-15 15:41:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(VllmWorker rank=6 pid=274251) /home/ubuntu/venv/lib/python3.12/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.
(VllmWorker rank=6 pid=274251)   warnings.warn(
(VllmWorker rank=3 pid=274248) WARNING 08-15 15:41:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:22 [cuda.py:290] Using Flash Attention backend on V1 engine.
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:22 [gpu_model_runner.py:1843] Starting to load model RedHatAI/gemma-3-27b-it-FP8-dynamic...
(VllmWorker rank=2 pid=274247) WARNING 08-15 15:41:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:22 [gpu_model_runner.py:1843] Starting to load model RedHatAI/gemma-3-27b-it-FP8-dynamic...
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:22 [gpu_model_runner.py:1843] Starting to load model RedHatAI/gemma-3-27b-it-FP8-dynamic...
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:22 [gpu_model_runner.py:1843] Starting to load model RedHatAI/gemma-3-27b-it-FP8-dynamic...
(VllmWorker rank=4 pid=274249) WARNING 08-15 15:41:22 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:22 [gpu_model_runner.py:1875] Loading model from scratch...
(VllmWorker rank=5 pid=274250) WARNING 08-15 15:41:22 [cuda.py:280] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:22 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:22 [gpu_model_runner.py:1875] Loading model from scratch...
(VllmWorker rank=5 pid=274250) WARNING 08-15 15:41:22 [layer.py:51] Xformers is not available, falling back.
(VllmWorker rank=7 pid=274252) WARNING 08-15 15:41:22 [cuda.py:280] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:22 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.
(VllmWorker rank=7 pid=274252) WARNING 08-15 15:41:22 [layer.py:51] Xformers is not available, falling back.
(VllmWorker rank=5 pid=274250) /home/ubuntu/venv/lib/python3.12/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.
(VllmWorker rank=5 pid=274250)   warnings.warn(
(VllmWorker rank=7 pid=274252) /home/ubuntu/venv/lib/python3.12/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.
(VllmWorker rank=7 pid=274252)   warnings.warn(
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:22 [cuda.py:290] Using Flash Attention backend on V1 engine.
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:22 [cuda.py:290] Using Flash Attention backend on V1 engine.
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:22 [gpu_model_runner.py:1875] Loading model from scratch...
(VllmWorker rank=1 pid=274246) WARNING 08-15 15:41:22 [cuda.py:280] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:22 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.
(VllmWorker rank=1 pid=274246) WARNING 08-15 15:41:22 [layer.py:51] Xformers is not available, falling back.
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:22 [gpu_model_runner.py:1843] Starting to load model RedHatAI/gemma-3-27b-it-FP8-dynamic...
(VllmWorker rank=1 pid=274246) /home/ubuntu/venv/lib/python3.12/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.
(VllmWorker rank=1 pid=274246)   warnings.warn(
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:22 [cuda.py:290] Using Flash Attention backend on V1 engine.
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:22 [gpu_model_runner.py:1875] Loading model from scratch...
(VllmWorker rank=3 pid=274248) WARNING 08-15 15:41:22 [cuda.py:280] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:22 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:22 [gpu_model_runner.py:1875] Loading model from scratch...
(VllmWorker rank=3 pid=274248) WARNING 08-15 15:41:22 [layer.py:51] Xformers is not available, falling back.
(VllmWorker rank=3 pid=274248) /home/ubuntu/venv/lib/python3.12/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.
(VllmWorker rank=3 pid=274248)   warnings.warn(
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=0 pid=274245) WARNING 08-15 15:41:22 [cuda.py:280] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:22 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.
(VllmWorker rank=0 pid=274245) WARNING 08-15 15:41:22 [layer.py:51] Xformers is not available, falling back.
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:22 [cuda.py:290] Using Flash Attention backend on V1 engine.
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:22 [weight_utils.py:296] Using model weights format ['*.safetensors']
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:23 [weight_utils.py:296] Using model weights format ['*.safetensors']
(VllmWorker rank=0 pid=274245) /home/ubuntu/venv/lib/python3.12/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.
(VllmWorker rank=0 pid=274245)   warnings.warn(
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:23 [gpu_model_runner.py:1875] Loading model from scratch...
(VllmWorker rank=2 pid=274247) WARNING 08-15 15:41:23 [cuda.py:280] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:23 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:23 [weight_utils.py:296] Using model weights format ['*.safetensors']
(VllmWorker rank=2 pid=274247) WARNING 08-15 15:41:23 [layer.py:51] Xformers is not available, falling back.
(VllmWorker rank=2 pid=274247) /home/ubuntu/venv/lib/python3.12/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.
(VllmWorker rank=2 pid=274247)   warnings.warn(
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:23 [cuda.py:290] Using Flash Attention backend on V1 engine.
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:23 [cuda.py:290] Using Flash Attention backend on V1 engine.
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:23 [weight_utils.py:296] Using model weights format ['*.safetensors']
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:23 [weight_utils.py:296] Using model weights format ['*.safetensors']
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:23 [gpu_model_runner.py:1875] Loading model from scratch...
(VllmWorker rank=4 pid=274249) WARNING 08-15 15:41:23 [cuda.py:280] FlashInfer failed to import for V1 engine on Blackwell (SM 10.0) GPUs; it is recommended to install FlashInfer for better performance.
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:23 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.
(VllmWorker rank=4 pid=274249) WARNING 08-15 15:41:23 [layer.py:51] Xformers is not available, falling back.
(VllmWorker rank=4 pid=274249) /home/ubuntu/venv/lib/python3.12/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.
(VllmWorker rank=4 pid=274249)   warnings.warn(
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:23 [weight_utils.py:296] Using model weights format ['*.safetensors']
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:23 [weight_utils.py:296] Using model weights format ['*.safetensors']
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:23 [cuda.py:290] Using Flash Attention backend on V1 engine.
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:23 [weight_utils.py:296] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:01,  4.78it/s]
Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:00<00:00,  4.43it/s]
Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:00<00:00,  4.31it/s]
Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:00<00:00,  4.22it/s]
Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:01<00:00,  4.39it/s]
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:24 [default_loader.py:262] Loading weights took 1.49 seconds
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:24 [default_loader.py:262] Loading weights took 1.53 seconds
Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:01<00:00,  4.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:01<00:00,  4.32it/s]
(VllmWorker rank=0 pid=274245) 
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:24 [default_loader.py:262] Loading weights took 1.42 seconds
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:25 [gpu_model_runner.py:1892] Model loading took 3.7361 GiB and 2.088911 seconds
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:25 [default_loader.py:262] Loading weights took 1.49 seconds
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:25 [default_loader.py:262] Loading weights took 1.45 seconds
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:25 [gpu_model_runner.py:1892] Model loading took 3.7361 GiB and 2.019897 seconds
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:25 [gpu_model_runner.py:1892] Model loading took 3.7361 GiB and 2.055278 seconds
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:25 [default_loader.py:262] Loading weights took 1.45 seconds
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:25 [gpu_model_runner.py:1892] Model loading took 3.7361 GiB and 2.153587 seconds
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:25 [default_loader.py:262] Loading weights took 1.48 seconds
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:25 [gpu_model_runner.py:1892] Model loading took 3.7361 GiB and 2.972694 seconds
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:25 [gpu_model_runner.py:1892] Model loading took 3.7361 GiB and 2.799482 seconds
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:25 [default_loader.py:262] Loading weights took 1.49 seconds
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:25 [gpu_model_runner.py:1892] Model loading took 3.7361 GiB and 2.787082 seconds
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:26 [gpu_model_runner.py:1892] Model loading took 3.7361 GiB and 2.650146 seconds
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:49 [backends.py:530] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/cb34fbec91/rank_6_0/backbone for vLLM's torch.compile
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:49 [backends.py:541] Dynamo bytecode transform time: 21.34 s
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:49 [backends.py:530] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/cb34fbec91/rank_3_0/backbone for vLLM's torch.compile
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:49 [backends.py:541] Dynamo bytecode transform time: 21.44 s
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:49 [backends.py:530] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/cb34fbec91/rank_2_0/backbone for vLLM's torch.compile
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:49 [backends.py:541] Dynamo bytecode transform time: 21.51 s
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:49 [backends.py:530] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/cb34fbec91/rank_1_0/backbone for vLLM's torch.compile
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:49 [backends.py:541] Dynamo bytecode transform time: 21.56 s
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:49 [backends.py:530] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/cb34fbec91/rank_0_0/backbone for vLLM's torch.compile
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:49 [backends.py:541] Dynamo bytecode transform time: 21.58 s
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:49 [backends.py:530] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/cb34fbec91/rank_5_0/backbone for vLLM's torch.compile
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:49 [backends.py:541] Dynamo bytecode transform time: 21.68 s
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:50 [backends.py:530] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/cb34fbec91/rank_7_0/backbone for vLLM's torch.compile
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:50 [backends.py:541] Dynamo bytecode transform time: 21.82 s
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:50 [backends.py:530] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/cb34fbec91/rank_4_0/backbone for vLLM's torch.compile
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:50 [backends.py:541] Dynamo bytecode transform time: 22.11 s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=6 pid=274251) INFO 08-15 15:41:53 [backends.py:194] Cache the graph for dynamic shape for later use
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=3 pid=274248) INFO 08-15 15:41:54 [backends.py:194] Cache the graph for dynamic shape for later use
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=2 pid=274247) INFO 08-15 15:41:54 [backends.py:194] Cache the graph for dynamic shape for later use
(VllmWorker rank=1 pid=274246) INFO 08-15 15:41:54 [backends.py:194] Cache the graph for dynamic shape for later use
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=0 pid=274245) INFO 08-15 15:41:54 [backends.py:194] Cache the graph for dynamic shape for later use
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=5 pid=274250) INFO 08-15 15:41:54 [backends.py:194] Cache the graph for dynamic shape for later use
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=7 pid=274252) INFO 08-15 15:41:54 [backends.py:194] Cache the graph for dynamic shape for later use
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(VllmWorker rank=4 pid=274249) INFO 08-15 15:41:54 [backends.py:194] Cache the graph for dynamic shape for later use
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)




(VllmWorker rank=0 pid=274245) INFO 08-15 15:43:07 [backends.py:215] Compiling a graph for dynamic shape takes 76.34 s
(VllmWorker rank=3 pid=274248) INFO 08-15 15:43:07 [backends.py:215] Compiling a graph for dynamic shape takes 76.88 s
(VllmWorker rank=6 pid=274251) INFO 08-15 15:43:07 [backends.py:215] Compiling a graph for dynamic shape takes 77.04 s
(VllmWorker rank=2 pid=274247) INFO 08-15 15:43:08 [backends.py:215] Compiling a graph for dynamic shape takes 77.72 s
(VllmWorker rank=7 pid=274252) INFO 08-15 15:43:08 [backends.py:215] Compiling a graph for dynamic shape takes 77.43 s
(VllmWorker rank=5 pid=274250) INFO 08-15 15:43:08 [backends.py:215] Compiling a graph for dynamic shape takes 77.59 s
(VllmWorker rank=1 pid=274246) INFO 08-15 15:43:08 [backends.py:215] Compiling a graph for dynamic shape takes 77.74 s
(VllmWorker rank=4 pid=274249) INFO 08-15 15:43:09 [backends.py:215] Compiling a graph for dynamic shape takes 77.76 s
(VllmWorker rank=2 pid=274247) INFO 08-15 15:43:19 [monitor.py:34] torch.compile takes 99.23 s in total
(VllmWorker rank=4 pid=274249) INFO 08-15 15:43:19 [monitor.py:34] torch.compile takes 99.88 s in total
(VllmWorker rank=1 pid=274246) INFO 08-15 15:43:19 [monitor.py:34] torch.compile takes 99.31 s in total
(VllmWorker rank=0 pid=274245) INFO 08-15 15:43:19 [monitor.py:34] torch.compile takes 97.92 s in total
(VllmWorker rank=5 pid=274250) INFO 08-15 15:43:19 [monitor.py:34] torch.compile takes 99.27 s in total
(VllmWorker rank=6 pid=274251) INFO 08-15 15:43:19 [monitor.py:34] torch.compile takes 98.38 s in total
(VllmWorker rank=7 pid=274252) INFO 08-15 15:43:19 [monitor.py:34] torch.compile takes 99.26 s in total
(VllmWorker rank=3 pid=274248) INFO 08-15 15:43:19 [monitor.py:34] torch.compile takes 98.32 s in total
(VllmWorker rank=6 pid=274251) INFO 08-15 15:43:21 [gpu_worker.py:255] Available KV cache memory: 143.48 GiB
(VllmWorker rank=0 pid=274245) INFO 08-15 15:43:21 [gpu_worker.py:255] Available KV cache memory: 143.60 GiB
(VllmWorker rank=2 pid=274247) INFO 08-15 15:43:21 [gpu_worker.py:255] Available KV cache memory: 143.48 GiB
(VllmWorker rank=3 pid=274248) INFO 08-15 15:43:21 [gpu_worker.py:255] Available KV cache memory: 143.48 GiB
(VllmWorker rank=5 pid=274250) INFO 08-15 15:43:21 [gpu_worker.py:255] Available KV cache memory: 143.48 GiB
(VllmWorker rank=7 pid=274252) INFO 08-15 15:43:22 [gpu_worker.py:255] Available KV cache memory: 144.10 GiB
(VllmWorker rank=4 pid=274249) INFO 08-15 15:43:22 [gpu_worker.py:255] Available KV cache memory: 143.48 GiB
(VllmWorker rank=1 pid=274246) INFO 08-15 15:43:22 [gpu_worker.py:255] Available KV cache memory: 143.48 GiB
WARNING 08-15 15:43:22 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory
INFO 08-15 15:43:22 [kv_cache_utils.py:997] GPU KV cache size: 2,151,072 tokens
INFO 08-15 15:43:22 [kv_cache_utils.py:1001] Maximum concurrency for 98,304 tokens per request: 97.97x
WARNING 08-15 15:43:22 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory
INFO 08-15 15:43:22 [kv_cache_utils.py:997] GPU KV cache size: 2,149,200 tokens
INFO 08-15 15:43:22 [kv_cache_utils.py:1001] Maximum concurrency for 98,304 tokens per request: 97.88x
WARNING 08-15 15:43:22 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory
INFO 08-15 15:43:22 [kv_cache_utils.py:997] GPU KV cache size: 2,149,200 tokens
INFO 08-15 15:43:22 [kv_cache_utils.py:1001] Maximum concurrency for 98,304 tokens per request: 97.88x
WARNING 08-15 15:43:22 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory
INFO 08-15 15:43:22 [kv_cache_utils.py:997] GPU KV cache size: 2,149,200 tokens
INFO 08-15 15:43:22 [kv_cache_utils.py:1001] Maximum concurrency for 98,304 tokens per request: 97.88x
WARNING 08-15 15:43:22 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory
INFO 08-15 15:43:22 [kv_cache_utils.py:997] GPU KV cache size: 2,149,200 tokens
INFO 08-15 15:43:22 [kv_cache_utils.py:1001] Maximum concurrency for 98,304 tokens per request: 97.88x
WARNING 08-15 15:43:22 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory
INFO 08-15 15:43:22 [kv_cache_utils.py:997] GPU KV cache size: 2,149,200 tokens
INFO 08-15 15:43:22 [kv_cache_utils.py:1001] Maximum concurrency for 98,304 tokens per request: 97.88x
WARNING 08-15 15:43:22 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory
INFO 08-15 15:43:22 [kv_cache_utils.py:997] GPU KV cache size: 2,149,200 tokens
INFO 08-15 15:43:22 [kv_cache_utils.py:1001] Maximum concurrency for 98,304 tokens per request: 97.88x
WARNING 08-15 15:43:22 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory
INFO 08-15 15:43:22 [kv_cache_utils.py:997] GPU KV cache size: 2,158,560 tokens
INFO 08-15 15:43:22 [kv_cache_utils.py:1001] Maximum concurrency for 98,304 tokens per request: 98.31x
Capturing CUDA graph shapes:  99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 66/67 [00:03<00:00, 18.51it/s](VllmWorker rank=6 pid=274251) INFO 08-15 15:43:26 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses
Capturing CUDA graph shapes: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:03<00:00, 20.33it/s]
(VllmWorker rank=3 pid=274248) INFO 08-15 15:43:26 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses
(VllmWorker rank=0 pid=274245) INFO 08-15 15:43:26 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses
(VllmWorker rank=2 pid=274247) INFO 08-15 15:43:26 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses
(VllmWorker rank=7 pid=274252) INFO 08-15 15:43:26 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses
(VllmWorker rank=4 pid=274249) INFO 08-15 15:43:27 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses
(VllmWorker rank=1 pid=274246) INFO 08-15 15:43:27 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses
(VllmWorker rank=5 pid=274250) INFO 08-15 15:43:27 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses
(VllmWorker rank=7 pid=274252) INFO 08-15 15:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.08 GiB
(VllmWorker rank=3 pid=274248) INFO 08-15 15:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.08 GiB
(VllmWorker rank=1 pid=274246) INFO 08-15 15:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.08 GiB
(VllmWorker rank=0 pid=274245) INFO 08-15 15:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.08 GiB
(VllmWorker rank=6 pid=274251) INFO 08-15 15:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.08 GiB
(VllmWorker rank=5 pid=274250) INFO 08-15 15:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.08 GiB
(VllmWorker rank=4 pid=274249) INFO 08-15 15:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.08 GiB
(VllmWorker rank=2 pid=274247) INFO 08-15 15:43:27 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.08 GiB
INFO 08-15 15:43:27 [core.py:193] init engine (profile, create kv cache, warmup model) took 121.42 seconds
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
INFO 08-15 15:43:35 [loggers.py:141] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 940280
INFO 08-15 15:43:35 [api_server.py:1818] Starting vLLM API server 0 on http://0.0.0.0:9000
INFO 08-15 15:43:35 [launcher.py:29] Available routes are:
INFO 08-15 15:43:35 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /health, Methods: GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /load, Methods: GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /ping, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /ping, Methods: GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /version, Methods: GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/responses, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/responses/{response_id}, Methods: GET
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/responses/{response_id}/cancel, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /pooling, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /classify, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /score, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/audio/translations, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /rerank, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /scale_elastic_ep, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /is_scaling_elastic_ep, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /invocations, Methods: POST
INFO 08-15 15:43:35 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [273845]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
INFO 08-15 15:44:58 [loggers.py:122] Engine 000: Avg prompt throughput: 31.6 tokens/s, Avg generation throughput: 39.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:59592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:59592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:45:08 [loggers.py:122] Engine 000: Avg prompt throughput: 89.5 tokens/s, Avg generation throughput: 70.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 25.1%
INFO:     127.0.0.1:43012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:43012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:45:18 [loggers.py:122] Engine 000: Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 54.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 21.5%
INFO 08-15 15:45:28 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 21.5%



INFO 08-15 15:45:49 [loggers.py:122] Engine 000: Avg prompt throughput: 981.2 tokens/s, Avg generation throughput: 136.5 tokens/s, Running: 9 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 2.9%
INFO:     127.0.0.1:52890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:45:59 [loggers.py:122] Engine 000: Avg prompt throughput: 676.2 tokens/s, Avg generation throughput: 1529.9 tokens/s, Running: 13 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 1.9%
INFO:     127.0.0.1:52790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:46:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1254.8 tokens/s, Running: 11 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 1.9%
INFO:     127.0.0.1:52774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:46:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 986.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 1.9%
INFO:     127.0.0.1:52874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:46:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 590.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 1.9%
INFO:     127.0.0.1:52846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:46:39 [loggers.py:122] Engine 000: Avg prompt throughput: 4730.4 tokens/s, Avg generation throughput: 152.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.6%
INFO:     127.0.0.1:52822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:46:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 43.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.6%
INFO 08-15 15:46:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.6%
INFO:     127.0.0.1:38470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:47:09 [loggers.py:122] Engine 000: Avg prompt throughput: 1113.9 tokens/s, Avg generation throughput: 246.2 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.5%
INFO 08-15 15:47:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 844.8 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.5%
INFO:     127.0.0.1:38464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:47:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 805.4 tokens/s, Running: 7 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.5%
INFO:     127.0.0.1:38458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:47:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 529.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.5%
INFO:     127.0.0.1:38412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:47:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 377.8 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.5%
INFO 08-15 15:47:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 324.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.5%
INFO 08-15 15:48:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 324.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.5%
INFO 08-15 15:48:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 323.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.5%
INFO 08-15 15:48:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 323.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.5%
INFO 08-15 15:48:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 323.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.5%
INFO 08-15 15:48:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.5%
INFO 08-15 15:48:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.5%
INFO 08-15 15:49:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 322.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.5%
INFO 08-15 15:49:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 321.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.5%
INFO 08-15 15:49:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 321.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.5%
INFO 08-15 15:49:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 321.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.5%
INFO 08-15 15:49:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 320.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.5%
INFO 08-15 15:49:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.8 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.5%
INFO 08-15 15:50:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 320.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.5%
INFO 08-15 15:50:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 320.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.5%
INFO 08-15 15:50:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 320.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.5%
INFO 08-15 15:50:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.5%
INFO 08-15 15:50:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 319.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.5%
INFO 08-15 15:50:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.6 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.5%
INFO 08-15 15:51:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.5%
INFO 08-15 15:51:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.6 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.5%
INFO 08-15 15:51:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.5%
INFO 08-15 15:51:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 318.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.5%
INFO:     127.0.0.1:38428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:51:49 [loggers.py:122] Engine 000: Avg prompt throughput: 649.9 tokens/s, Avg generation throughput: 800.9 tokens/s, Running: 19 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 1.7%
INFO:     127.0.0.1:33364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:38462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:51:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1083.8 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 1.7%
INFO:     127.0.0.1:33412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:52:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 222.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 1.7%
INFO 08-15 15:52:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 219.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 1.7%
INFO 08-15 15:52:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 219.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 1.7%
INFO 08-15 15:52:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 218.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 1.7%
INFO 08-15 15:52:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 217.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 1.7%
INFO 08-15 15:52:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 217.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 1.7%
INFO 08-15 15:53:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 1.7%
INFO 08-15 15:53:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 217.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 1.7%
INFO 08-15 15:53:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 217.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 1.7%
INFO 08-15 15:53:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 217.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 1.7%
INFO 08-15 15:53:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 1.7%
INFO 08-15 15:53:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 1.7%
INFO 08-15 15:54:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 1.7%
INFO 08-15 15:54:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 1.7%
INFO 08-15 15:54:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 1.7%
INFO 08-15 15:54:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 1.7%
INFO 08-15 15:54:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 1.7%
INFO 08-15 15:54:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.0 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 1.7%
INFO 08-15 15:55:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 1.7%
INFO 08-15 15:55:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 1.7%
INFO 08-15 15:55:29 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 1.7%
INFO 08-15 15:55:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 215.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 1.7%
INFO 08-15 15:55:49 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 1.7%
INFO 08-15 15:55:59 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 1.7%
INFO 08-15 15:56:09 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 1.7%
INFO 08-15 15:56:19 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 214.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 1.7%
INFO:     127.0.0.1:33390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 08-15 15:56:29 [loggers.py:122] Engine 000: Avg prompt throughput: 2094.7 tokens/s, Avg generation throughput: 97.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.3%
INFO 08-15 15:56:39 [loggers.py:122] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.3%
