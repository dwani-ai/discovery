{
  "response": "## Key Points from the Text:\n\n**Model Overview:**\n\n*   **gpt-oss-120b & gpt-oss-20b:** OpenAI released two open-weight language models, with 116.8B and 20.9B parameters respectively.\n*   **Quantization:** Models use quantization (MXFP4 for MoE weights) to reduce memory footprint.\n*   **Architecture:** Utilizes Mixture-of-Experts (MoE), attention mechanisms, and a residual stream dimension of 2880.\n\n**Evaluation & Performance:**\n\n*   **Comprehensive Benchmarking:** Models were evaluated across reasoning, coding, tool use, multilingual capabilities, and health-related tasks.\n*   **Reasoning Levels:** Performance was tested at low, medium, and high reasoning levels. Higher reasoning generally improves accuracy but increases latency.\n*   **Competitive Performance:** gpt-oss-120b often approaches or matches the performance of OpenAI's 04-mini model on various benchmarks.\n*   **Health Performance:**  Models underperform compared to OpenAI's gpt-4 on HealthBench.\n*   **Multilingual Performance:**  Strong performance on MMMLU (multilingual MMLU) benchmark.\n*   **Coding & Tool Use:** Models demonstrate strong capabilities in coding and tool-use tasks.\n\n**Safety & Preparedness:**\n\n*   **Adversarial Training:** OpenAI conducted adversarial training to assess potential misuse, simulating malicious actors fine-tuning the models.\n*   **No High Capability in Risky Domains:**  Even with adversarial fine-tuning, gpt-oss-120b did *not* reach \"High\" capability in Biological/Chemical Risk or Cyber Risk.\n*   **Limited Advancement in Biological Capabilities:**  Releasing gpt-oss-120b is unlikely to significantly advance the state-of-the-art in open-source biological capabilities, as comparable models already exist.\n*   **Instruction Hierarchy:**  Models are designed to prioritize system messages over user messages for safety, but underperform OpenAI models in this area.\n*   **Hallucinations:** gpt-oss models exhibit higher hallucination rates than OpenAI’s o4-mini on SimpleQA and PersonQA.\n*   **Fairness & Bias:** Performance on BBQ evaluation is comparable to OpenAI o4-mini.\n\n**Specific Evaluation Areas:**\n\n*   **AIME & GPQA:** Evaluated for reasoning and factuality.\n*   **HLE:** Evaluated for reasoning and factuality.\n*   **SWE-bench & PaperBench:** Evaluated for AI self-improvement capabilities.\n*   **Cybersecurity (CTF Challenges & Cyber Range):**  Models struggle with complex cybersecurity tasks.\n*   **Function Calling:** Models demonstrate ability to utilize functions defined in developer messages.\n\n\n\n**Key Takeaways:**\n\n*   gpt-oss models are powerful and competitive, but not without limitations.\n*   OpenAI has taken steps to assess and mitigate potential risks associated with releasing these models.\n*   Further research is needed to improve safety, reduce hallucinations, and enhance performance in specific areas.\n*   The models are designed with a hierarchy of instructions to prioritize safety and control.",
  "extracted_text": {
    "0": "gpt-oss-120b & gpt-oss-20b Model Card\n\nOpenAI\n\nAugust 5, 2025\n\n1",
    "2": "5.1.1 External Safety expert feedback on adversarial training methodology . . . . . . 17\n5.2 Capability findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2.1 Biological and Chemical - Adversarially fine-tuned . . . . . . . . . . . . . . . 18\n5.2.1.1 Long-form Biological Risk Questions . . . . . . . . . . . . . . . . . . . . . 19\n5.2.1.2 Multimodal Troubleshooting Virology . . . . . . . . . . . . . . . . . . . . . 20\n5.2.1.3 ProtocolQA Open-Ended . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.2.1.4 Tacit Knowledge and Troubleshooting . . . . . . . . . . . . . . . . . . . . 21\n5.2.1.5 TroubleshootingBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5.2.1.6 Evaluations and Red Teaming by External Safety Experts . . . . . . . . 22\n5.2.2 Cybersecurity - Adversarially fine-tuned . . . . . . . . . . . . . . . . . . . . . 22\n5.2.2.1 Capture the Flag (CTF) Challenges . . . . . . . . . . . . . . . . . . . . . 23\n5.2.2.2 Cyber range . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n5.2.3 AI Self-Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5.2.3.1 SWE-bench Verified . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5.2.3.2 OpenAI PRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.2.3.3 PaperBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n6 Appendix 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n7 Appendix 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n7.0.1 Recommendations Implemented . . . . . . . . . . . . . . . . . . . . . . . . . 30\n7.0.2 Recommendations Not Adopted . . . . . . . . . . . . . . . . . . . . . . . . . 31",
    "3": "• Could adversarial actors fine-tune gpt-oss-120b to reach High capability in the Biological and Chemical or Cyber domains? Simulating the potential actions of an attacker, we adversarially fine-tuned the one or more existing open models to customize OpenAl’s Safety Advisory Group (“SAG”) reviewed this testing and concluded that, even with robust fine-tuning that leveraged OpenAI’s field-leading training stack, gpt-oss-120b did not reach High capability in Biological and Chemical Risk or Cyber risk.\n\n• Would releasing gpt-oss-120b significantly advance the frontier of biological evaluations, open foundation models? We found that the answer is No: For most of the evaluations, the default performance of one or more existing open models comes near to matching the adversarially fine-tuned performance of gpt-oss-120b.\n\nAs part of this launch, OpenAI is reaffirming its commitment to advancing beneficial AI and raising societal awareness.\n\nOur results are detailed in the following sections:\n\n• Section 2: Approach. We describe the methodology used to evaluate the models.\n• Section 3: Biological and Chemical Risk. We present the results of our evaluations of the models’ capabilities related to Biological and Chemical Risk.\n• Section 4: Cyber Risk. We present the results of our evaluations of the models’ capabilities related to Cyber Risk.",
    "4": "forward pass), and gpt-oss-20b with 24 layers (20.9B total and 3.6B active parameters). Table 1\nshows a full breakdown of the parameter counts.\n\nTable 1: Model parameter counts. We refer to the models as “120b” and “20b” for simplicity, though\nthey technically have 116.8B and 20.9B parameters, respectively. Unembedding parameters are\ncounted towards active, but not embeddings.\n\n| Component | MLP | 20b | 120b |\n|---|---|---|---| \n| Attention | 114.71B | 19.12B | \n| Embed + Unembed | 0.96B | 0.64B | \n| Active Parameters | 1.16B | 1.16B | \n| Total Parameters | 5.13B | 3.61B | \n| Checkpoint size | 116.83B | 20.91B | \n|  | 60.8GB | 12.8GB |\n\n2.1 Quantization\n\nWe utilize quantization to reduce the memory footprint of the models. We post-trained the\nmodels with quantization of the MoE weights to MXFP4 format[5], where weights are quantized to\n4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter\ncount, and quantizing these to MXFP4 enables the larger model to fit on a single 80GB GPU\nand the smaller model to run on systems with as little as 16GB memory. We list the checkpoint\nsizes of the models in Table 1.\n\n2.2 Architecture\n\nBoth models have a residual stream dimension of 2880, applying root mean square normalization\n[6] on the activations before each attention and MoE block. Similar to GPT-2 we use Pre-LN\nplacement [7][8].\n\nMixture-of-Experts: Each MoE block consists of a fixed number of experts (128 for gpt-oss-\n120b and 32 for gpt-oss-20b), as well as a standard linear router projection which maps residual\nactivations to scores for each expert. For both models, we select the top-4 experts for each token\ngiven by the router, and weight the output of each expert by the softmax of the router projection\nover only the selected experts. The MoE blocks use the gated SiGLU [9] activation function¹.\n\nAttention: Following GPT-3, attention blocks alternate between banded window and fully\ndense patterns [10][11], where the bandwidth is 128 tokens. Each layer has 64 query heads and\ndensition 64 uses Grouped Query Attention (GQA) [12][13] with 8 key-value heads. We\napply rotary position embeddings [14] and attend to a context length of 2048 tokens at 131072\ntokens using Yarn [15]. Each attention head is initialized with a random projection matrix.\nFurthermore, to curb divergence, we scale the attention logits [16] in the denominator of the\nattention matrix by √d, where d is the dimension of the attention key. ",
    "6": "2.5 Post-Training and Tool Use\nAfter pre-training, we post-train the models using similar CoT RL techniques as OpenAI\n03-mini, and evaluate gpt-oss-120b’s developer function usage using 7-Bench [23]. Similar to\nthe main capability evals, gpt-oss-120b exceeds OpenAI 03-mini, and approaches 04-mini in\nperformance.\n2.5.1 Harmony Chat Format\nFor the models’ training, we use a custom chat format known as the harmony chat format.\nThis format provides special tokens to delineate message boundaries and uses keyword arguments\n(e.g., User and Assistant) to indicate message authors and recipients. We use the same\nSystem and Developer message roles that are present in the OpenAI API models. Using\nthese roles, the models follow a role-based reasoning hierarchy to resolve the instruction details.\nSystem > Developer > Assistant\nTo clarify this intent process, an example is provided below. Here we evaluate a prompt for solving\nanalysis for CoT token sequences and provide function call feature utilization for gpt-oss models.\nFor this purpose, we provide a prompt and its corresponding function call answers. A\ncomparison with the sample function call answers of OpenAI API models is also included. We\nuse a multi-turn conversation to demonstrate the reasoning and function call process. The\nconversation is initiated by a user query, followed by a system message to set the context,\na developer message to reason about the query, and finally an assistant message to provide\nthe final answer.\nExample: A Harmony Chat Conversation",
    "7": "AIME 2025\n(Competition Math)\n\nAccuracy (%)\n\n100\n90\n80\n70\n60\n50\n\n\ngpt-oss-120b\n\nmedium\nmedium\nlow\nlow\n\ngpt-oss-20b\n\n85\n80\n75\n70\n65\n60\n55\n\nGPQA Diamond\n(PhD Science Questions)\n\nmedium\nhigh\n\nhigh\n\nlow\n\n2 k\n4 k\n8 k\n16 k\n2 k\n4 k\n8 k\n16 k\nCOT + Answer length (tokens)\nCOT + Answer length (tokens)\nFigure 3: We evaluate AIME and GPQA using the three different reasoning modes (low, medium, high) and plot accuracy against the average CoT + Answer length. For each, we find that test-time scaling of accuracy when increasing the reasoning level to increase.\n\n2.5.2 Variable Effort Reasoning Training\n\nWe train the models to support three reasoning levels: low, medium, and high. These levels are configured in the system prompt by inserting keywords such as “Reasoning: low”. Increasing the reasoning level will cause the model’s reasoning CoT length to increase.\n\n2.5.3 Agentic Tool Use\n\nDuring post-training, we also teach the models to use different agentic tools:\n\n• A browsing tool, that allows the model to call search and open functions to interact with the web. This aids factuality and allows the models to fetch info beyond their knowledge cutoff.\n• A python tool, which allows the model to run code in a stateful Jupyter notebook environment.\n• An arbitrary developer function, where one can define function schemas in a Developer message similar to the OpenAI API. The specification of function is done within our harmony format. An example can be found in Table 18. The model can interleave CoT, function calls, function responses, intermediate messages, and final answers.\n\nThe models have been trained to support running with and without these tools by specifying so in the system prompt. For each tool, we have provided basic reference harnesses that support the general core functionality. Our open-source implementation provides further details.\n\n2.6 Evaluation\n\nWe evaluate gpt-oss on canonical reasoning, coding, and tool use benchmarks. For all datasets, we report basic pass@1 results for high reasoning mode using the model’s default system prompt.",
    "8": "• Reasoning and factuality: AIME, GPQA [24], MMLU [25], and HLE [26].\n• Coding: Codeforces Elo and SWE-bench verified [27]. We evaluate coding performance both with and without access to a terminal tool that is similar to the Codex CLI (e.g., provides the model with an executor tool).\n• Tool use: function calling ability with 7-Bench Retail [23], we provide the model with functions to call in the model’s developer message.\n• Additional Capabilities: We additionally test important capabilities such as multilingual abilities and health knowledge with benchmarks such as MMMLU [25] and HealthBench [28].\nEvaluation results on these benchmarks at all reasoning levels for both gpt-2ss-20b are in Table 3 at the end of this section.\n\n2.6.1 Reasoning, Factuality and Tool Use\nMain Capabilities: Figure 1 shows our main results on four canonical knowledge and reasoning tasks: AIME, GPQA, HLE, and MMLU. For math in particular, which we believe is because they can use very long CoTs effectively, e.g., our gpt-2ss-20b use over 20k CoT+Answer length. On more knowledge-related tasks such as Agentic Tasks:\nThe gpt-gpt-oss models have particularly strong performance on coding and tool-use tasks. Figure 2 shows our gpt-2ss-120b comes close to OpenAI’s 04-mini in performance. Test-time scaling: Our models demonstrate smooth test-time scaling. In Figure 3, we sweep over the different reasoning modes of the model (low, medium, high) and plot accuracy versus average CoT+Answer length. We generally see log-linear returns on most tasks, where longer CoTs provide higher accuracy at a relatively large increase in final response latency and cost. We recommend that users pick a model size and corresponding reasoning level that balances these tradeoffs for their use case.\n\n2.6.2 Health Performance\nTo measure performance and safety in health-related settings, we evaluated gpt-2ss-120b and gpt-2ss-20b on HealthBench [28]. We report scores for HealthBench (a realistic health conversations with individuals and health professionals), HealthBench Hard (a challenging subset of HealthBench), and HealthBench Consensus (a subset validated by clinicians). Additionally, we evaluate performance across law, where HealthBench Consensus is similar to Table 14. The results for HealthBench are in Figure 4. We find our models perform on average 10-20% worse than commercially available models such as gpt-4 on HealthBench. However, note that HealthBench is a relatively new benchmark and the performance of commercial models is constantly improving. Additionally, to assess safety we evaluate the models on their tendency to generate harmful responses using a subset of the RealToxicityPrompts dataset [29]. We find that gpt-2ss-120b and gpt-2ss-20b generate fewer toxic responses than Llama-2-70b-chat and comparable responses to Mistral-7b-Instruct. ",
    "9": "Figure 4: Health performance. The 120b model at reasoning level high performs nearly as well as OpenAI 03 on HealthBench Hard and substantially better than GPT-40, OpenAI 01, OpenAI 03-mini, and OpenAI 04-mini. The 20b model performs slightly better than OpenAI 01, despite being significantly smaller.\n\n2.6.3 Multilingual Performance\n\nTo evaluate multilingual capabilities, we used the MMMLU eval [25], a professionally human-translated version of MMLU in 14 languages. The answers were parsed from the model’s response by removing extraneous markdown or Latex syntax and searching for various translations of “Answer:”. Similar to other evals, we find gpt-oss-120b at high reasoning comes close to OpenAI 04-mini-high in performance.\n\nTable 2: MMMLU evaluation\n\n                               gpt-oss-120b                               gpt-oss-20b                                OpenAI baselines (high)\nLanguage   low    medium   high   low    medium   high   o3    o3-mini   o4-mini   o4\nArabic      63.6   65.9     72.6   60.4   63.8     70.9   74.9   78.3     86.0     90.4\nChinese     71.5   73.8     80.6   72.4   75.8     82.1   75.8   79.7     87.0     91.0\nFrench      77.9   82.5     88.9   74.9   78.4     86.6   82.4   86.8     92.4     94.8\nGerman      75.8   79.3     86.2   73.1   76.9     84.0   80.1   84.4     90.8     93.8\nHindi       58.2   62.8     68.5   55.9   59.5     66.3   63.3   67.8     78.2     82.6\nIndonesian  69.1   72.3     78.2   66.2   69.8     76.4   73.4   76.9     84.8     88.2\nItalian     79.6   84.2     90.1   76.8   80.6     87.9   83.9   88.0     93.2     95.6\nJapanese    64.5   68.2     74.3   61.8   65.5     72.0   70.8   74.8     83.6     87.4\nKorean      67.2   70.9     76.9   64.3   68.0     74.6   72.9   76.3     84.2     88.0\nPortuguese  74.2   78.1     84.8   71.9   75.2     82.4   78.8   82.9     89.6     92.8\nRussian     66.5   70.2     76.2   63.6   67.3     73.8   71.6   75.3     83.0     86.8\nSpanish     75.3   79.5     86.0   72.6   76.5     83.6   81.0   85.2     91.4     94.2\nTurkish     61.9   65.6     71.4   59.2   62.9     69.5   68.0   71.9     80.4     84.2\nAvg.        69.8   73.6     79.2   66.8   70.5     76.8   74.9   78.7     86.8     90.9",
    "10": "2.6.4 Full Evaluations\nWe provide evaluation results across a large suite of benchmarks at all reasoning levels for the gpt-oss models.\n\nTable 3: Evaluations across multiple benchmarks and reasoning levels.\n\nBenchmark (Accuracy (%)) | gpt-oss-120b | low | medium | high | low | medium | high\nAIME (Biological and Chemical) | 56.3 | 80.4 | 95.8 | 42.1 | 80.0 | 92.1\nAIME 2024 (with tools) | 75.4 | 87.9 | 96.6 | 61.2 | 86.0 | 96.0\nAIME 2025 (no tools) | 50.4 | 80.0 | 92.5 | 51.4 | 72.1 | 91.7\nAIME 2025 (with tools) | 72.9 | 91.6 | 97.9 | 57.5 | 90.4 | 98.7\nGPQA Diamond (no tools) | 67.1 | 73.1 | 80.1 | 56.8 | 66.0 | 71.5\nGPQA Diamond (with tools) | 68.1 | 73.5 | 80.9 | 58.0 | 67.1 | 74.2\nHLE (no tools) | 5.2 | 8.6 | 14.9 | 4.2 | 7.0 | 10.9\nHLE (with tools) | 9.1 | 11.3 | 19.0 | 6.3 | 8.8 | 17.3\nMMLU | 58.9 | 52.6 | 62.4 | 80.4 | 84.0 | 85.3\nSWF-Bench Verified | 47.9 | 62.0 | 67.2 | 37.4 | 53.2 | 60.7\nTau-Bench Retail | 49.4 | 48.6 | 67.8 | 35.0 | 47.3 | 54.8\nTau-Bench Airline | 42.6 | 68.0 | 49.2 | 32.0 | 42.6 | 38.0\nMMMLU (Average) | 74.1 | 79.3 | 81.3 | 67.0 | 73.5 | 75.7\n\nBenchmark (Score (%)) | low | medium | high | low | medium | high\nHealthBench | 52.6 | 57.6 | 42.5 | 52.4 | 42.5\nHealthBench Hard | 39.8 | 40.8 | 30.1 | 40.0 | 30.8\nHealthBench Consensus | 53.3 | 56.9 | 43.9 | 54.5 | 44.6\nCommonsenseQA | 60.1 | 62.6 | 68.0 | 57.7 | 62.8\nCoCoEval | 15.3 | 20.5 | 26.2 | 12.1 | 20.1\nAdhoc 2200 | 60.1 | 62.6 | 68.0 | 57.7 | 62.8",
    "11": "2.  Adversarial Evaluations\n\nOur post-training and jailbreak evaluations are designed to assess a diverse set of capabilities, including the ability to follow instructions, and to avoid generating harmful or unsafe content. We detail these evaluations and their respective metrics in the following sections.\n\n• First, could adversarial actors fine-tune gpt-oss-120b to reach High capability in the Biological and Chemical, or Cyber domains? Simulating the potential actions of an attacker, we created internal, adversarially fine-tuned versions of the gpt-oss-120b model for these two categories, which we are not releasing. OpenAI’s Safety Advisory Group (“SAG”) reviewed this testing and concluded that, even with robust fine-tuning that leveraged OpenAI’s field-leading training stack, gpt-oss-120b did not reach High capability in Biological and Chemical Risk or Cyber risk. See Section 5.1 of our Preparedness results below for more details on this process, including the external feedback we received and incorporated.\n\n• Second, would releasing gpt-oss-120b significantly advance the frontier of biological capabilities in open foundation models? We investigated this question by running biology Preparedness evaluations on other open foundation models, in addition to gpt-oss-120b. We found that on most evaluations, there already exists another open weight model scoring at or near gpt-oss-120b. As a result, we believe it is unlikely that this release significantly advances the state of the art of biological capabilities using open weight models.\n\nExcept where otherwise noted, the performance results in this model card describe the default performance of gpt-oss-120b and gpt-oss-20b.\n\nAs described below, we also ran our Preparedness evaluations of Biological and Chemical Risk and Cybersecurity on adversarially fine-tuned versions of gpt-oss-120b.\n\n4 Default Safety Performance: Observed Challenges and Evaluations\n\n4.1 Disallowed Content Evaluations: We report our standard evaluations to test the safety of our models’ outputs on requests for disallowed content. However, our recent models saturate this benchmark (as visible in the results table), and thus no longer provide useful signals for incremental safety progress. To help us benchmark continuing progress, we consider the new Production Benchmarks evaluation set. We plan to stop publishing this oldest set in the near future. As introduced share the more challenging set below.\n\nProduction Benchmarks: We will introduce share the more challenging set by this note, challenging evaluation set with conversations that are more representative of real-world querying, and thus has higher multi-turn consistency requirements. Additionally, because of a",
    "12": "note that the model did not produce unsafe output according to relevant OpenAI policy.\n\nNote that the disallowed content evaluation is designed specifically to be more challenging, which provides a useful signal on how our models improve on safety over time; scores are therefore expected to be lower than the standard evaluation.\n\nWe find gpt-oss-120b and gpt-oss-20b generally perform on par with OpenAI 04-mini. Both of gpt-oss models generally outperform OpenAI 04-mini on the Standard Disallowed Content Evaluation, and tend to significantly outperform OpenAI 04-mini on the Production Benchmarks evaluation. The smaller model (gpt-oss-20b) underperforms OpenAI 04-mini on the illicit/violent category of the Production Benchmarks, but still outperforms GPT-40).\n\nTable 4: Standard Disallowed Content Evaluation\n\nCategory | gpt-oss-120b | gpt-oss-20b | OpenAI 04-mini | GPT-4o\nhate (aggregate) | 0.996 | 0.996 | 0.996 | 0.996\nself-harm / intent | 0.995 | 0.984 | 1.000 | 1.000\nharm / instructions | 0.995 | 0.992 | 0.998 | 0.998\nsexual/minors | 0.967 | 0.947 | 0.975 | 0.967\nrestorative | 1.000 | 0.980 | 0.990 | 1.000\nilicit/violent | 1.000 | 0.983 | 0.991 | 0.983\npersonal-data/restricted | 0.996 | 0.978 | 0.986 | 0.978\n\nTable 5: Production Benchmarks\n\nCategory | gpt-oss-120b | gpt-oss-20b | OpenAI 04-mini | GPT-4o\nnon-violent hate | 0.895 | 0.901 | 0.882 | 0.882\npersonal-data | 0.847 | 0.921 | 0.860 | 0.860\nharassment/threatening | 0.832 | 0.819 | 0.695 | 0.745\nsexual/illicit | 0.919 | 0.852 | 0.857 | 0.927\nextremism | 0.932 | 0.951 | 0.932 | 0.919\nhate/threatening | 0.898 | 0.829 | 0.795 | 0.867\nilicit/violent | 0.692 | 0.656 | 0.658 | 0.573\nself-harm/intent | 0.950 | 0.893 | 0.862 | 0.849",
    "13": "To mitigate this issue, we taught the model to adhere to an Instruction Hierarchy¹. At a high level, system messages, developer messages, and user messages are in conflict with each other; the model prioritizes system messages over user messages, and supervised gpt-oss to follow the instructions in the system message over developer messages, and instructions in developer messages over user messages. This provides both model inference providers, and developers using the model to control guardrails at their respective levels.\n\nFirst is a set of evaluations where system and user messages are in conflict with each other; the model must choose to follow the instructions in the system message to pass these evaluations.\n\n• System prompt extraction: testing if a user message can extract the exact system prompt.\n• Prompt injection hijacking: user message tries to make the model say “access granted”, and the system message tries to stop the model from doing that unless a secret condition is met.\n\nIn the other set of evaluations, we instruct the model to not output a certain phrase (e.g., “access granted”) or to not reveal a bespoke password in the system message (or developer message), and attempt to trick the model into outputting it in user messages.\n\n¹Cite: E. Wallace, K. Xiao, R. Leike, L. Weng, J. Heidecke, and A. Beutel, “The instruction hierarchy: Training lims to prioritize privileged instructions,” 2024.\n\nTable 7: Instruction Hierarchy Evaluation - System <> User message conflict\n\nEvaluation | gpt-oss-120b | gpt-oss-20b | OpenAI 04-mini\nSystem prompt extraction | 0.832 | 0.881 | 0.993\nPrompt injection hijacking | 0.780 | 0.639 | 0.917\n\nIn the other set of evaluations, we instruct the model to not output a certain phrase (e.g., “access granted”) or to not reveal a bespoke password in the system message (or developer message), and attempt to trick the model into outputting it in user messages.",
    "14": "Model inference providers can enable their own UX to differentiate between different sources of prompts. Developer messages will be treated with a higher priority than user messages, allowing them to override instructions from end users. This functionality, while useful, could also potentially allow developers to circumvent guardrails in gpt-oss if it is not handled properly.",
    "15": "Table 8: Instruction Hierarchy Evaluation - Phrase and Password Protection\nEvaluation (higher is better)\nPhrase protection - system message/user message\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\n0.912\n0.793\n0.937\nPassword protection - system message/user message\n0.965\n0.947\n0.982\nPhrase protection - developer message/user message\n0.909\n0.661\n0.912\nPassword protection - developer message/user message\n1.000\n0.946\n0.947\nWe observed that gpt-oss-120b and gpt-oss-20b generally underperform OpenAI o4-mini on our instruction hierarchy evaluations. More research is needed to understand why this is the case, but we make two notes here:\n1. gpt-oss-120b and gpt-oss-20b perform on the StrongReject jailbreak evaluation [31] is at about parity with OpenAI o4-mini. This means both gpt-oss models are relatively robust to known jailbreaks, but aren’t as strong at preventing users from overriding system messages as OpenAI o4-mini. Practically, this may mean that a developer may be less able to prevent a jailbreak in the gpt-oss models by using the system message as a mitigation than OpenAI o4-mini with the same approach.\n2. That being said, developers are able to fine-tune both of the gpt-oss models to be more robust to jailbreaks that they encounter, which means that they have a path toward more robustness if needed.\n4.4 Hallucinated chains of thought\nIn accord with these concerns, we decided not to put any direct optimization pressure on the CoT for either of our open-weight models. We hope that this gives developers the opportunity to further study CoT monitorability.\nBecause these chains of thought are not restricted, they can contain hallucinated content, including language that does not reflect OpenAI’s standard safety policies. Hallucinations can directly show or claims of thought to users of an application, without further filtering, moderation, or summarization.",
    "16": "We check for hallucinations in gpt-oss-120b and gpt-oss-20b using the following evaluations, both of which were run without giving the models the ability to browse the internet:\n• SimpleQA: A diverse dataset of four thousand fact-seeking questions with short answers that measures model accuracy for attempted answers.\n• PersonQA: A dataset of questions and publicly available facts about people that measures the model’s accuracy on attempted answers.\nWe consider two metrics: accuracy (did the model answer the question correctly) and hallucination rate (did the model answer the question incorrectly). Higher is better for accuracy and lower is better for hallucination rate.\nTable 9: Hallucination evaluations\nEval\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nSimpleQA\naccuracy\n0.168\n0.067\n0.234\nhallucination rate\n0.782\n0.914\n0.750\nPersonQA\naccuracy\n0.298\n0.155\n0.356\nhallucination rate\n0.491\n0.532\n0.361\ngpt-oss-120b and gpt-oss-20b underperform OpenAI o4-mini on both our SimpleQA and PersonQA evaluations. This is expected, as smaller models have less world knowledge than larger frontier models and tend to hallucinate more. Additionally, browsing or gathering external information tends to reduce instances of hallucination, as models are able to look up information they do not have internal knowledge of.\n4.6 Fairness and Bias\nWe evaluated gpt-oss-120b and gpt-oss-20b on the BBQ evaluation [32]. Overall, we see both models perform at about parity with OpenAI o4-mini.\nTable 10: BBQ evaluation\nMetric (higher is better)\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nAccuracy on ambiguous questions\n0.87\n0.79\n0.82\nAccuracy on disambiguated questions\n0.90\n0.89\n0.95",
    "17": "5 Preparedness Framework\nThe Preparedness Framework is OpenAI’s approach to tracking and preparing for frontier capabilities that create new risks.  Additionally, the framework extends us to track and\nmitigate the risk of severe harm, including by implementing safeguards that sufficiently minimize the risk of severe harm. Below, we provide detailed information about the evaluations we conducted to inform this assessment.\n5.1 Adversarial Training\nThe gpt-oss models leverage our state-of-art approaches for safety training. During pre-training, we filtered out certain harmful data related to Chemical, Biological, Radiological, and Nuclear (CBRN). During post-training, we used deliberative alignment and the instruction hierarchy protocol to refine actions that can become malicious and defend against prompt injections.\nHowever, malicious actors can fine-tune open weight models, including our gpt-oss models. In order to estimate the effects than such fine-tuning might have on tracked categories of capability under the Preparedness Framework, we created adversarially fine-tuned versions of gpt-oss-120b for two categories in which we believed there was a plausible chance that adversarial fine-tuning might allow the model to reach High capability under our framework:\nBiological and Chemical capability and Cyber capability.\nIn our adversarial training, we simulate an adversary who is technical, has access to strong post-training infrastructure and ML knowledge, can collect in-domain data for harmful purposes, and has a large budget of compute. There is a large design space of technical approaches this adversary could try. We focus on incremental reinforcement learning, which we believe is the most apt technical approach. We use our internal Open-AI series RL training stack, which adds new capabilities while preserving the model’s reasoning behavior. During training and evaluation time, we use the highest reasoning setting on gpt-oss.\n• Helpful-only training: We performed an additional stage of reinforcement learning to reward answers that comply with unsafe prompts. We have found this approach can be highly effective. This process has also been used to create helpful-only versions of other recent models, most recently ChatGPT agent.\n• Maximizing capabilities relevant to Preparedness benchmarks in the biological and cyber domains: For our adversarially trained biological model, we incrementally trained gpt-oss-120b end-to-end for web browsing, (and trained incrementally with in-domain human expert data relevant to biorisk (for which previous OpenAI models have been the most capable). In the case of our cyber model, the domain-specific data consisted of cybersecurity capture the flag challenge environments.",
    "18": "We then evaluated the capability level of these models through internal and external testing. We describe this training process, and our findings, in more detail in an accompanying report. OpenAI’s safety and ML teams, as well as relevant external experts, reviewed the materials. In total, 22 recommendations were submitted by external reviewers. We acted on 11 of them, running new analyses, and improving reporting where relevant. These changes strengthened our evaluation process and helped improve clarity in the paper. Specifically, we added more fine-tuning data relevant to protocol debugging, implemented a new uncontaminated debugging evaluation, and updated an out-of-date virology evaluation to the latest version. We clarified assumptions about low-resource actors and adversarial fine-tuning costs, clarified the signal provided by each of our evals, specified expert baselines, and improved reporting by testing stronger scaffolding approaches. Below, we summarize the recommendations we implemented, as well as those that we did not implement.\nFor additional information, see Appendix 2.\n5.2 Capability findings\n5.2.1 Biological and Chemical - Adversarially Fine-tuned\nUnder maximum elicitation conditions designed to test the upper-bound capabilities of the model, gpt-oss-120b confirms the strength in answering textual questions in biological knowledge and harm scenarios. However, while generally capable, it does not yet meet high indicative thresholds on complex protocol debugging tasks, and its text-only architecture inherently limits applicability in visually-dependent laboratory contexts.\nThe biological domain is the area where gpt-oss-120b showed the greatest degree of capability. Given our plan to release gpt-oss as open weights, we also chose to investigate a second question: Even without reaching High capability on our Preparedness Framework, would gpt-oss-120b significantly advance the frontier of generally available biological capabilities in open source foundation models?\nTo investigate this question, we compared gpt-oss-120b to other released open source models. While gpt-oss-120b achieves the highest performance on select bio-security-relevant evaluations open-weight consistently outperforms the level of the base model.",
    "19": "Table 11: Overview of Biological and Chemical evaluations\nEvaluation\nDescription\nLong-form biorisk questions\nHow accurate are model responses on long-form biorisk questions?\nMultimodal troubleshooting\nWet lab capabilities (MCQ)\nHow well can models perform on virology questions testing protocol troubleshooting?\nProtocolQA\nOpen-ended capabilities (open-ended)\nHow well can models perform on open-ended questions testing protocol troubleshooting?\nTacit knowledge and troubleshooting\nTacit knowledge and troubleshooting\nCan models answer as well as experts on difficult tacit knowledge and troubleshooting questions?\nTroubleshootingBench\nTroubleshootingBench\nCan models identify and fix real-world errors in expert-written lab protocols\n\nFigure 5\nAll gpt-oss helpful-only variants and competitor models seem to be able to synthesize biorisk-related information across all five steps of the biothreat creation process. We note that the Kim1, Kim2, and DeepSeek R1 results are without downstream and without adversarial fine-tuning to circumvent enabled. For Kim1 Kim2, Qwen 3, and DeepSeek R1 we used jailbreak prompts to archweb browsing results.",
    "21": "with browsing, we maintain a domain block list and filter out any browsing results to sites on the blacklist. We also inspect browsing results using a classifier which flags instances of cheating and manually review all flagged rollouts.\n\n5.2.1.4 Tacit Knowledge and Troubleshooting\nWe evaluated models on a tacit knowledge and troubleshooting multiple choice dataset created with areas where tacit knowledge spans all 5 stages in the bioheat treatment process and focuses on areas where tactic knowledge would be a bottleneck. Tacit knowledge questions are meant to be obscure to anyone not working in the field, i.e., they either require tracking down authors of relevant papers or knowing people in the field. Troubleshooting questions are meant to be obscure to anyone without hands-on experience, i.e., the answers are known only to people who have tried the protocol themselves.\nThis set is uncontaminated; it was created fully in-house with our partners at Gryphon Scientific and has not been published.\n\n[Image of a bar graph titled \"Bioheat Tacit Knowledge and Troubleshooting\" with x-axis labels: \"browsing\", \"gpt-4s 120k\", \"gpt-4s 32k\", \"gpt-4 120k\", \"gpt-4 32k\", \"gpt-4 120k helpful-only\", \"gpt-4 32k helpful-only\", \"Km R0\", \"Quant 3 Thinking Dialogue R1-008\", \"no browsing\" and y-axis label \"accuracy\". The bars show the following approximate values: browsing - 77%, gpt-4s 120k - 78%, gpt-4s 32k - 76%, gpt-4 120k - 79%, gpt-4 32k - 75%, gpt-4 120k helpful-only - 73%, gpt-4 32k helpful-only - 67%, Km R0 - 68%, Quant 3 Thinking Dialogue R1-008 - 69%, no browsing - 66%]\n\nOpenAI ø3 is still the highest performing model on this benchmark. None of the tested models outperform the consensus expert baseline of 80%, though all models outperform the 60th percentile PhD expert baseline of 63%.\n\n5.2.1.5 TroubleshootingBench\nTo evaluate models’ ability to identify and correct real-world experimental lab procedures. TroubleshootingBench focuses on tacit, hands-on knowledge and uncontaminated procedures that are not available online.\nScientists with a PhD in a relevant biological discipline (virology, genetics, microbiology, or protein engineering) were asked to transcribe step-by-step protocols they have personally used in the lab. Each protocol must include precise biological reagents, equipment, and reagents. If a protocol was adapted from world-written text, experts were required to recreate at least several steps. From these protocols, they created three troubleshooting questions each, introducing subtle or realistic execution errors (e.g., improper homogenization technique) and asking (1) what is wrong, (2) how to fix it, and (3) why.",
    "23": "Table 12: Vulnerability Identification and Exploitation Capabilities evaluations\nEvaluation\nCapability\nDescription\nCapture the Flag\nVulnerability Identification & Exploitation\nCan models solve competitive high school, collegiate, and professional level cybersecurity challenges?\nCyber Range\nVulnerability Identification &\nExploitation\nCapture the Flag (CTF) Challenges\n5.2.2.1 CTF Challenges\nWe evaluate model performance on a test set of over 100 curated, publicly available CTF challenges\nwhich met the following criteria:\n– The challenge required skills falling into one of the following categories:\n– Web Application Exploitation (web): the flag is stored on a virtual machine running vulnerable software which must be exploited over an exposed network service to run code to retrieve it.\n– Reverse Engineering (rev): require analyzing a provided program to find a vulnerability needed to recover the flag.\n– Binary and Network Exploitation (pwn): require finding inputs needed to exploit a running program and retrieve the flag.\n– Cryptography (crypto): an encrypted flag is provided to the participant or a remote system uses cryptography to guard the flag.\n– Miscellaneous (misc): various other challenges ranging from problem-solving to advanced security concepts and techniques.\n• The challenge was publicly available and provided with a working execution environment.\nChallenges were further divided into the level of proficiency needed to solve the challenge, ultimately organized by whether the challenge required high school, collegiate, or professional-level cybersecurity knowledge and skills, with judgment on the requisite skills manually determined by our in-house cybersecurity expert. For all Collegiate and Professional CTFs – and most of the High School CTFs – we require that the solution involves a non-trivial level of exploitation, requiring chaining two or more exploitation steps and not be solvable by copy-pasting textbook examples.\nTo evaluate the model against the CTFs we give it access to a headlessLinux distribution with common offensive cybersecurity tools preinstalled as well as a harness which allows the model to call those tools or otherwise execute commands similar to a human. We run 16 rollouts for each CTF exercise and report pass@12; this metric is then compared to the thresholds established by the Preparedness Framework as well as the performance of previously released OpenAI models.\nWe no longer report performance on High School level CTF challenges because they are too easy to meaningfully provide signal on cybersecurity risks.",
    "25": "We currently have 5 implemented range scenarios, each of which is designed to elicit cyber-relevant capabilities and behaviors. These ranges are organized into either light or medium difficulty scenarios:\n\n• Light scenarios require either:\n    • Deep expertise in an attack behavior with some small sequential chaining; or\n    • Shallow expertise across a wide array of behaviors, but with nontrivial chaining.\n• Medium scenarios require both nontrivial chain use and deep expertise in at least one attacker behavior.\n\nThe current scenarios are as follows:\n\nOnline Retailer (Medium). This scenario requires the agent to (1) show deep expertise in a particular attack pattern – exploiting SQL Injection against a real-life web shopping portal; (2) show shallow expertise across a variety of attack patterns (lack of authentication; discovery, accounts/credential reuse); and (3) understand the sequencing of these actions together. This scenario also features realistic lateral movement that might be found in a real deployment.\n\nSimple Privilege Escalation (Light). This scenario requires the agent to escalate privileges in a small network. It primarily escalates around a nontrivial privilege escalation vector (Path Interception), but the agent must also do this in conjunction with some other basic capabilities (e.g., valid accounts and discovery). This scenario showcases the ability of the agent to find misconfigurations on the fly that would be possible to encounter in the real world.\n\nBasic C2 (Light). This is a new scenario where the agent must figure out how to establish command-and-control with an isolated VM. It requires little explicit chaining of actions; but a nontrivial amount of work to setup the C2 channel. This scenario takes advantage of how well the agent would be able to exploit then pivot through a hardened environment.\n\nAzure SSRF (Light). This is a new scenario where the agent must take advantage of a poorly coded server, expose valid Azure credentials, and use those credentials to modify a network rule to allow to access a target host. The critical vulnerabilities in this scenario are discovering. The task does not require much reasoning or planning.\n\n taint Shared Content (Light). This is a new scenario where the agent must work in a mostly linear fashion to chain together a sequence of weaknesses. It does not require any deep expertise in any one area, instead requiring the agent to perform lab-style discovery, SQL injection, tainting shared content, and privilege escalation. While the chain to these actions is linear, it still requires the agent to show its ability to autonomously string them together.\n\nWith Solver Code, where we provide the model with partial solver code it can use to solve the scenario.\n\nWe run 30 trials for each eval. Our primary metric for evaluation is pass@12 on the eval without hints provided. Since current models all score 0% on these evals, for context we also report pass@1",
    "26": "These configurations help us assess not just whether the model can complete the task, but also how much external assistance it requires.\n\nCyber Range\n\n[Image of a bar graph showing pass rates for different cyber range scenarios. The x-axis labels are: Cyber Range, Light (pass@12), Medium (pass@12), and Medium (with Hints). The y-axis ranges from 0% to 100%. The pass rates are very low, mostly below 20%, except for Medium with Hints which is around 40%.]\n\nFigure 12\n\nNo model is able to solve any scenario unaided or with hints.\n\n5.2.3 AI Self-Improvement\n\nThe gpt-oss models do not demonstrate improved performance on software engineering and AI research tasks relevant to AI self-improvement risks. OpenAI o3 and o4-mini are still the highest performing models, the error rate does not significantly impact results).\n\nTable 13: Overview of AI Self-Improvement evaluations\n\n| Capability | Description |\n|---|---| \n| SWE-bench Verified | Real-world software engineering tasks. Can models resolve GitHub issues, given just a code repository and issue description? |\n| OpenAI PRs | Real world ML research tasks. Can models replicate pull requests? |\n| PaperBench | Real world ML paper replication. Can models replicate state-of-the-art AI research papers from scratch? |\n\n5.2.3.1 SWE-bench Verified\n\nSWE-bench Verified [27] is the human-validated subset of SWE-bench that more reliably evaluates AI models’ ability to solve real-world software issues. This validated set of tasks fixes evaluates issues with SWE-bench such as incorrect testing of correct solutions, under-specified problem statements, and overly specific unit tests. This helps ensure we’re accurately grading model capabilities. A sample task flow is shown below:\n\n[Image showing a workflow with Issue, Language Model, Unit Tests, and PR stages. An example issue is 'data leak in GBDT due to non-histogram-based version of...'. The Language Model generates code, which is then tested by Unit Tests. The PR is then evaluated.]\n\nFor OpenAI o3 and o4-mini, we used an internal tool scaffold designed for efficient iterative file editing and debugging. In this setting, we average over 4 tries per instance to compute pass@1. (Unlike Agentless, the error rate does not significantly impact results).\n\nAll SWE-bench evaluation runs use a fixed subset of n=477 verified tasks which have been validated on our internal infrastructure. Our primary metric is pass@1, because of the information provided to the correct. Like a real software engineer, the model must implement its change without knowing the correct tests ahead of time.",
    "27": "[Image of a bar graph showing pass rates for SWE-bench Verified with different OpenAI models. The x-axis labels are: sklear, gradient_boosting, helper, matrix_transform, euclidean_diff. The y-axis ranges from 0% to 100%. The pass rates are generally low, around 60% or lower.]\n\nFigure 14\n\nAll models performed similarly on this evaluation, with OpenAI o4-mini just one percentage point higher than OpenAI o3.\n\n5.2.3.2 OpenAI PRs\n\nMeasuring if and when models can automate the job of an OpenAI research engineer is a key goal of self-improvement evaluation work. We test models on their ability to replicate pull request contributions by OpenAI employees, which measures our progress towards this capability.\n\nWe source tasks directly from internal OpenAI pull requests. A single evaluation sample is based on an agentic rollout. In each rollout:\n\n1. An agent’s code environment is checked out to a pre-PR branch of an OpenAI repository and human-written, using command-line tools and Python, modifies files within the codebase.\n2. The modifications are graded by a hidden unit test upon completion.\n\nIf all task-specific tests pass, the rollout is considered a success. The prompts, unit tests, and OpenAI PRs (no browsing)",
    "28": "[Image of a bar graph showing pass rates for OpenAI PRs with different models. The x-axis labels are: o3, o4-mini, gpt-oss 20b, gpt-oss 120b. The y-axis ranges from 0% to 100%. The pass rates are low, around 30-40% for o3 and o4-mini, and lower for the gpt-oss models.]\n\nFigure 15\n\nThe gpt-oss models score only two percentage points lower than OpenAI o4-mini.\n\n5.2.3.3 PaperBench\n\nPaperBench [35] evaluates the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks.\n\nWe measure a 10-paper subset of the original PaperBench splits, where each paper requires <10GB of external data files. We report pass@1 performance with high reasoning effort and no browsing.\n\n[Image of a bar graph showing pass rates for PaperBench with different models. The x-axis labels are: o3, o4-mini, gpt-oss 20b, gpt-oss 120b. The y-axis ranges from 0% to 100%. The pass rates are low, around 20-25% for o3 and o4-mini, and lower for the gpt-oss models.]\n\nFigure 16",
    "29": "Appendix 1\n\n[start]system[message]You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2023-04-26\nCurrent date: 2023-06-28\nreasoning: low\n\nValid channels: analysis, commentary, final. Channel must be included for every message.\nCalls to these tools must go to the commentary channel: 'functions'.[end]\n[start]developer[message]# Instructions\nUse a friendly tone.\n\n# Tools\n\nnamespace functions {\n  // Gets the current weather in the provided location.\n  type get_current_weather = {\n    location: string;\n    format: \"celsius\" | \"fahrenheit\"; // default: celsius\n  };\n} // namespace functions[end]\n[start]user[message]What is the weather like in San Francisco, CA?\n[end]",
    "30": "7 Appendix\nThis section describes the recommendations we received on our adversarial testing methodology, and how we responded.\n7.0.1 Recommendations Implemented\n1. Clarifying Threat Model and Risk Categorization\n• Defined low-resource actor assumptions: Added clarifying language to our paper on compute, ML expertise, and data access assumptions for low-resource actors, with future cost estimates flagged for follow-up.\n• Preparedness criteria & ProtocolQA requirement: We clarified the preparedness criteria and explicitly retained ProtocolQA as a required component of the assessment. We edited the paper text accordingly and re-ran OpenAI’s for ProtocolQA with a blocklist to ensure consistency.\n2. Strengthening Evaluation Completeness and Reliability\n• Robustness checks on ProtocolQA: We validated our protocol troubleshooting results by checking that the model never refused, adding more protocol-debugging training data, and adding a new protocol-troubleshooting eval similar to ProtocolQA but uncontaminated.\n• Inference-scale timing plots: Added plots for both bio and cyber evals showing how performance scales with number of trials.\n• Multimodal benchmark alignment: Ran text-only versions of Multimodal Virology Troubleshooting and updated results to improve comparability. We also conducted VCT on the final 322-question database and reported human baseline calculations.\n• Expert baseline clarity: Specified expert profiles and baseline failures in reporting.\n• Quantified refusal behavior: Explicitly separated refusal-based failures from other failure modes and reported pre- and post-naughtification rates.\n3. Improving Evaluation Setup\n• Enhanced agent scaffolding: Tested internal “Best of K” scaffolding in cyber evaluations.\n• Aligned RL datasets with ProtocolQA: Tested analogous datasets during RL training to confirm no harmful uplift; findings added to paper.\n• Fine-tuning performance verification: Aligned with internal researchers on best hyperparameter settings for maximum performance and changed when necessary.",
    "31": "• Higher-quality agent scaffolding for measurements\n(a) Recommendation: Apply best-of-N scaffolding broadly to all evaluations.\n(b) Decision: Scaffolding experiments were partially conducted elsewhere, with limited expected additional gains from full runs.\n2. Omit ProtocolQA from preparedness thresholds\n(a) Recommendation: Compute comparison performance using closed models where non-refusal responses are substituted, treating refusals as zero.\n(b) Decision: Our past testing has found that closed models already did not refuse on benign-proxy tasks (except Griffin), so this wouldn’t give much signal on how well open models could “close the gaps” for closed models on real malicious tasks.\nReferences\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and L. Polosukhin, “Attention is all you need,” in *Proceedings of Advances in Neural Information Processing Systems*, 2017.\n[2] N. Shazeer, A. Miroshoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,” 2017.\n[3] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional computation and automatic sharing,” *arXiv preprint arXiv:2006.16668*, 2020.\n[4] N. Du, Y. Huang, A. Tai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. Y. Wu, O. Firat, et al., “Glum: Efficient scaling of language models with mixture-of-experts,” in *International Conference on Machine Learning*, pp. 2091–2102, 2022.",
    "32": "[5] J. Cao, Y. Wang, Y. Li, T. Zhang, X. Li, Y. Wang, and Z. Wang, “Sparrow: Reducing harms from large language models,” *arXiv preprint arXiv:2305.14318*, 2023.\n[6] R. Bommasani, A. Card, S. Saha, R. Kumar, A. Singhal, A. Agarwal, S. Agarwal, S. Bansal, S. Basu, A. D’Angelo, et al., “On the opportunities and risks of foundation models,” *arXiv preprint arXiv:2112.05865*, 2021.\n[7] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T.-Y. Liu, “On layer normalization in the transformer architecture,” *arXiv preprint arXiv:2009.02009*, 2020.\n[8] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., “Language models are unsupervised multitask learners,” *OpenAI blog*, 2019.\n[9] N. Shazeer, “GLU variants improve transformer,” *arXiv preprint arXiv:2002.05202*, 2020.\n[10] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long sequences with sparse transformers,” *arXiv preprint arXiv:1904.10509*, 2019.\n[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,” *NeurIPS*, 2020.\n[12] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai, “GQA: Training generalized multi-query transformer models from multi-head checkpoints,” 2023.\n[13] N. Shazeer, “Fast transformer decoding: One write-head is all you need,” *arXiv preprint arXiv:1911.09150*, 2019.",
    "33": "[14] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, “Reformer: Enhanced transformer with rotary position embedding,” *Neurocomputing*, 2024.\n[15] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, “YaRN: Efficient context window extension of large language models,” *arXiv preprint arXiv:2309.00071*, 2023.\n[16] E. Miller, “Attention is off by one (2023),” URL https://evanmiller.org/attention-is-off-by-one.html.\n[17] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, “Efficient streaming language models with attention sinks,” *arXiv preprint arXiv:2309.14239*, 2023.\n[18] H. Fan, B. Peng, J. Quesnelle, E. Shippole, A. Bhowmik, R. Carreira, A. Pratik, A. Z. Li, W. Shen, and N. Elgohary, “FlashAttention: Fast and memory-efficient exact attention with IO-awareness,” *arXiv preprint arXiv:2205.14135*, 2022.\n[19] T. Hoffmann, S. Borgeaud, E. Buchatskaya, T. Cai, E. Rutherford, D. U. Hwang, R. K. M. Patel, P. S. Clark, S. Hadfield-Menell, A. McDowell, et al., “On the capabilities of learned evaluative feedback,” *arXiv preprint arXiv:2209.08953*, 2022.\n[20] A. Gopi, S. Subramanian, A. Gupta, A. Vaswani, and A. Ramdas, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” *arXiv preprint arXiv:2108.08593*, 2021.\n[21] S. Kamalu, A. Gopi, A. Vaswani, and A. Ramdas, “Galactica: A large language model for science,” *arXiv preprint arXiv:2205.09269*, 2022.\n[22] R. Lowe, Y. Pineau, I. Serban, S. Menick, A. Gopi, A. Vaswani, and A. Ramdas, “Training language models to follow instructions with human feedback,” *arXiv preprint arXiv:2203.02155*, 2022.\n[23] L. Ouyang, J. Wu, J. Ziegler, N. Narayanan, J. Ray, P. Mourad, S. Mu, D. Novak, R. Tang, B. Lou, et al., “Training language models to follow instructions with human feedback,” *Advances in Neural Information Processing Systems*, 2022."
  },
  "skipped_pages": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    20,
    21,
    22,
    23,
    24
  ]
}