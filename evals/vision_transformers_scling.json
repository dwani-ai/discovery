{
  "response": "## Key Points from the Text: \"Scaling Vision Transformers to 22 Billion Parameters\"\n\nHere's a breakdown of the key points from the provided text, categorized for clarity:\n\n**1. Core Achievement: Scaling ViT to 22 Billion Parameters**\n\n*   The paper presents **ViT-22B**, a significantly larger Vision Transformer (22 billion parameters) than previously achieved.\n*   They demonstrate a **recipe for stable and efficient training** of this large model.\n\n**2. Performance Gains & Benefits of Scale**\n\n*   **Improved Accuracy:** Scaling up the model consistently leads to performance gains on various downstream tasks.\n*   **Fairness:** Larger models show improved fairness, reducing disparities in performance across subgroups.\n*   **Human Alignment:** ViT-22B aligns better with human visual perception (shape/texture bias).\n*   **Robustness:**  Increased scale leads to improved robustness.\n*   **Calibration:** ViT-22B improves calibration (better confidence estimates).\n\n**3. Technical Details & Methods**\n\n*   **Asynchronous All-Reduce:**  Utilized for efficient communication during training.\n*   **Communication Overlap:**  Overlapping communication and computation to maximize hardware utilization.\n*   **Parameter Sharding:** Sharing model parameters across devices to fit larger models.\n*   **QK-Normalization:** Used to stabilize training at larger scales.\n*   **Training Data:** Trained on a large dataset (1.8 billion images ‚Äì JFT-4B).\n\n**4. Transfer Learning Results**\n\n*   **Image Classification:** Strong performance on ImageNet and related datasets.\n*   **Dense Prediction:**  Effective as a backbone for semantic segmentation (ADE20K, Pascal VOC, Mapillary Vistas) and monocular depth estimation.\n*   **Video Classification:**  Outperforms previous vision backbones on Kinetics-400 and Moments in Time.\n*   **Zero-Shot Transfer:**  Improved zero-shot performance with Locked-image Tuning (LiT).\n*   **Head2Toe Transfer:** Demonstrated improvements with this transfer learning technique.\n\n**5. Analysis & Insights**\n\n*   **Model Calibration & Reliability:**  ViT-22B shows improvements in calibration and reliability.\n*   **Feature Attribution:** Analysis of feature attribution reveals how the model processes images.\n*   **Perceptual Similarity:** ViT-22B learns more human-like representations of images.\n\n**6. Model Card & Responsible AI**\n\n*   The paper includes a model card outlining usage, limitations, and ethical considerations.\n*   Emphasis on the need for careful evaluation and mitigation of risks when deploying the model in real-world applications.\n\n\n\n**7. Experimental Setup & Comparisons**\n\n*   Experiments conducted on NVIDIA A100 GPUs with NVLink.\n*   Comparisons to other state-of-the-art models (ViT-L, ViT-G, CLIP, ALIGN, etc.).\n\n\n\n**In essence, the paper demonstrates that scaling Vision Transformers to a very large size (22B parameters) yields significant benefits across a wide range of vision tasks, but also requires careful engineering and consideration of ethical implications.**",
  "extracted_text": {
    "0": "Scaling Vision Transformers to 22 Billion Parameters\nMostafa Dehghani* Josip Djolonga* Basil Mustafa* Piotr Padlewski* Jonathan Heek*\nJustin Gilmer Andreas Steiner Matthias Minder Joan Puigcerver Utku Evci Manoj Kumar\nCarlos Riquelme Matthias T√§ndler Frank Bastings Arvindh Mahendran Fisher Yu\nSjoerd van Steenkiste Gamaleldin F. Elsayed Thomas Collier Alexey A. Gritsenko\nVignesh Birodkar Cristina Vasconcelos Yi Tay Thomas Mensink Alexander Kolesnikov\nFilip Pavetiƒá Dustin Tran Jeremiah Harmsen Neil Houlsby\nGoogle Research\n\n\nAbstract\nThe scaling of Transformers for language models has driven breakthrough capabilities for language models. At present, the\nlargest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have\nintroduced the same architecture to image and video modelling, but these have not yet been successfully scaled to\nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We\npresent a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a\nwide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a\nlightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale.\nWe\nfurther observe other interesting benefits of scale, including an improved tradeoff between fairness and\nperformance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and\nimproved robustness getting there.\n\n1 Introduction\nSimilar to natural language processing, transfer of pre-trained vision backbones has improved performance on\na wide variety of vision tasks (Pan and Yang, 2010; Zhou et al., 2016; Kolesnikov et al., 2020). Larger\ndatasets, scalable architectures, and new training methods (Mahajan et al., 2018; Dosovitskiy et al., 2021;\nHe et al., 2022; Zhuang et al., 2021) have enabled the training of vision models with hundreds of millions or\neven billions of parameters. Specifically, the Vision Transformer (ViT) architecture (Dosovitskiy et al., 2021)\nhas become one of the dominant paradigms in computer vision, achieving state-of-the-art results on a wide\nrange of tasks. LLMs such as PaLM (Chowdhery et al., 2022) and LLaMA (Touvron et al., 2023) with 50B to\n65B+ parameters have demonstrated emergent capabilities. Scaling ViTs to comparable sizes, however, has\nproven challenging. The largest dense ViT model, published by Chen et al. (2022), contains 4B parameters.\nScaling is not merely a matter of increasing model size; it requires careful consideration of architectural\nchoices, training stability, and computational efficiency. In this work, we present a recipe for training a\n22B-parameter ViT (ViT-22B) and demonstrate its effectiveness across a wide range of vision tasks.\nOur main contributions are:\n‚Ä¢ We demonstrate that scaling ViTs to 22B parameters leads to consistent performance gains on\ndownstream tasks.\n‚Ä¢ We observe improved tradeoffs between fairness and performance with scale.\n‚Ä¢ We find that ViT-22B aligns better with human visual perception in terms of shape/texture bias.\n‚Ä¢ We show that ViT-22B exhibits improved robustness.\n‚Ä¢ We release the pre-trained weights of ViT-22B to the research community.\n\n\n\narXiv:2302.05442v1 [cs.CV] 10 Feb 2023",
    "4": "ideally to have them overlap (Wang et al., 2022a) so that we can keep the matrix multiply unit, where most\nof the FLOP capacity is, busy at all times.\n\nTo illustrate the process, consider the problem of computing ùí¥ = ùë®ùíô under the constraint that the i-th block of\nùíô and ùíö both reside on the i-th device. We denote the blocks of ùë® ‚àà ‚Ñù·µêÀ£‚Åø by ùë®·µ¢,‚±º ‚àà ‚Ñù·µèÀ£·µè, and analogously\nùíô·µ¢ ‚àà ‚Ñù·µè and ùíö·µ¢ ‚àà ‚Ñù·µè, with i, j ‚àà {1, ..., k}. The first option is to have device i hold the i-th block of rows,\nnecessary for computation of ùíö·µ¢, so that to compute ùíö·µ¢, the chip needs to communicate k ‚Äì 1 times to complete\nùíô·µ¢, a total of (k ‚Äì 1)(k) floats. Alternatively, device i can hold the i-th block of columns, all acting on ùíô·µ¢. This\nway, the device computes the vectors ùíö·µ¢,‚±º = ùë®·µ¢,‚±ºùíô·µ¢, which have to be communicated (scatter-reduced) with the\nother devices. Note that here the communicated vectors belong to the output space, a total of (k ‚Äì 1)(m/k)\nfloats. This asymmetry is leveraged in communication costs when n ‚â† m; column-sharding is used in the\ncomputation of the output of the MLP in a Transformer, where n = 4m, and row-sharding elsewhere.\n\nFurthermore, matrix multiplications are overlapped with the communication with the neighbours. This asynchronous approach allows for high matrix core utilization and increased device efficiency, while minimizing\nwaiting on incoming communication. Figure 3 presents the overlapping communication and computation\nacross devices with the parallel linear operation in row-sharding and column-sharding modes. The general\ncase of this technique is presented in Wang et al. (2022a), who also introduce the XLA operations we leverage\nhere.\n\nParameter sharing. The model is data-parallel on the first axis. Each parameter can be either fully replicated\nover this axis, or have each device hold a chunk of it. We opted to shard some large tensors from the\nmodel parameters to be able to fit larger models and batch sizes. This means that the device would have to\ngather the parameters before computing of the forward and scatter on the backward pass, but again, note\nthat this happens asynchronously with computation. In particular, while computing one layer the device can\nstart communicating the weights of the next one, thus minimizing the communication overhead.\n\nUsing these techniques, ViT-2B achieves 1.15k tokens per second for the backward pass (forward and\nbackward pass latency 2.3ms). Figure 4(a), 4(b) visualize an ablation study for ViT-2B with and without\ncommunication overlap. Figure 4(c) and 4(d) present a comparison against JAX/Mu (Frobenius et al.,\n2022) and PyTorch FSDP (Lialin et al., 2022). The vertical axis is total latency, and the horizontal axis is\nmodel size.\n\n3. Experiments\n\n3.1. Experimental Setup\n\nWe evaluate our system on 8 NVIDIA A100 80GB GPUs with NVLink interconnects. We use the Megatron-\nLM (Shoeybi et al., 2019) implementation with the following modifications: we replace the standard\nall-reduce with our asynchronous all-reduce, and we implement the overlapping communication and\ncomputation as described in Section 2.3. We use the default hyperparameters from the Megatron-LM\nrepository, unless otherwise specified. We evaluate on the following models: GPT-3 175B (Brown et al.,\n2020), ViT-2B (Dosovitskiy et al., 2021), and Chinchilla-70B (Hoffmann et al., 2022). We use batch size\n1 for GPT-3 and Chinchilla, and batch size 8 for ViT-2B. We report the average throughput over 1000\niterations. We measure the end-to-end latency using the NVIDIA Nsight Systems profiler.\n\nWe compare our system with the following baselines: JAX/Mu (Frobenius et al., 2022) and PyTorch FSDP\n(Lialin et al., 2022). For JAX/Mu, we use the official implementation with the default hyperparameters. For\nPyTorch FSDP, we use the latest version of PyTorch with the default hyperparameters. We run all experiments\non the same hardware and with the same batch size.",
    "5": "4.2 Transfer to image classification\n384 √ó 384\nEfficient transfer learning with large scale backbones is often achieved by using them as frozen feature\nextractors. This section presents the evaluation results of ViT-22B for image classification using linear probing\nand locked-image tuning as well as out-of-distribution transfer. Additional results for Head2Toe transfer,\nfew-shot transfer, and linear probing with L-BFGS can be found in Appendix D.1.\n\n4.2.1 Linear probing\nEfficient various ways of training a linear probe, our final setup on ImageNet uses SGD with momentum\nfor 10 epochs at 224px resolution, with mild random cropping and horizontal flipping as the only data\naugmentations, and no further regularizations.\n\nThe results presented in Table 2 show that while the returns are diminishing, there is still a notable improve-\nment at this scale¬π. Furthermore, we show that linear probing of larger models like ViT-22B can approach or\nexceed performance of full fine-tuning of smaller models with high-resolution, which can be often cheaper\nor easier to do.\n\nTable 2: Linear evaluation on ImageNet-1k (Deng et al., 2009) with varying scale. All models pre-trained on\nlarge datasets. Performances of a few high-resolution fine-tuned models from are provided for reference.\n\n| Model | IN | Real. | INv2 | ObjectNet | IN-R | IN-A |\n|---|---|---|---|---|---|---|\n| 224px linear probe (frozen) | | | | | | |\n| B/32 | 80.18 | 86.00 | 69.56 | 46.03 | 75.03 | 31.2 |\n| B/16 | 84.20 | 88.79 | 75.07 | 56.01 | 82.50 | 52.67 |\n| ALIGN (360px) | 85.5 | ‚Äì | ‚Äì | ‚Äì | ‚Äì | ‚Äì |\n| BASIC | 85.7 | 80.6 | 78.57 | 63.84 | 89.92 | 67.96 |\n| g/14 | 88.61 | 90.05 | 81.10 | 68.84 | 91.23 | 72.51 |\n| g/16 | 88.85 | 90.50 | 81.32 | 69.55 | 91.74 | 77.33 |\n| C/14 | 89.26 | 90.76 | 82.51 | 71.54 | 94.33 | 81.56 |\n| e/14 | 89.51 | 90.94 | 83.15 | 74.30 | 94.27 | 83.80 |\n| High-res fine-tuning | | | | | | |\n| ALIGN-XL | 89.4 | 92.4 | 83.8 | ‚Äì | ‚Äì | ‚Äì |\n| MaxiPixel-L | 89.8 | 92.8 | 83.3 | ‚Äì | ‚Äì | ‚Äì |\n| TiTa-XL | 90.5 | 93.0 | 84.0 | ‚Äì | ‚Äì | ‚Äì |\n| VoIT-22B | 93.8 | 94.9 | 87.2 | 79.0 | 96.1 | 89.2 |\n\n¬πNotably, the performance of ViT-e/14 with 384px input resolution is 89.7% (Touvron et al., 2021), which is close to the\nperformance of ViT-B/16 (84.2%) with 224px input resolution. This suggests that the input resolution is an important factor\nin the performance of ViT models. We also observe that the performance of ViT-22B is significantly higher than that of\nother ViT models, which is consistent with the fact that it is a larger model.\n\n",
    "6": "Figure 4: Linear probing on ImageNet 2017 with different input resolutions. ViT-22B leads to significant\naccuracy improvement especially when the input size is small.\n\nTable 3: Zero-shot transfer results on ImageNet (variants).\n\n| Model | IN | IN-v2 | IN-R | IN-A | ObjNet | Real. |\n|---|---|---|---|---|---|---|\n| CLIP | 76.2 | 70.1 | 77.2 | 72.3 | ‚Äì | ‚Äì |\n| ALIGN | 76.4 | 70.1 | 79.2 | 75.8 | 72.2 | ‚Äì |\n| CoCa | 86.3 | 80.7 | 86.5 | 85.6 | 82.7 | ‚Äì |\n| Lit-g/14 | 85.2 | 79.8 | 94.9 | 81.8 | 82.5 | 88.6 |\n| Lit-e/14 | 85.4 | 80.6 | 96.0 | 88.0 | 84.9 | 88.4 |\n| Lit-22B | 85.9 | 80.9 | 96.0 | 90.1 | 87.6 | 88.6 |\n\n4.2.2 Zero-shot via locked-image tuning\n\nExperimental setup. Following the Locked-image Tuning (LiT) (Zhai et al., 2022b) protocol, we train a text\ntower contrastively to match the embeddings produced by the frozen ViT-22B model. With this text tower,\nwe can easily perform zero-shot classification and zero-shot retrieval tasks. We train a text Transformer with\nthe same size as (Zhai et al., 2022a) on the English subset of the WebLI dataset (Chen et al., 2022) for\n1M steps with a 32K batch size. The optimizer is AdamW with a peak learning rate of 5e-5 and a linear\nwarmup schedule. We evaluate the performance of the trained model on ImageNet.\n\nResults. Table 3 presents the zero-shot transfer results on ImageNet (variants). We observe that LiT\ncan significantly improve the zero-shot performance of ViT-22B. For example, the accuracy on ImageNet\nincreases from 76.2% to 85.9% when using LiT. The improvement is more pronounced for out-of-distribution\ndatasets, which suggests that LiT can help to reduce the domain gap between the pre-training data and the\ntest data. Furthermore, we observe that the performance of LiT-22B is comparable to that of other state-\nof-the-art zero-shot models, such as CLIP and ALIGN.\n\n",
    "7": "Figure 5: OOD classification performance. Axes are log-scaled as proposed in (Taori et al., 2020). ViT-B\nand ViT-L are trained on models fine-tuned on ImageNet.\n\nscaling the model increases in line with the improvement in transfer learning on ImageNet. This holds true for models that have only seen JFT images, and for models fine-tuned on ImageNet. In both\ncases, ViT-22B continues the trend of better OOD performance with larger models (Figure 5). While\nfine-tuning boosts accuracy on both ImageNet and out-of-distribution datasets, the effective robustness (An-\ndreassen et al., 2021) decreases (Figure 5). Even though ImageNet accuracy saturates, we see a significant\nincrease on ObjectNet from ViT-e/14 to ViT-22B.\n\n4.3 Transfer to dense prediction\n\nTransfer learning is critical especially since obtaining pixel-level labels can be costly.\n\n4.3.1 Semantic segmentation\n\nExperimental setup. We evaluate ViT-22B as a backbone in semantic segmentation on three benchmarks:\nADE20K (Zhou et al., 2017b), Pascal VOC (Everingham et al., 2010), and Mapillary Vistas (Lam et al.,\n2019). We analyze the performance in two scenarios: first, using a linear decoder, and second, fine-tuning\nthe entire network. The linear decoder is a simple fully connected layer that maps the ViT-22B features to\nsemantic segmentation maps. For fine-tuning, we freeze the ViT-22B backbone for the first few epochs and\nthen unfreeze it for the remaining epochs. We use a polyak learning rate schedule with an initial learning\nrate of 1e-4 and a momentum of 0.9. We train the network for 30 epochs with a batch size of 16.\n\nResults. We compare ViT-22B to the ViT-L (Touvron et al., 2022) and ViT-G of Zhai et al. (2022a),\nusing only the linear decoder and end-to-end fine-tuning. From Table 4, we observe that our ViT-22B backbone transfers better when\nseeing only few segmentation tasks is available. For example, when fine-tuning with only 1200 images (1/16 of\nADE20K training data), we reach a performance of 44.7 mIoU over DeiT-III Large (Touvron et al., 2022) and ViT-G (Zhai et al., 2022a).\nWhen transferring with more data, the performance of ViT-G and ViT-22B converge.\n\n",
    "8": "Table 4: Fewshot semantic segmentation on ADE20K, when only a fraction of the training set is used.\n\n| Fraction of ADE20K train data | 1/16 | 1/8 | 1/4 | 1/2 | 1 |\n|---|---|---|---|---|---|\n| ViT-L (Touvron et al., 2022) | 36.1 | 41.3 | 45.6 | 48.4 | 51.9 |\n| ViT-G (Zhai et al., 2022a) | 42.4 | 47.0 | 50.2 | 52.4 | 55.6 |\n| ViT-22B (Ours) | 44.7 | 47.2 | 50.6 | 52.5 | 54.9 |\n\n[Image of semantic segmentation results]\n\n(a) Semantic segmentation (b) Depth estimation\n\nResults. We compare ViT-22B to the ViT-L of DeiT-III and ViT-G of Zhai et al. (2022a), and use the linear decoder\nand end-to-end fine-tuning. From Table 4, we observe that our ViT-22B backbone transfers better when seeing only few\nsegmentation tasks is available. For example, when fine-tuning with only 1200 images (1/16 of ADE20K training data),\nwe reach a performance of 44.7 mIoU over DeiT-III Large (Touvron et al., 2022) and ViT-G (Zhai et al., 2022a). When\ntransferring with more data, the performance of ViT-G and ViT-22B converge.\n\n4.3.2 Monocular depth estimation\n\nWe largely mirror the set-up explored in Ranftl et al. (2021) and train their Dense\nPrediction Transformer (DPT) on top of frozen ViT-22B features. We use only a feature obtained from the\nWaymo Open real-world driving dataset (Sun et al., 2020). Here we explore features obtained from the Waymo\nOpen dataset. We also explore a simple feature map of (Lazaroiu et al., 2021) as a baseline. We use a standard\nloss function (L1 loss) and optimize using the AdamW optimizer. We report the Mean Squared Error (MSE) as the\nprimary metric, as well as Absolute Relative Difference (<1.25¬≤) for evaluation. We use the DPT decoder\nand linear decoding suggests that while enough geometric information is retained in the ViT features, only some of it is available for a\ntrivial readout.\n\n",
    "9": "Table 5: Monocular depth estimation from frozen ViT features using different decoders on the Waymo Open\ndataset.\n\n| Model | Œ¥‚Üë | AbsRel‚Üì | <1.1 | <1.25 | <1.25¬≤ |\n|---|---|---|---|---|---|\n| DPT | 0.024 | 0.121 | 0.594 | 0.871 | 0.972 |\n| Linear | 0.021 | 0.095 | 0.702 | 0.909 | 0.979 |\n| ViT-L | 0.060 | 0.222 | 0.304 | 0.652 | 0.926 |\n| ViT-e | 0.053 | 0.204 | 0.332 | 0.687 | 0.938 |\n| ViT-22B | 0.039 | 0.166 | 0.412 | 0.779 | 0.960 |\n\nResults. Table 5 summarizes our main findings. From the top rows (DPT decoder), we observe that\nusing ViT-22B features yields the best performance (across all metrics) compared to different backbones. By\ncomparing the ViT-22B architecture transforms performance. Using the linear decoder, it can be observed that\nthat scaling the architecture results in less data) we find that these improvements also come from scaling\nthe pre-training data. These findings demonstrate that both the greater model size and the greater dataset\nsize contribute to the improved performance. Using the linear decoder, it can be observed again\n\n4.4 Transfer to video classification\n\nWe evaluate the quality of the representations learned by ViT-22B by adapting the\narchitecture of the ‚Äúfactorised encoder,‚Äù which encodes each frame of a video into a\nsequence of tokens and then uses a spatial transformer to encode the sequence into a\nfixed-size representation (Arnab et al., 2021). Our video model consists of an initial\nspatial transformer, which encodes each frame of a video into a sequence of tokens,\nand then uses a temporal transformer to encode the sequence into a fixed-size\nrepresentation. We train the video model on two benchmarks: Kinetics-400 (Kay et al.,\n2017) and Something-Something V2 (Goyal et al., 2017). We use a standard cross-\nentropy loss function and optimize using the AdamW optimizer. We report the top-1\naccuracy on the test set. We compare our results to those of other state-of-the-art video\nclassification models.\n\n",
    "10": "Table 6: Video classification results. We evaluate the ViT-22B representations by freezing the backbone, and training a small transformer to aggregate frozen, per-frame representations. ViT-22B outperforms the largest previous vision backbone, ViT-e (Chen et al., 2022) which contains 4 billion parameters and is also pretrained on JFT.\n\n Kinetics 400 Moments in Time\n\n | Frozen backbone | CoCa¬π | \n |---|---| \n | ViT-e | 88.0 | 47.4 |\n | ViT-22B | 86.5 | 43.6 |\n | Fully finetuned ViT-22B | 91.1 | 44.9 |\n ¬πNote that CoCa uses pre-spatial features and higher spatial reso-lution for both datasets. More details in Appendix F.\n\nevidence by the current state-of-the-art on Kinetics 400 (Wang et al., 2022b) and Moments in Time (Yu et al., 2022a) which leverage a combination of large-scale video pretraining and full end-to-end fine-tuning on the target dataset.\n\n4.5 Beyond accuracy on downstream tasks\n\nWhen studying the impact of scaling, there are important aspects to consider beyond downstream task performance. In this section, we probe ViT-22B‚Äôs fairness, alignment with human perception, robustness, reliability, and calibration. We find that favorable characteristics emerge when increasing model size. Additional analysis on perceptual similarity and feature attribution can be found in Appendix K and Appendix L.",
    "11": "4.5.1 Fairness\n\nMachine learning models are susceptible to unintended bias. For example, they can amplify spurious correlations in the training data (Hendricks et al., 2018; Caliskan et al., 2017; Zhao et al., 2017; Wang et al., 2020) and result in error disparities (Zhao et al., 2017; Buolamwini and Gebru, 2018; Deuschel et al., 2020). Here, we identify how scaling the model size can help mitigate such issues by evaluating the bias of ViT-22B and ViT-L/16, g, G (Zhai et al., 2022a; Chen et al., 2022) using demographic parity (DP) as a measure of fairness (Dwork et al., 2012; Zagate et al., 2017).\n\nExperimental Setup. We use CelebA (Liu et al., 2015) with binary gender as a sensitive attribute while the target is ‚Äúattractive‚Äù or ‚Äúsmiling‚Äù. We emphasize that such experiments are carried out only to verify mechanical calibration and to aggregate performance for the model. All results shown in Table 9 are reported with respect to the majority group. We report demographic parity (DP) as the difference in acceptance rates between groups. A DP score of 0 indicates perfect fairness, while positive (negative) values indicate that the model is more (less) likely to accept instances from the privileged (unprivileged) group. Details on the implementation can be found in Appendix G.",
    "12": "Figure 7: rerr: Accuracy and ACC for ViT variants after debiasing for each DP level. MIDDLE: Accuracy for each subgroup in CelebA prior to debiasing. BOTTOM: MAL-S and OC-AUC (Kivilcim et al., 2021).\n\nconsistent with earlier observations reported in the literature (Alabdulmohsin and Lucic, 2021). Second, all subgroups tend to benefit from the improvement in scale. Third, ViT-22B reduces disparities in performance across subgroups. \n\n4.5.2 Human Alignment\n\nHow well do the ViT-22B classification decisions align with human classification decisions? Using the model-vs-human toolbox (Geirhos et al., 2021), we evaluate three ViT-22B models fine-tuned on ImageNet with different resolution (224, 384, 560). Across all three metrics, ViT-22B is SOTA: ViT-22B outperforms all other ViT variants (ViT-L/16, g, G) for expected alignment (Figure 8). ViT-22B also favors the highest-ever recorded shape bias in vision models: while most models have a strong texture bias (more than 20% shape bias) ViT-22B‚Äôs texture bias is lower (close to 0%); humans are at 6%. ViT-22B models have the lowest error recorded at a human proxy task (Geirhos et al., 2021): ViT-22B (384) is 13% better than ViT-L/16 and ViT-22B (560) is 13% better than ViT-L/16 (Figure 8).",
    "13": "Figure 8: Shape bias: many vision models have a low shape / high texture bias as indicated by brackets over ML model names, whereas ViT-22B fine-tuned on ImageNet (red, green, blue trained on 4B images as indicated by brackets over ML model names, unless trained on ImageNet only) have the highest shape bias recorded in ML model to date, bringing them closer (Tran et al., 2022)*.\n\nTable 7: ViT-22B evaluated on some representative metrics from the Plex flexibility benchmark (Tran et al., 2022)*.\n\nDietterich, 2019), which we evaluate not only with the accuracy but also uncertainty metrics measuring the calibration (NLL, ECE) and the selective prediction (EI-Yaniv and Wiener, 2010) (OC-AUC, see Section 4.5.1), and (2) open-set recognition‚Äîalso known as OOD detection (Fort et al., 2021), which we evaluate via the AUROC and AUPRC, with Places365 as the OOD dataset (Hendrycks et al., 2019); for more details, see Appendix I.\n\n4.5.3 Calibration\n\nWe assess the calibration of ViT-22B (384) and ViT-L/16 (384) using Expected Calibration Error (ECE) and Negative Log-Likelihood (NLL). To do so, we evaluate on ImageNet with resolution 384 (Figure 9). ViT-22B reduces calibration error in both ECE and NLL (Figure 9).",
    "14": "Figure 9: ViT-22B (light-blue circle) improves the Pareto frontier of the accuracy vs. the calibration (ECE). Left/right panels are without/with temperature scaling, respectively.\n\nTable 8: Distillation results, finetuned at 384 resolution.\n\n(Dosovitskiy et al., 2021) (JFT ckpt.) 84.2\n(Zhai et al., 2022a) (JFT ckpt.) 86.5\n(Touvrron et al., 2022) (INet21k ckpt.) 88.6\n\nWe perform model distillation (Hinton et al., 2015) to compress the ViT-22B into smaller, more widely usable models. We distill ViT-22B into ViT-B/16 and ViT-L/16 by annotating 500 random augmentations and mixing transforms of each ImageNet image with ViT-22B. We then fine-tune the student model using the teacher predictive distribution. ViT-B/16 learns from ViT-22B with an accuracy of 87.1 (compared to 86.2 without teacher predictive distribution) and ViT-L/16 learns from ViT-22B with an accuracy of 88.5 (compared to 87.9 without teacher predictive distribution). We achieve state-of-the-art on distillation.\n\n5 Conclusion\n\nWe present ViT-22B, a virtually larger transformer for computer vision. We demonstrate that scaling up the model size yields substantial gains in accuracy, fairness, alignment, robustness, reliability, and calibration. The benefits of scale are evident across a wide range of downstream tasks and evaluation metrics. Our findings highlight the potential of scaling vision transformers for achieving SOTA performance.",
    "15": "Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on scientific computing, 16(5):1190‚Äì1208, 1995.\n\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 2017.\n\nXi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Abbasi, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhoseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Housby, and Radu Soricut. PaLI: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.\n\n\n\nGong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE/ICSP, 105(1):1883‚Äì1893, 2017.\n\nAakansha Chowdhury, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nHyung Won Chung, Le Hou, Sharnya Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mustafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 3606‚Äì3613, 2014.\n\nMark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton, and Jesse Berent. Correlated input-dependent label noise in large-scale image classification. In CVPR, pages 1550‚Äì1559, 2021.\n\nJean-Baptiste Cordonnier, Adrien Lebastard, Loic Jollivet, and Gregory Guillemot. Real-time and accurate visual place recognition. In CVPR, pages 6334‚Äì6343, 2019.\n\n\n\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Innovations in Theoretical Computer Science, 2012.\n\nDavid Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NeurIPS, 2014.\n\nRan El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(5), 2010.\n\nUtku Evci, Vincent Dumoulin, Hugo Larochelle, and Michael C Mozer. Head2Toe: Utilizing intermediate representations for better transfer learning. In ICML, 2022.\n\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge. IJCV, 2010.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021.",
    "16": "Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. In NeurIPS, pages 7068‚Äì7081, 2021.\n\nAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231‚Äì1239, 2013.\n\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In ICLR, 2019.\n\nRobert Geirhos, Kanzhuoqi Narayana, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Partial success in the gap between human and machine vision. In NeurIPS, pages 2389‚Äì2399, 2021.\n\nJustin Gilmer, Andrew Schopola, and Ferruccio Hodjera. Neural Transformer. arXiv preprint arXiv:1703.03450, 2017.\n\nChuan Guo, Zhen Yuan, Xiaohua Zhou, and Jianwei Yang. Image recognition via relational inductive bias. In ICCV, 2017.\n\n\n\nGuillaume Jaume, Florent Hayou, Jean-Philippe Tarel, and Florent Perronnin. Beyond bag-of-words: Spatial pyramid matching for fine-grained image retrieval. In CVPR, 2014.\n\n\n\nKaggle and EyePACS. Kaggle diabetic retinopathy detection, 2015. URL https://www.kaggle.com/c/ diabetic-retinopathy-detection/data.\n\nJakob Nikolas Kather, Ole-Aaron Weiss, Francesco Bianconi, Susanne M Melchers, Lothar R Schad, Timo Gaisser, Alexander Marx, and Gerrit Z√∂llner. Multi-class texture analysis in colorectal cancer histology. Scientific reports, 6:27898, 2016.\n\nChao Jia, Yifei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904‚Äì4916, 2021.\n\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 2901‚Äì2910, 2017.",
    "17": "Fei-Fei Li, Marco Andreetto, Marc‚ÄôAurelio Ranzato, and Pietro Perona. Caltech 101, 2002. CaltechDATA, doi: 10.22002/D1.20086.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoyu Tang. Deep learning face attributes in the wild. In ICCV, 2015.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017.\n\nDhrub Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, pages 181‚Äì196, 2018.\n\nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset, 2018. URL https://github.com/deepmind/dsprites-dataset.\n\nMatthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubbs, Xiaohua Zhai, Neil Housby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. NeurIPS, 34:15682‚Äì15694, 2021.\n\nMatthew Monfort, Alex Andonian, Ramakrishnan Sarah Abel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfriend, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2), 2019.\n\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-gyu Cho, Seong-Whan Lee, Sadjad Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014.\n\nMahdi Pakdaman Naeini, Gregory Cooper, and Miles Huskrecht. Obtaining well calibrated probabilities using bayesian binning. In AAAI, 2015.\n\nYoval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in Feature Learning 2011. In NIPS, 2011.\n\n\n\nPauliina Lehtinen, Alexey Artemov, Matthias Kokkola, and Samuel Kaski. Noise contrastive estimation based feature learning. In AISTATS, 2016.\n\nRen√© Ranft, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In CVPR, pages 12179‚Äì12188, 2021.\n\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr√© Pinto, Daniel Keysers, and Neil Housby. Scaling vision with sparse mixture of experts. In NeurIPS, volume 34, pages 8583‚Äì8595, 2021.",
    "18": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasempour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022.\n\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In ICCV, 2021.\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML, pages 3319‚Äì3328, 2017.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n\nRohan Taori, Achal Dave, Vaishak Nikhil, Shankar Carini, Benjamin Recht, and Ludvig Schmidt. Measuring robustness to natural distribution shifts in image classification. In NeurIPS, 2020.\n\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Housby, and Donald Metzler. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n\n\n\nThomas Unterthiner, Alexander Krizhevsky, and Yoshua Bengio. Towards a generic understanding of the taskonomy benchmark. arXiv preprint arXiv:1806.06707, 2018.\n\nAndrea Vedaldi, Simone Ciompi, and Subhransu Maji. Beyond simple features: learning discriminative representations for visual tasks. In CVPR, 2015.\n\n\n\nVincent Vanhoucke, Aryeh Kontorovsky, and Geoffrey Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2016.\n\n\n\nXiaolong Wang, Zhe Gan, Yu Wu, Rui Zhao, Yingwei Pan, and Jingdong Chen. Meta-learning for few-shot image classification. In CVPR, 2018.\n\n\n\nZewei Wu, Feng Liu, Jianping Shi, and William Yang Wang. AutoAugment: Learning augmentation policies from data. arXiv preprint arXiv:1803.01534, 2018.",
    "19": "Bingxin Wu, Tianyu Gao, and Fei Wu. Visual prompt tuning. In CVPR, pages 13268‚Äì13277, 2022.\n\n\nXingjian Wu, Yifan Zhang, Yuxin Peng, Yifan Sun, and Jianfeng Feng. Block-wise contrastive learning for image retrieval. In CVPR, 2018.\n\n\n\nZhen Xu, Xiaohua Shen, and Jianfeng Feng. Weakly supervised fine-grained image classification with multi-label learning. In CVPR, 2017.\n\n\n\nJiaqi Yang, Yifan Zhang, Xiaohua Shen, and Jianfeng Feng. Learning to learn knowledge for visual question answering. In CVPR, 2019.\n\n\n\nJiaqi Yang, Yifan Zhang, Xiaohua Shen, and Jianfeng Feng. Self-supervised learning of visual features by contrastive predictive coding. In CVPR, 2020.\n\n\n\nZhiyuan Ye, Jianwei Yang, and Xin Zhao. Relation-aware graph attention network for visual reasoning. In CVPR, 2019.\n\n\n\nZheng Yuan, Jingyi Yu, Xiaohua Shen, and Jianfeng Feng. Multi-modal transformer for unaligned multi-modal learning. In CVPR, 2021.\n\n\n\nZheng Yuan, Xiaohua Shen, and Jianfeng Feng. Adversarial feature learning for fine-grained image classification. In CVPR, 2017.\n\n\n\nZheng Yuan, Xiaohua Shen, and Jianfeng Feng. Robust visual feature learning with online hard example mining. In CVPR, 2019.\n\n\n\nZheng Yuan, Xiaohua Shen, and Jianfeng Feng. Transferable adversarial training for robust visual recognition. In CVPR, 2020.\n\n\n\nZheng Yuan, Xiaohua Shen, and Jianfeng Feng. Learning to transfer: a unified framework for domain adaptation. In CVPR, 2021.\n\n\n\nXiaohua Zhai, Sergei Levine, and Jitendra Malik. Learning from demonstration with shared representations. In ICML, 2017.",
    "20": "Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu,\nZun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. In-\nternvideo: General video foundation models via generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022b.\n\nZeyu Wang, Klint Qinami, Ioannis Christos Karakoziis, Kyle Genova, Prem Nair, Kenji Hata, and Olga\nRussakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In CVPR,\n2020.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Berth Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint\narXiv:2206.07682, 2022.\n\nYeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient ensemble and\nlifelong learning. In ICLR, 2019.\n\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale\nscene recognition from abbey to zoo. In CVPR, pages 3485‚Äì3492, 2010.\n\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene\nunderstanding. In ECCV, 2018.\n\nYi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In\nSIGSPATIAL international conference on advances in geographic information systems, pages 270‚Äì279, 2010.\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. Transactions on Machine Learning Research, 2022a.\n\nJiahui Yu, Yuanzhong Xu, Jing Ku Kolei Zhang, Liang Guonban, Gujian Bai, Zirui Wang, Vijay Vasudevan, Alexander\nKu, Yinfeng Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image\ngeneration. Transactions on Machine Learning Research, 2022b.\n\nMuhammad Bilal Zafar, Isabella Learning Gomez, Rodrigo Kaze, and Yair Neiman. Towards fairness\nbeyond disparate impact: Learning representations that decompose sensitivity. In NeurIPS, 2017.\n\nJia Zou and Quoc V. Le. Demographic collapse: The globalization of styles in image generation. In\nInternational Conference on Learning Representations, 2021.\n\nZhengxuan Wu, Xinlong Wang, Pengfei Wang, Wenxuan Li, and Jianwei Yang. Visual information bottleneck\nfor image generation. In International Conference on Machine Learning, pages 1633‚Äì1642, 2021.\n\nYan Zhang, Rui Zhao, and Jiaya Jia. Relation-aware graph attention network for visual reasoning. In CVPR,\n2018.\n\nShuang Zhang, Xinlei Lei, and Xiaohui Xie. Towards robust visual question answering with knowledge\naugmentation. In CVPR, 2021.\n\nZhiyuan Zhao, Jingjing Liu, and Jianfeng Feng. Knowing what makes a good prompt for stable diffusion.\n2022.\n\nZheng Zhao, Haotian Liu, and Jianwei Yang. Semantic image synthesis via conditional generative adversarial\nnetworks. In CVPR, 2017.",
    "21": "Figure 10: Zero-shot classification examples of generated images. These images were provided by the\nParti (Yu et al., 2022b) and Imagen (Saharia et al., 2022) models. The training data for the ViT-22B vision\nbackbone and the LiT text backbone were created before these models were trained; therefore these images\nare not present in the training data. Further, the objects and scenes contained in these images are highly\nout-of-distribution relative to the distribution of natural images on the web.\n\n[Image of various generated images with classification labels and confidence scores]",
    "22": "Scalability\n\nWhen scaling up the default ViT architecture, we encountered training instability in ViT at Adam 1e-3.\nInitially, the loss would decrease as normal, but within 2000 steps the loss steadily increased. Figure 1 shows\nthe behavior of attention logits during training for an 8B parameter model. Without parameter normalization, attention\nlogits quickly grow to over 50000 in magnitude, resulting in one-hot attention weights after the softmax, and\nsubsequently unstable training losses and gradients.\n\nTo avoid instability, the learning rate of ViT was originally reduced with increasing model scale, from 1e-3\ndown to 4e-4 for ViT-H (Dosovitskiy et al., 2021). Comparing models trained similar to ViT-22B, similar to the normalization/reduced precision. For the latter, the learning rate is\nkept at 1e-3 and not reduced for larger models. With the QK-normalization, the higher 1e-3 learning rate\nremains stable. The results, shown in Figure 11, demonstrate increasing benefits with scale, likely due to\nenabling the larger learning rate.\n\n[Image of a graph showing ImageNet training accuracy vs. model size with and without query/key normalization]",
    "23": "Model Card\n\nTable 9 presents the model card (Mitchell et al., 2021) of the ViT-22B model.\n\nModel Summary\nModel Architecture\tDense encoder-only model with 22 billion parameters. Transformer model\n\tarchitecture with variants to speed up and stabilize the training. For details,\n\tsee Model Architecture (Section 2).\nInput(s)\tThe model takes images as input.\nOutput(s)\tThe model generates a class label as output during pretraining.\nUsage\nThe primary use is research on computer vision applications as a feature extrac-\ntor, that can be used in image recognition (finetuning, depth estimation, linear-probing,\nzero-shot, dense prediction) and so on. See Section 6.2 of ViT-22B is used in research to video\naction recognition and so on.\nKnown Caveats\tWhen using ViT-22B for any large scale model, it is difficult to under-\n\tstand how the model arrived at a specific decision, which could lead to lack of\ntrust and accountability.\n\tMoreover, we demonstrated that ViT-22B is less prone to unintentional bias\n\tand enhances current vision backbones by reducing spurious correlations.\n\tHowever, this was done through limited studies and particular benchmarks.\n\tBesides, there is always a risk of misuse in harmful or deceitful contexts when\nit comes to large scale machine learning models.\n\tViT-22B should not be used for downstream applications without a prior as-\nsessment and mitigation of the safety and fairness concerns specific to the\ndownstream application. We recommend spending enough time and energy\non mitigation the risk at the downstream application level.\n\nSystem Type\nSystem Description\tThis is a standalone model.\nUpstream Dependencies\tNone.\nDownstream Dependencies\tNone.\nImplementation Frameworks\nHardware & Software:\tTraining: TPU v4 (Jouppi et al., 2020). Software: Jax (Bradbury et al., 2018),\n\tFlax (Heek et al., 2020), Scenic (Dehghani et al., 2021).",
    "24": "Hardware & Software:\tDeployment: TPU v4 (Jouppi et al., 2020). Software: Scenic (Dehghani et al.,\n\t2021).\n\nCompute Requirements\tViT-22B was trained on 128M TPUs for 77K steps.\n\nModel Characteristics\nModel Initialization\tThe model is trained from a random initialization.\nModel Status\tViT-22B model has 22 billion parameters.\n\nData Overview\nTraining Dataset\tViT-22B is trained on a version of JFT (Sun et al., 2017), extended to contain\n\tapproximately 1.8B images (Zhai et al., 2022a). Section 4.1 for the datasets used to train ViT-22B.\nEvaluation Dataset\tWe evaluate the models on a wide variety of tasks and datasets. Specifically,\n\twe evaluate the models on ADE20K (Zhou et al., 2017b), Berkeley Adobe\n\tMDS (Deshpande et al., 2017), Caltech-101 (Li et al., 2005), Cars (Krause et al., 2013), Cifar-10\n\t(Krizhevsky et al., 2009), Cifar-100 (Krizhevsky et al., 2009), Cityscapes/distance (Cordts et al.,\n\t2016), CLEVR (Johnson et al., 2017), CLEVR-distance (Johnson et al., 2017), Colfist (Kather et al.,\n\t2016), DMLab (Beattie et al., 2016), Dsprites/location (Matthey et al., 2017), DTID (Cimpoi et al.,\n\t2014), EuroSAT (Bosch et al., 2019), Flowers102 (Nilsson et al., 2014), Food-5K (Felbo et al., 2014),\n\tImageNet (Deng et al., 2009), iNaturalist (Van Horn et al., 2018), ImageNet-R (Recht et al., 2021),\n\tImageNet-Sketch (Van et al., 2021), Kinetics 400 (Kay et al., 2017), Kinetics 600 (Kay et al.,\n\t2017), Moments in Time (Monfort et al., 2019), Pascal Context (Brodatz et al., 2018), Places\n\t(Zhou et al., 2017a), RefCOCO (Yu et al., 2016), ScanNet (Dai et al., 2017), SUN RGB-D (Khosla\n\tet al., 2012), Visual Genome (Krishna et al., 2017), Visual Commonsense (Zellers et al., 2019),\n\tVQA (Goyal et al., 2017), and YouTube-8M (Reiher et al., 2017).",
    "26": "SENSITIVE Use\nVIT-22B Model Usage & Limitations\nVIT-22B should not be used for any unacceptable vision model use cases. For example, for detecting demographic human features for unethical purposes, or as a feature extractor used to condition on and generate toxic content, or for captcha-breaking. We also do not approve use of VIT-22B in applications like surveillance, law enforcement, healthcare, or hiring and employment, and self-driving cars without putting measures in place to mitigate the ethical risks.\nVIT-22B is designed for research. The model has not been tested in settings outside of research that can affect performance, and it should not be used for downstream applications without further analysis on factors in the proposed downstream application.\nEthical Considerations\n& Risks\nIn order to train VIT-22B, we conducted an analysis of sensitive category associations on the JFT-4B dataset as described in Alka et al. (2021). This process involved measuring the per label distribution of sensitive categories across the raw data, cleaned data, and models trained on this data, as well as labels verified by human raters. To further enhance the data quality, human raters also assisted in removing offensive content from the dataset. Our analysis using standard fairness benchmarks shows that VIT-22B increases performance for all subgroups while minimizing disparities among them. However, it is important to note that there may be situations where utilizing VIT-22B could pose ethical concerns. Therefore, we recommend conducting custom ethical evaluations for any new applications of VIT-22B.\n\n27",
    "27": "D Transfer to image classification: More results and addition details\n\nAn alternative to doing linear probing with SCD is to use the convex optimization technique L-BFGS (Byrd et al., 1995). It is very effective and has strict convergence guarantees. We compare SGD and L-BFGS for a variety of ViT models using the ImageNet-1k dataset. Specifically, we precompute image embeddings by resizing input images to 224px resolution and then solve the multiclass logistic regression problem with L-BFGS. We also sweep the L2 regularization parameter and select the optimal one using 20000 holdout images from the training data (approximately 2% of the training data). In Table 10 we compare the resulting model with the SGD baseline from the main text. It demonstrates that L-BFGS matches or lags behind SGD approach, so we selected the latter technique for our core experiments.\n\nTable 10: Comparison of SGD and L-BFGS for linear probing on ImageNet-1k. The numbers indicate top-1 accuracy.\n\nLinear Probing B/32 B/16 L/16 g/14 G/14 e/14 22B\nSGD 79.94 84.06 86.46 88.46 88.79 89.08 89.27\nL-BFGS 80.18 84.20 86.66 88.51 88.98 89.26 89.51\n\nD.2 Out of distribution classification\n\nTable 11: OOD Classification. Results from models fine-tuned on ImageNet (top half), and models that were only trained on JFT and evaluated with a label-map (bottom half). Models with ‚Äúema‚Äù are fine-tuned with Polyak averaging, similar to (Dosovitskiy et al., 2021). B/16, L/16, g/14, and G/14 are from (Zhai et al., 2022a), and e/14 is from (Chen et al., 2022). IN‚Äô uses same resize without crop like in the original publication. See Figure 5 and Section 4.2.3 for discussion of the results, and details about datasets and pre-processing.\n\n\n\nModel Fine-tuned IN‚Äô IN INv1 INv2 ObjectNet IN-A IN-K\ne/14 ema 90.70 90.84 84.38 72.53 94.49 88.44\ne/14 89.56 89.61 83.64 71.86 94.35 88.16\ng/14 ema 92.43 92.46 88.11 80.41 96.04 93.65\ng/14 91.94 91.98 87.32 79.86 95.76 93.34\nL/16 ema 93.02 93.03 88.83 81.35 96.34 94.08\nL/16 92.51 92.52 88.01 80.76 95.98 93.72\nB/32 ema 93.61 93.62 89.55 82.08 96.62 94.56\nB/32 93.08 93.09 88.74 81.44 96.28 94.24\n\nD.3 Few-shot learning\n\nWe evaluate the few-shot learning performance of our models using the standard evaluation protocol from (Dosovitskiy et al., 2021). We report the mean accuracy over 5 runs with different random seeds. Results are shown in Table 12.",
    "28": "run, however it often performs worse than fine-tuning. Recent work showed that training a linear classifier on\ntop of the intermediate features can provide significant gains compared to using the last-layer only, especially\nfor target tasks that are significantly different from the original pre-training task (Evci et al., 2022; Adler et al.,\n2020; Khalifa et al., 2022).\n\nIn Table 12 we compare Head2Toe (Evci et al., 2022) with Linear probe on common vision benchmarks and\nVTAB-1k (Zhai et al., 2019). We include Finetuning results as a comparison point. We use a simplified version\nof Head2Toe with no feature selection. Experimental details are shared below. Head2Toe achieves 7% better\nresults on VTAB-1k, however fails to match the full finetuning performance (~6%). On other benchmarks\n(CIFARs, Flowers and Pets), all methods perform similarly potentially. Head2Toe improves over Linear only for\nthe Cifar-100 task. For the remaining tasks it either achieves the same performance or worse (Pets).\n\nAll experiments presented here use images with the default resolution of 224. Head2Toe uses the following\nintermediate features: (1) output of each of the 48 blocks, (2) features after the positional embedding,\n(3) features after the pooling head (4) pre-logits and logits. We average each of these features among the\nthree dimension and concatenate them, resulting in a 349088-dimensional feature vector. In contrast, linear\nprobe uses the 6144-dimensional pre-logits features, which makes Head2Toe training roughly 50 times more\nexpensive. However, given the extraordinary size of the original model, Head2Toe requires significantly\nless FLOPS and memory compared to fine-tuning. For all tasks (4 standard and 19 VTAB-1k), we search\nover 2 learning rates (0.01, 0.001) and 2 training lengths (500 and 10000 (2500 for VTAB-1k) steps) using the\nvalidation set.\n\nTable 12: Frozen evaluation using linear and Head2Toe (H2T) probe on the VTAB-1k benchmark and four\nother image classification tasks. We report mean accuracies averaged using 3 seeds.\n\nMethod | VTAB-Average | Natural | Specialized | Structured | CIFAR-10 | CIFAR-100 | Flowers | Pets\n------- | ------------ | ------- | ----------- | ---------- | ------- | -------- | ------- | ----\nFinetuning | 76.71 | 89.09 | 87.08 | 61.83 | 99.63 | 99.95 | 97.89 | 99.75\nLinear | 70.22 | 86.62 | 82.41 | 59.07 | 99.48 | 99.72 | 96.99 | 99.52\nHead2Toe | 77.04 | 89.37 | 87.66 | 62.12 | 99.54 | 99.81 | 97.95 | 99.67\n\nTable 13: Frozen evaluation using linear and Head2Toe (H2T) probe on the VTAB-1k benchmark and four\nother image classification tasks. We report mean accuracies averaged using 3 seeds. Training length: 500 steps\n\nMethod | VTAB-Average | Natural | Specialized | Structured | CIFAR-10 | CIFAR-100 | Flowers | Pets\n------- | ------------ | ------- | ----------- | ---------- | ------- | -------- | ------- | ----\nFinetuning | 76.71 | 89.09 | 87.08 | 61.83 | 99.63 | 99.95 | 97.89 | 99.75\nLinear | 70.22 | 86.62 | 82.41 | 59.07 | 99.48 | 99.72 | 96.99 | 99.52\nHead2Toe | 77.04 | 89.37 | 87.66 | 62.12 | 99.54 | 99.81 | 97.95 | 99.67\n\n\n\n\n",
    "29": "Figure 12: Few-shot transfer with 1, 5, 10, and 25 shots on 25 vision tasks (Abnar et al., 2021).\n30",
    "30": "Table 13: Summary of datasets used in our few-shot experiments in Figure 12\nDataset\nDescription\nReference\nImageNet\n1.28M labelled natural images.\n[Deng et al., 2009]\nCaltech101\nThe task consists in classifying pictures of object (101 classes plus a background clutter\nclass), including animals, airplanes, chairs, or scissors. The image size varies, but it\nusually ranges from 200x200 pixels or larger.\n[Li et al., 2022]\nCIFAR-10\nThe task consists in classifying natural images (10 classes, with 6000 training images\neach). Some examples include apples, bottles, dinosaurs, and bicycles. The image size\nis 32x32.\nhttps://www.cs.toronto.edu/\n~kriz/cifar.html\nCIFAR-100\nThe task consists in classifying natural images (100 classes, with 500 training images\neach). Some examples include apples, bottles, dinosaurs, and bicycles. The image size\nis 32x32.\nhttps://www.cs.toronto.edu/\n~kriz/cifar.html\nDTD\nThe task consists in classifying images of textures (47 classes, with 120 training\nimages per class). Some of the textures are banded, bubbly, knotted, or porous. The\nimage size ranges between 300x300 and 640x640 pixels.\n[Cimpoi et al., 2014]\nPets\nThe task consists in classifying pictures of cat and dog breeds (37 classes with around\n200 images each), including Burmese, Chihuahua, English Bulldog, or Bengal. The\nimage size is typically between 200x200 and 500x500 pixels.\nhttps://www.robots.ox.ac.uk/\n~vgg/data/pets/\nFlowers102\nThe Sun397 task is a scenery benchmark with 397 classes, and at least 100 images per\nclass. The images are typically outdoor scenes, such as buildings, forests, mountains,\nor archipelago. The images are (colour) 200x200 pixels or larger.\nhttps://vision.princeton.edu/\nprojects/outdoor/\nSun397\nThe task consists in classifying images of flowers (102 classes, with 40 to 258 images\nper class, and an average of 80 images per class). Common flower types include\ndaisies, tulips, roses, and sunflowers. The image size is between 224x224 pixels.\nhttps://www.robots.ox.ac.uk/\n~vgg/data/flowers/102/\nDTD\nThe task consists in classifying images of Globaly Viewed Textures (47 classes, with 120\ntraining images per class). The images are 300x300 pixels.\n[Cimpoi et al., 2014]\nEuroSAT\nThe task consists in classifying images from the Sentinel-2 satellite with 10 different\nland use and land cover classes. The dataset is organised into a 10-class\nclassification problem. The images are 64x64 pixels.\nhttps://datascience.robovision.io/\nEuroSAT/\nFood-101\nThe task consists in classifying images of food (101 classes, with 1000 images per\nclass). Some examples include sushi, pizza, steak, or chocolate cake. The image size\nis 224x224 pixels.\nhttps://data.vision.ee.ethz.ch/\ndata/food-101/\nFGVC Aircraft\nThe task consists in classifying images of aircraft (100 classes, with 100 images per\nclass). The images are typically taken from above, and show different types of\naircraft, such as Boeing 747, Airbus A320, or Cessna 172. The image size is\nvariable, but typically ranges from 256x256 pixels.\nhttps://www.fgvc.ox.ac.uk/\ndatasets/fgvc-aircraft/\nNABirds\nThe task consists in classifying images of North American birds (400 classes, with at least\n10 images per class). The images are typically taken in the wild, and show different\ntypes of birds, such as American Robin, Bald Eagle, or Blue Jay. The image size is\nvariable, but typically ranges from 256x256 pixels.\nhttps://www.fgvc.ox.ac.uk/\ndatasets/nabirds/\nStanford Dogs\nThe task consists in classifying images of dog breeds (120 classes, with around 200\nimages per class). Some examples include Afghan Hound, Beagle, Bulldog, or\nChihuahua. The image size is typically between 200x200 and 500x500 pixels.\nhttp://vision.stanford.edu/\nprojects/dogs/\nOxfordIIITPet\nThe task consists in classifying images of pet animals (37 classes, with around 200\nimages per class). Some examples include Abyssinian cat, American Bulldog, or Bengal\ncat. The image size is typically between 200x200 and 500x500 pixels.\nhttps://www.robots.ox.ac.uk/\n~vgg/data/pets/\nCUB-200-2011\nThe task consists in classifying images of bird species (200 classes, with 11789 images\nin total). The images are typically taken in the wild, and show different types of birds,\nsuch as American Goldfinch, Bald Eagle, or Blue Jay. The image size is variable, but\ntypically ranges from 256x256 pixels.\nhttp://www.vision.caltech.edu/\nvisipedia/CUB-200-2011.html",
    "31": "E Transfer to dense prediction: More results and addition details.\n\nIn this experiment, we evaluate the effect of fine-tuning versus freezing the ViT-22B backbone when transfer-\nring to semantic segmentation. The results are shown in Table 14. We observe that for the linear decoder\nfine-tuning results in much better performance than using frozen features. For the UperNet decoder, however,\nthe gap between fine-tuning and freezing the backbone is much smaller. This can be explained by the fact\nthat UperNet has ~870 times more parameters than the linear model. Figure 6 shows qualitative results\nusing Upernet.\n\nTable 14: Frozen versus fine-tuning transfer of ViT-22B to semantic segmentation. We report mean IoU on\nthe validation set of 3 popular datasets, namely ADE20K (‚ÄúA-150‚Äù) (Zhou et al., 2017b), Pascal Context\n(‚ÄúP-60‚Äù) (Mottaghi et al., 2014), Pascal VOC and (‚ÄúP-20‚Äù) (Everingham et al., 2010), for different protocols: (i)\nfrozen versus finetuned backbone; (ii) linear (Strudel et al., 2021) versus UperNet (Xiao et al., 2018) decoder.\n\n| Decoder | Dataset | A-150 | P-60 | P-20 |\n|---|---|---|---|---| \n| ViT-22B frozen | 34.6 | 38.6 | 65.0 | \n| ViT-22B fine-tuned | 54.9 | 61.6 | 79.0 | \n| Linear | A-150 | P-60 | P-20 |\n|  | 55.3 | 58.7 | 78.7 |\n\nE.2 Monocular Depth Estimation\n\nE.2.1 Dataset\n\nWe pre-process Waymo Open video and LiDAR data to obtain RGB frames and associated sparse depth\nimages. The camera frames are extracted from the front-facing camera mounted on the vehicle, while the\nsparse depth images are obtained by projecting the LiDAR point cloud of a single time step onto the camera\nframe. We use the camera and LiDAR calibration parameters to compute the distance of each LiDAR point to\nthe camera. For training, we normalize the depth targets using a log(1 + x) transformation, and undo this\ntransformation for metric computation. As the signal during nighttime is weak, we only pick frames with\nsufficient lighting. We do not use any data augmentation during the first 20k iterations of training, and then\nwe apply random color jitter, random scaling, and random cropping. The Waymo Open Dataset provides\nlabels for the first 100k frames. We use the first 70k frames for training, the next 20k frames for validation,\nand the remaining 10k frames for testing. We filter out frames where the ego-vehicle‚Äôs speed exceeds 15m/s\nand the frames with too many LiDAR points missing. We also remove frames where the depth map has more\nthan 50% of its pixels masked. The resulting dataset contains 59,052 training frames, 14,019 validation\nframes, and 7,007 testing frames. We report the following metrics: Absolute Relative Difference (Abs Rel),\nSquared Relative Difference (Sq Rel), Root Mean Squared Error (RMSE), and Œ¥1 (percentage of predictions\nwithin Œ¥ of the ground truth). We use the official evaluation scripts provided by the Waymo Open Dataset.\n\nE.2.2 Implementation Details\n\nWe train our models for 30 epochs with a batch size of 16 using the AdamW optimizer with a learning rate of\n1e-4 and a weight decay of 0.01. We use a linear learning rate schedule with a warmup period of 500 iterations.",
    "34": "E.2.5 Qualitative Results\nWe report qualitative depth predictions by DPT from different ViT backbones in Figure 13, and absolute prediction errors in Figure 14.\n\nF Video Classification\nWe sample 128 and 32 frames with a stride of 2 frames from Kinetics 400 videos (Kay et al., 2017) and Moments in Time (Monfort et al., 2019) videos, respectively. For both ViT-22B and ViT-L/16 we rely on the frozen, pre-trained models and use the pre-logit feature representation to extract a single embedding per frame, resulting in a token sequence of length 128 and 32, respectively, which are then processed by a shallow transformer model equipped with a class-token classifier.\n\nThis is in contrast to CoCa (Yu et al., 2022a), which uses one token per image patch for their video classification experiments and therefore requires much longer token sequences. We explored using one token per image patch (i.e. unpooled features) in preliminary experiments, but found that this leads to inferior performance. One potential reason for this could be that CoCa applies a contrastive loss to a pooled feature representation, and additionally feeds the unpooled token sequences to a generative decoder, which might lead to a different structure in the unpooled representation than the supervised classification loss used to pretrain ViT-22B and ViT-L.\n\nTo facilitate experimentation, we pre-compute frozen features for the two ViT variants we consider, using the same augmentations as (Arnab et al., 2021). To improve the robustness of our model and prevent overfitting we feed the entire training set ten times, with different data augmentations for every pass. We train for 30 epochs on these precomputed features with a batch size of 256 using SGD with momentum and with a cosine schedule including a linear warmup of 2.5 epochs. We sweep the following hyperparameters and corresponding value ranges to train our video model: {1, 2} transformer layers of width {1024, 2048, 4096}, using a learning rate in [10‚Åª¬π, 10‚Åª¬≤] and a weight decay in [10‚Åª¬≥, 10‚Åª¬≤].\n\nG Fairness\n0.47\n0.18\nDP in Data = 0.40\nDP in Data = 0.14\n0.32\n0.12\n0.15\n0.06\nViT-22B\nViT-L/16\n\nFigure 15: DP in the encoder output for attractive and smiling faces. We observe that ViT-22B amplifies DP for attractive faces while ViT-L/16 amplifies DP for smiling faces. DP is computed on the embeddings of the class token. We report the mean DP and standard deviation across 2000 samples from the CelebA dataset (Liu et al., 2015). DP is computed as the difference between the probability of a positive attribute (attractive or smiling) in the data and the probability of the same attribute in the model‚Äôs embeddings, averaged over all samples. We use a pre-trained attribute predictor (He et al., 2019) to estimate the probability of each attribute in both the data and the model‚Äôs embeddings.",
    "35": "Figure 16: Performance, in terms of either accuracy (top), ECE (middle), or OC-AUC (bottom), is plotted for each ViT variant after debiasing the model to meet the prescribed level of bias shown in the legends. Refer to Section 4.5.1 for details.\n\nFigure 17: Performance is plotted for each subgroup in the ViT-22B offers better performance overall across all three metrics, not just overall, but also within each subgroup separately. Refer to Section 4.5.1 for details.\n\n36",
    "36": "Figure 18: The y-axis is the absolute difference in performance across the two subgroups: females and males. ViT-22B provides a more equitable performance, compared to earlier/smaller ViT architectures in all three metrics.\n\nH. Calibration\n\nWe precisely follow the setup of Minderer et al. (2021): Since temperature scaling (Guo et al., 2017) requires some hold-out data, we use 20% of the ImageNet validation set to learn the temperature parameter while we report the accuracy and expected calibration error on the remaining 80%.\n\nMoreover, since the expected calibration error is defined with respect to a probability distribution normalised over the classes, we use a softmax loss function during fine tuning. The sigmoid loss function is defined independently across the classes and does not yield the required normalisation. We use 20k steps with a learning rate of 0.03.",
    "37": "OOD detection (Hendrycks et al., 2019; Fort et al., 2021). In this task, we try to classify whether a given test point belongs to the in-distribution dataset (in our case, ImageNet) or an out-of-distribution dataset (following Real-H (Hendrycks et al., 2019); Tran et al., 2022). We use the maximum softmax probability (MSP) (Hendrycks et al., 2019; Tran et al., 2022) to perform the detection, we use the performance of the resulting binary classification tasks to the AUROC and AUPRC.\n\nSelective prediction. In this task, a model may defer its predictions to human experts when it is not confident in its predictions. We jointly assess a model‚Äôs predictive performance and quality of uncertainty estimates (El-Yaniv and Wiener, 2010). Following Tran et al. (2022), we measure the performance with the oracle collaborative AUC (Kivlchan et al., 2021), with a review fraction of 0.5% of all predictions.\n\nLabel uncertainty. For this evaluation, we aim at demonstrating the ability of the model to capture the inherent ambiguity of image labels assigned by humans. Following Tran et al. (2022), we focus on the ImageNet-H dataset that exploits the human ratings from Beyer et al. (2020) to construct a label distribution representing rate uncertainty for each image. The performance is measured by the negative log likelihood computed with respect to the soft labels (i.e., vectors in the simplex as opposed to the usual one-hot vectors).\n\nI.1 Details about the Plex architecture\n\nWe start by providing some details about the datasets and the different evaluation protocols based on Djolonga et al. (2020).\n\nImageNet-C (Hendrycks and Dietterich, 2019). This variant of the ImageNet dataset contains algorithmically generated corruptions (e.g., blur and noise) applied to the ImageNet test set. The results are reported in the paper are averaged over the 16 corruptions and over their 5 different intensity levels.",
    "38": "Table 15: Evaluation on representative metrics from the Plex reliability benchmark (Tran et al., 2022).\n\n| Metrics | IN-C | IN-Real-H | NLL‚Üì | ECE‚Üì | OC-AUC‚Üë | AUROC‚Üë | AUPRC‚Üë |\n|---|---|---|---|---|---|---|---|\n| ViT-L/32 (Tran et al., 2022) | 70.1 | 1.28 | 0.05 | 0.91 | 0.83 | 0.96 | 1.09 |\n| Plex-L/32 (Tran et al., 2022) | 71.3 | 1.21 | 0.02 | 0.91 | 0.83 | 0.96 | 1.09 |\n| ViT-22B | 81.0 | 0.98 | 0.18 | 0.95 | 0.86 | 0.98 | 0.94 |\n| Plex-22B [BE] | 80.9 | 0.97 | 0.18 | 0.95 | 0.86 | 0.98 | 0.94 |\n| Plex-22B [HET] | 80.9 | 0.97 | 0.18 | 0.94 | 0.86 | 0.97 | 0.93 |\n\nThe main observation is that the increased scale of ViT-22B comes with substantial improvements across all metrics, except for the label uncertainty over ImageNet-Real-H.\n\nMore surprisingly, we can see that across all metrics (except the label uncertainty over ImageNet-Real-H), the Plex-22B variants (both datasets; accuracy difference in Figure 19(b)) consistently leads to improvement at the S, B and L scales.\n\nWe believe that this surprising observation may be related to specific challenges faced at the 22B scale:\n\n‚Ä¢ Pre-training vs. fine-tuning: While Tran et al. (2022) introduce BatchEnsemble layers already at pre-training time, the high training cost of ViT-22B forces us to only operate at fine-tuning time. In this regime, it may be that probabilistic consistency of the BatchEnsemble and heterogeneous variants, while beneficial in general, is less important and can hinder performance.\n\n‚Ä¢ Fine-tuning with label noise: As discussed in the previous section, ImageNet contains a non-trivial amount of label noise. Version 2.0 of the dataset (Xiao et al., 2015) provides a probabilistic correction for a subset of the labels. However, even using this information, training a 22B scale model with fine-tuning may lead to overfitting to the provided label corrections.\n\n‚Ä¢ Data parallelism: As discussed in the previous section, training ViT-22B requires a substantial amount of data parallelism. ",
    "39": "Figure 19: Model-vs-human (Geirhos et al., 2021) benchmarking results for ViT-22B models fine-tuned on ImageNet with different classification. Overall, while the three ViT-22B models trained with different resolutions include standard CNNs (grey), adversarial trained models (blue), self-supervised models (orange) as well as other models evaluated by Geirhos et al. (2021).\n\ndataset than ImageNet can break the observed trade-off. To compare the perceptual similarity of ViT-22B to existing CNNs, we trained models on ImageNet 64 √ó 64 achieves 84.2 accuracy on ImageNet 64 √ó 64 which is 1.6% better. ViT-22B fine-tuned models trained on ImageNet 64 √ó 64 achieves 84.2 accuracy on ImageNet 64 √ó 64 which is 1.6% better. ViT-22B fine-tuned models trained on ImageNet 64 √ó 64 achieves 84.2 accuracy on ImageNet 64 √ó 64 which is 1.6% better than its CNN counterpart. Interestingly, perceptual similarity for ViT-22B does not come at a cost in the downstream transfer performance: ViT-22B trained on ImageNet 64 √ó 64 achieves 60.2% accuracy on a downstream task (zero-shot transfer) while the best CNN achieves 55.2% accuracy (zero-shot transfer). The results demonstrate that ViT-22B learns a more human-like representation of images and transfers better to downstream tasks. Additionally, we observe that ViT-22B trained on ImageNet 64 √ó 64 achieves 76.1% accuracy on ImageNet 224 √ó 224 (linear probing) while the best CNN achieves 72.4% accuracy (linear probing). The results demonstrate that ViT-22B learns a more transferable representation of images and performs better on downstream tasks.",
    "41": "Figure 20: ViT-22B lies on the bottom-right of the accuracy-perceptual similarity tradeoff. It achieves the best validation accuracy on 64 √ó 64 ImageNet with the worst perceptual scores.\n\nand Perceptual Scores 3) ViT-22B with the newly proposed Mean Pool distance function (Kumar et al., 2022) can improve its Perceptual Score up to 66.2.\n\nL Feature attribution analysis\n\nTo get a better understanding on how ViT-22B arrives at its predictions we make use of gradient-based feature attribution methods (a.k.a. saliency maps). Figure 21 shows the result of applying Integrated Gradients (Sundararajan et al., 2017, IG) to three example datapoints before and after ViT-22B cooldown. We find that using a gray 224 baseline and 1024 steps yields qualitatively the best results. The images show a subtle difference in how the two model checkpoints process the example inputs, where more yellow indicates a higher sensitivity. We can also clearly see the patches in which ViT-22B processes input images. This means that the model is less sensitive around the edges of each patch, and suggests a path for future work to improve the model to better deal with patch edges.\n\nSurfer\tAlaskan Malamute\tPalm tree\nInput\nBefore cooldown\nAfter cooldown\nFigure 21: Saliency before and after model cooldown."
  },
  "skipped_pages": [
    0,
    1,
    2,
    3,
    4,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32,
    33,
    34
  ]
}