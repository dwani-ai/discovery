services:
  app:
    image: dwani/discovery_server:latest
    ports:
      - "18889:18889"
    environment:
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      - DWANI_API_BASE_URL=host.docker.internal
    volumes:
      - app-data:/data
    networks:
      - app-network
  llama:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    ports:
      - "9000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - LLAMA_ARG_N_GPU_LAYERS=999
      - LLAMA_ARG_MODEL=/models/gemma-3-4b-it-qat-q4_0-gguf
    volumes:
      - llama-data:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    networks:
      - app-network

volumes:
  app-data:
  llama-data:

networks:
  app-network:
    driver: bridge